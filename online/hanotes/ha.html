<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
               "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="es" xml:lang="es">
<head>
<title>Anotaciones sobre Alta Disponibilidad y tecnologías afines.</title>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8"/>
<meta name="title" content="Anotaciones sobre Alta Disponibilidad y tecnologías afines."/>
<meta name="generator" content="Org-mode"/>
<meta name="generated" content="2013-07-11 17:24:56 CEST"/>
<meta name="author" content="Félix Alises-Casas"/>
<meta name="description" content="Anotaciones sobre Alta Disponibilidad y tecnologías afines. Félix Alises-Casas."/>
<meta name="keywords" content="anotaciones,pacemaker,corosync,ocfs2,clvm"/>
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  html { font-family: Times, serif; font-size: 12pt; }
  .title  { text-align: center; }
  .todo   { color: red; }
  .done   { color: green; }
  .tag    { background-color: #add8e6; font-weight:normal }
  .target { }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .right  {margin-left:auto; margin-right:0px;  text-align:right;}
  .left   {margin-left:0px;  margin-right:auto; text-align:left;}
  .center {margin-left:auto; margin-right:auto; text-align:center;}
  p.verse { margin-left: 3% }
  pre {
	border: 1pt solid #AEBDCC;
	background-color: #F3F5F7;
	padding: 5pt;
	font-family: courier, monospace;
        font-size: 90%;
        overflow:auto;
  }
  table { border-collapse: collapse; }
  td, th { vertical-align: top;  }
  th.right  { text-align:center;  }
  th.left   { text-align:center;   }
  th.center { text-align:center; }
  td.right  { text-align:right;  }
  td.left   { text-align:left;   }
  td.center { text-align:center; }
  dt { font-weight: bold; }
  div.figure { padding: 0.5em; }
  div.figure p { text-align: center; }
  div.inlinetask {
    padding:10px;
    border:2px solid gray;
    margin:10px;
    background: #ffffcc;
  }
  textarea { overflow-x: auto; }
  .linenr { font-size:smaller }
  .code-highlighted {background-color:#ffff00;}
  .org-info-js_info-navigation { border-style:none; }
  #org-info-js_console-label { font-size:10px; font-weight:bold;
                               white-space:nowrap; }
  .org-info-js_search-highlight {background-color:#ffff00; color:#000000;
                                 font-weight:bold; }
  /*]]>*/-->
</style>
<script type="text/javascript">
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>

</head>
<body>

<div id="preamble">

</div>

<div id="content">
<h1 class="title">Anotaciones sobre Alta Disponibilidad y tecnologías afines.</h1>

<p>/*Este documento tiene una redacción informal y, como conjunto de simples
anotaciones, está sujeto a imprecisiones y falta de revisión*/
</p>
<div id="table-of-contents">
<h2>&Iacute;ndice</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#sec-1">1 Conceptos sobre HA</a>
<ul>
<li><a href="#sec-1-1">1.1 Ejemplos de comandos crm</a>
<ul>
<li><a href="#sec-1-1-1">1.1.1 Config gnl y queries al CRM:</a></li>
<li><a href="#sec-1-1-2">1.1.2 Cluster configuration examples:</a></li>
<li><a href="#sec-1-1-3">1.1.3 crm<sub>resource</sub> command</a></li>
</ul>
</li>
<li><a href="#sec-1-2">1.2 Funcionalidad equivalente a ucarp</a>
<ul>
<li><a href="#sec-1-2-1">1.2.1 Añadiendo un servicio</a></li>
</ul></li>
</ul>
</li>
<li><a href="#sec-2">2 DESPLIEGUE: DRBD Primary-Secondary failover + LVM2 + Pacemaker/Corosync + LAMP</a>
<ul>
<li><a href="#sec-2-1">2.1 NTP</a></li>
<li><a href="#sec-2-2">2.2 DRBD</a></li>
<li><a href="#sec-2-3">2.3 LVM2</a></li>
<li><a href="#sec-2-4">2.4 Apache2, MySQL, Phpmyadmin</a></li>
<li><a href="#sec-2-5">2.5 Gestión de enlaces</a>
<ul>
<li><a href="#sec-2-5-1">2.5.1 Una posibilidad es el sw drbdlinks. Ejem para LAMP:</a></li>
<li><a href="#sec-2-5-2">2.5.2 Alternativa: utilizar el RA SYMLINK de pacemaker luego.</a></li>
</ul>
</li>
<li><a href="#sec-2-6">2.6 Corosync</a></li>
<li><a href="#sec-2-7">2.7 Pacemaker</a>
<ul>
<li><a href="#sec-2-7-1">2.7.1 instalación</a></li>
<li><a href="#sec-2-7-2">2.7.2 configuración general</a></li>
<li><a href="#sec-2-7-3">2.7.3 STONITH</a></li>
<li><a href="#sec-2-7-4">2.7.4 Configuración de Pacemaker para failover:</a></li>
</ul>
</li>
<li><a href="#sec-2-8">2.8 Test failover</a></li>
<li><a href="#sec-2-9">2.9 Test phpmyadmin</a>
<ul>
<li><a href="#sec-2-9-1">2.9.1 nota via web:</a></li>
</ul></li>
</ul>
</li>
<li><a href="#sec-3">3 Anotaciones sobre Stacked DRBD para Primary-multiSlave</a></li>
<li><a href="#sec-4">4 DESPLIEGUE: DRBD M-M + cLVM2 + OCFS2 + Pacemaker/Corosync + LAMP</a>
<ul>
<li><a href="#sec-4-1">4.1 Enlaces, Bonding, /etc/hosts, NTP, puppet|cfengine|csync2</a>
<ul>
<li><a href="#sec-4-1-1">4.1.1 iSCSI (Storage Area Network)</a></li>
</ul>
</li>
<li><a href="#sec-4-2">4.2 NTP</a></li>
<li><a href="#sec-4-3">4.3 DRBD (Storage Replication)</a></li>
<li><a href="#sec-4-4">4.4 cLVM2</a>
<ul>
<li><a href="#sec-4-4-1">4.4.1 Respecto al script RA ocf</a></li>
</ul>
</li>
<li><a href="#sec-4-5">4.5 Corosync</a></li>
<li><a href="#sec-4-6">4.6 Pacemaker. OCFS2: instalación y requerimientos. Recursos previos a FS</a>
<ul>
<li><a href="#sec-4-6-1">4.6.1 Variables globales de la CIB</a></li>
<li><a href="#sec-4-6-2">4.6.2 Configuración de STONITH</a></li>
<li><a href="#sec-4-6-3">4.6.3 Recurso ClusterIP1</a></li>
<li><a href="#sec-4-6-4">4.6.4 OCFS2: instalación y requerimientos</a></li>
<li><a href="#sec-4-6-5">4.6.5 Anotaciones sobre el recurso IPAddr2, IP flotante, en primary-primary: estrategia "load sharing" con clone</a></li>
<li><a href="#sec-4-6-6">4.6.6 Recursos DRBD, cLVM, OCFS, LVM</a></li>
</ul>
</li>
<li><a href="#sec-4-7">4.7 Creación de volúmenes LVM2</a></li>
<li><a href="#sec-4-8">4.8 Formateo OCFS2</a></li>
<li><a href="#sec-4-9">4.9 Sorporte de Pacemaker para el resto de recursos (FS en adelante)</a>
<ul>
<li><a href="#sec-4-9-1">4.9.1 Recurso PostgreSQL con streaming replication síncrona master-slave</a></li>
</ul>
</li>
<li><a href="#sec-4-10">4.10 Alternativa a OCFS2: GFS2</a></li>
<li><a href="#sec-4-11">4.11 Load balancing with LVS y Bonding (High Throughput)</a></li>
</ul>
</li>
<li><a href="#sec-5">5 Anotaciones Multi-site clusters</a></li>
<li><a href="#sec-6">6 HA y firewalls</a></li>
<li><a href="#sec-7">7 Tests y desempeño</a></li>
<li><a href="#sec-8">8 Anotación Samba clustering</a></li>
<li><a href="#sec-9">9 Anotación Ganeti2</a></li>
</ul>
</div>
</div>

<div id="outline-container-1" class="outline-2">
<h2 id="sec-1"><span class="section-number-2">1</span> Conceptos sobre HA</h2>
<div class="outline-text-2" id="text-1">


<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<caption></caption>
<colgroup><col class="left" /><col class="left" />
</colgroup>
<tbody>
<tr><td class="left">Pacemaker   CRM</td><td class="left"><a href="http://www.clusterlabs.org/">http://www.clusterlabs.org/</a></td></tr>
<tr><td class="left">Corosync    CM&amp;ML</td><td class="left"><a href="http://www.corosync.org/">http://www.corosync.org/</a></td></tr>
<tr><td class="left">Linux ha ra RA</td><td class="left"><a href="http://www.linux-ha.org/wiki/Resource_Agents">http://www.linux-ha.org/wiki/Resource_Agents</a></td></tr>
<tr><td class="left">INTRO</td><td class="left"><a href="http://theclusterguy.clusterlabs.org/post/1262495133/pacemaker-heartbeat-corosync-wtf">http://theclusterguy.clusterlabs.org/post/1262495133/pacemaker-heartbeat-corosync-wtf</a></td></tr>
<tr><td class="left"></td><td class="left"><a href="http://earlruby.org/corosync-pacemaker-notes/">http://earlruby.org/corosync-pacemaker-notes/</a></td></tr>
<tr><td class="left">GNL HOWTO</td><td class="left"><a href="http://clusterlabs.org/doc/en-US/Pacemaker/1.1-plugin/html/Clusters_from_Scratch/index.html">http://clusterlabs.org/doc/en-US/Pacemaker/1.1-plugin/html/Clusters_from_Scratch/index.html</a></td></tr>
<tr><td class="left">REFERENCIA</td><td class="left"><a href="http://clusterlabs.org/doc/en-US/Pacemaker/1.1-plugin/html/Pacemaker_Explained/index.html">http://clusterlabs.org/doc/en-US/Pacemaker/1.1-plugin/html/Pacemaker_Explained/index.html</a></td></tr>
<tr><td class="left">FENCING/STONITH</td><td class="left"><a href="http://clusterlabs.org/doc/crm_fencing.html">http://clusterlabs.org/doc/crm_fencing.html</a></td></tr>
<tr><td class="left">RA TEMPLATES</td><td class="left"><a href="http://hg.clusterlabs.org/pacemaker/1.1/file/tip/extra/resources/Dummy">http://hg.clusterlabs.org/pacemaker/1.1/file/tip/extra/resources/Dummy</a></td></tr>
<tr><td class="left"></td><td class="left"><a href="http://linux-ha.org/doc/man-pages/re-ra-anything.html">http://linux-ha.org/doc/man-pages/re-ra-anything.html</a></td></tr>
</tbody>
</table>


<ul>
<li>Capas y objetivos a grandes rasgos:

<p>
  Un 1er objetivo es el de lograr levantar recursos en los nodos de forma
  controlada y gestionando las interdependencias. Recursos y dependencias son
  lo suficientemente abstractos para que puedan adaptarse a cualquier situación
  de computación distribuída (en verdad, a cualquier situación de C.D. para
  alta disponibilidad, pues serían necesarias modificaciones para adaptarse,
  por ejemplo, a C.D. para gestión de alta computación y grids tal como Globus,
  aunque hay un parecido claro). En HA se contemplan por ejemplo conceptos de
  recursos de i/o, recursos de almacenamiento, relaciones de primario y
  secundario, relaciones de activo y pasivo, relaciones de orden, relaciones de
  localización y colocación&hellip;
</p>
<p>
  El 2º objetivo es manejar situaciones de Split Brain: situación en que un
  nodo o grupo de ellos detectan la falta de comunicación con otro nodo o grupo
  pero no pueden diferenciar si la causa es que 1) están caídos o 2) no lo
  están y sólo hay un fallo de comunicación; a consecuencia de esa
  incomunicación, se producen varios subclusters independientes. El problema es
  que no saber diferenciar el estado caído o no del/los nodos suele tener
  graves consecuencias (por ejemplo si un nodo errático es interfaz a un
  recurso necesario para el clúster pero no compartible, la posibilidad de
  sustituir al nodo caído haciendo que otro alternativo use el recurso deja de
  ser segura: si no se hace el cluster pierde el recurso necesario, si sí,
  puede corromperlo; es lo común con recursos que almacenan un estado, SAN
  etc). Los mecanismos de resolución son dos: fencing y quorum. Son
  complementarios, actúan conjuntamente.
</p><ul>
<li>Fencing: de "poner una valla" para que el nodo errático no acceda a los
    recursos. Es lo que pone a salvo el recurso crítico. Puede hacerse a dos
    niveles:
<ul>
<li>Resource Fencing: ordenar al recurso sensible que obbie peticiones de
      servicio del nodo errante. Ej: drbd, clvm&hellip;
</li>
<li>Node Fencing = STONITH: denegamos todo servicio a ese nodo apagándolo o,
      más favorable, reiniciándolo. En ambos casos el estado del nodo vuelve a
      ser conocido. El reinicio y su confirmación deben venir dados por un hw
      independiente al nodo errante.
</li>
<li>Es posible que no dispongas de hw para fencing, entonces puedes utilizar
      otras técnicas como node-suicide (self-shutdown sería un subtipo, el
      resto se apagan sin esperar a que los servicios se detengan por init:
      self-STONITH). Pero es menos seguro: puede que el nodo defectuoso no
      pueda ni hacer self-stonith&hellip; Lo mejor es siempre hacer fencing a varios
      niveles.
      <a href="https://www.suse.com/documentation/sle_ha/book_sleha/data/sec_ha_storage_protect_fencing.html">https://www.suse.com/documentation/sle_ha/book_sleha/data/sec_ha_storage_protect_fencing.html</a>
</li>
</ul>

</li>
<li>Quorum: resuelve situaciones que, aunque no induzcan corrupción, disminuyen
    el desempeño, y sin este mecanismo el anterior queda como una solución
    incompleta. Quorum es: la elección de qué subcluster independiente
    permanece, y por tanto cuál es objeto de STONITH. Se resuelve de varias
    maneras:
<ul>
<li>Quorum por votación mayoritaria: sabiendo que todos los participantes
      saben cuántos componen el cluster completo (por la capa de membership),
      cada subcluster organiza una votación a voto por miembro: el mayor
      prevalece. Pero no siempre se encuentra el &gt;N/2, por ejemplo en un número
      par; es el caso de 2 (despliegue más frecuente). En esos casos se suele
      utilizar quorum con un 3º imparcial para esos casos (y sólo para ésos, si
      no sería un SPF).
</li>
<li>Quorum + 3º hardware: por ejem el SCSI reserve
</li>
<li>Quorum + 3º sofware: por ejem el Quorum Daemon
</li>
</ul>

<p>    <a href="http://techthoughts.typepad.com/managing_computers/2007/10/split-brain-quo.html">http://techthoughts.typepad.com/managing_computers/2007/10/split-brain-quo.html</a>
</p>
</li>
</ul>

<p>  Anotación: OpenAFS era un sistema distribuído con alta disponibilidad, pero
  era específico, para almacenamiento, y por ello no necesitaba abstraerse
  tanto, por ejemplo no hacía falta contemplar relaciones de orden dado que
  éste era fijo por la propia naturaleza del servicio, etc. Tenía su propio
  mecanismo de quorum built-in sin interfaz a otros servicios.
</p>
<p>
  Podemos distinguir 3 capas funcionales: una provee servicios de membresía
  (para que todos los nodos conozcan quién forma parte del clúster) y
  comunicación entre nodos; otra asigna (levanta, baja, mueve) recursos en los
  nodos y por último los recursos en sí del clúster:
</p><table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<caption></caption>
<colgroup><col class="left" /><col class="left" />
</colgroup>
<tbody>
<tr><td class="left">Resources</td><td class="left">{cualquier ente que tenga un Resource Agent}</td></tr>
<tr><td class="left">Resource Allocation</td><td class="left">Cluster Resource Manager (pacemaker)</td></tr>
<tr><td class="left">Messaging</td><td class="left">Cluster Messaging and Membership (corosync)</td></tr>
</tbody>
</table>


<p>
  Por su lado, el descubrimiento de los servicios debiera ser el usual: los
  recursos se encuentran consultando los registros SRV de DNS sabiendo el
  protocolo y nombre del entorno distribuído. La IP devuelta (otro recurso del
  clúster) puede ser ahora una flotante gestionada por pacemaker.
</p></li>
</ul>




<ul>
<li>Implementaciones de cada capa:
<ul>
<li>Corosync es el CM&amp;ML, cluster messaging and membership layer (quiénes forman
    parte del clúster y cómo comunicarse con ellos).
</li>
<li>Pacemaker es el CRM, cluster resource manager (carga y descarga servicios)
    se divide en: <img src="http://clusterlabs.org/wiki/File:Stack.png"  alt="http://clusterlabs.org/wiki/File:Stack.png" />
<ul>
<li>CRMd, según la CIB una de ellas toma el rol de master, designed coordinator "DC"
</li>
<li>CIB = cluster information base. Representación y estado del clúster.
</li>
<li>PE  = policy engine, que le calcula la solución óptima que sigue el CRMd master
</li>
<li>STONITHd = shoot-other-node-in-the-head, el interruptor para prevenir
                 corrupciones de datos y otros. Un cluster en producción debe
                 tener algún dispositivo físico (hw) para que STONITHd lo use.
                 STONITH consigue devolver el cluster a un estado conocido.
</li>
<li>otros: attrd (attribute daemon), mgmtd (management daemon).
</li>
</ul>

</li>
<li>RA, resource agents son los tests en formato OCF y un pequeño demonio local
    de iface.
<ul>
<li>agents. Se parece a un script init LSB, pero reciben params como variables
      de entorno, y soportan argumentos adicionales a los típicos (start,
      stop, promote y demote en caso de multi-state&hellip;) como monitor.
</li>
<li>LRM daemon.
</li>
</ul>

</li>
<li>Interfaz para DLM's. Aunque no es un componente fundamental como los
    anteriores, los clústeres necesitan demonios y sus correspondientes RA que
    proveean servicios de lock al resto de servicios. Ésto quiere decir que si
    un software ocupado del almacenamiento del modelo de datos a algún nivel,
    prevee a dicho nivel la posibilidad de accesos concurrentes de escritura
    por parte de sistemas independientes (no comparten el estado de esas
    transacciones), entonces puede implementar un módulo DLM, de Distributed
    Lock Manager, que gestione esos accesos. Esa gestión implica entre otras
    acciones el registro y comunicación de los contendientes, es decir,
    funciones de messaging y membership&hellip; que puede proveerlas
    corosync/pacemaker a través de su iface para DLM. Parte del DLM es
    implementado como un módulo del kernel al que el resto del sw accede a
    través de alguna librería:
    <img src="http://clusterlabs.org/wiki/File:Stack-ais.png"  alt="http://clusterlabs.org/wiki/File:Stack-ais.png" />. Usan la interfaz al DLM:
<ul>
<li>GFS2, OCFS2. Sistemas de ficheros compartidos.
</li>
<li>cLVM2. Extensiones para clústeres de administración de volúmenes LVM2.
</li>
<li>Se supone que DRBD en modo primario-primario tiene escrituras
      concurrentes si un cliente escribe en nodo A y nodo B intenta actualizar
      esa misma información por escritura inmediatamente anterior de un cliente
      en B. Sin embargo se soluciona con gfs2 u ocfs2, como si se arreglase en
      un nivel superior&hellip; quizás lo que ocurre es que drbd ya resuelve el
      problema al nivel en que él actúa, pero no a niveles superiores. En este
      caso drbd implementaría su DLM propio. Otra posibilidad es que DRBD no
      implementase nada porque se esperase que se resolviese todo más arriba al
      usar OCFS2. En cualquier caso no parece que DRBD use la iface DLM de
      pacemaker.
</li>
</ul>

</li>
</ul>

</li>
</ul>




<ul>
<li>Configuraciones posibles: cualquiera (además, se contempla que un servicio -
  y su RA - pueda tener varios estados, tal como primary y secondary).
  <a href="http://ral-arturo.blogspot.com.es/2011/04/aproximacion-al-clustering-de-alta.html">http://ral-arturo.blogspot.com.es/2011/04/aproximacion-al-clustering-de-alta.html</a>
<ul>
<li>Active/Passive
</li>
<li>Active/Active  (permite balanceo de carga)
</li>
<li>N+1 (a/p o a/a pero con un nodo de backup de alguno)
</li>
<li>N+M (varios de backups: uno de backup del nodo1, otro de backup del nodo2&hellip;)
</li>
<li>N-to-1
</li>
<li>N-to-N (todos pueden actuar de cualquier cosa; modelo de datos es compartido)
</li>
</ul>

</li>
</ul>



<ul>
<li>CIB es un xml en memoria, con la configuración y el estado según cada local
  LRMd, a veces rehecho desde cero, otras actualizado. El apartado de config
  es:
<ul>
<li>opciones generales
</li>
<li>nodos
</li>
<li>recursos
</li>
<li>relaciones entre recursos
</li>
<li>Delegaciones. Autorizaciones con ACL a entidades POSIX.
<ul>
<li>Los miembros del grupo POSIX haclient tienen acceso a las utilidades
      pero, además, el acceso a partes de la configuración se puede regular
      mediante ACL y roles (conjuntos de ACL predefinidos, como
      "monitor"). Está claro que deben ser usuarios POSIX centralizados o, al
      menos, que sus UID sean idénticos en todos los nodos. Se activa con: la
      propiedad enable-acl=true y se definen usando XPath, aunque también hay
      una sintaxis reducida con tags; ambas se pueden usar en crm:
      <a href="https://www.suse.com/documentation/sle_ha/book_sleha/data/sec_ha_acl_basics.html">https://www.suse.com/documentation/sle_ha/book_sleha/data/sec_ha_acl_basics.html</a>
      <a href="https://www.suse.com/documentation/sle_ha/book_sleha/data/sec_ha_acl_config_crm.html">https://www.suse.com/documentation/sle_ha/book_sleha/data/sec_ha_acl_config_crm.html</a>
</li>
</ul>

</li>
</ul>

</li>
</ul>









<ul>
<li>Propiedades globales: configuran algunos comportamientos globales,
  independientes de un recurso concreto.
<ul>
<li>quorum
</li>
<li>stonith
</li>
<li>placement-strategy
</li>
<li>enable-acl=true
</li>
<li>&hellip;
</li>
</ul>

</li>
</ul>






<ul>
<li>Conceptos relativos a los recursos:
<ul>
<li>Grupos: recursos que deben activarse en un orden concreto, desactivarse en
    el orden inverso y activarse juntos en el mismo nodo. Si la activación de
    uno falla los siguientes no lo hacen.
</li>
<li>Clones: recursos que deben ser activados a la vez en más de un nodo (su RA
    debe soportar ésto). Los hay
<ul>
<li>anónimos si funcionan igual en cualquier nodo,
</li>
<li>únicos globales si no, por ejemplo si necesitan config distinta según el nodo.
</li>
<li>hay un 3er tipo, stateful clones de los multi-state:
</li>
</ul>

</li>
<li>Multistate clones (ms): clones especializados donde las instancias pueden
    estar en dos modos, como master y slave.
</li>
<li>Manejo de restricciones:
    <a href="https://www.suse.com/documentation/sle_ha/book_sleha/data/sec_ha_configuration_basics_constraints.html">https://www.suse.com/documentation/sle_ha/book_sleha/data/sec_ha_configuration_basics_constraints.html</a>
<ul>
<li>Selectividad de nodo (location)
</li>
<li>Orden (order)
</li>
<li>Co-localizaciones (colocation)
</li>
<li>Scores: al definir restricciones se suelen hacer de forma cuantizada (que
      puede hacerse dicotómica utilizando como cuantizador el literal inf o
      -inf).
</li>
<li>Disposición relativa, según la disposición de otros.

</li>
</ul>

<p>    &hellip; permiten definir la restricción no sólo respecto a un recurso, también
        respecto a ese recurso en un estado, modo, evento: se ha promocionado
        en algún nodo, se ha levantado, está en modo master, está en modo
        secundario&hellip; Hay una restricción location especial, relativa a
        carga/capacidad:
</p>
<ul>
<li>LIBL: Location relativo a los requerimientos de carga del servicio y la
      capacidad del nodo (load impact based location). Para ello el cluster
      tiene soporte para definir ambas así como la estrategia que se utilizará.
<ul>
<li>Respecto a los requerimientos, estáticamente se usa la opción
        utilization &lt;type&gt;=&lt;value&gt; al definir el primitive.
</li>
<li>El RA ofc:pacemaker:NodeUtilization, tras crear una primitiva y clones,
        permite que el CIB se rellene de información relativa a la capacidad de
        cada nodo. También se puede definir estáticamente.
<ul>
<li>Parecido es ofc:heartbeat:VirtualDomain con
          dynamic<sub>utilization</sub>=1. Éste más que para detectar mínimos, se utiliza
          como monitorizador de CPU y RAM, rellena los valores en cada ciclo.
</li>
</ul>

</li>
<li>La propiedad global "placement-strategy" inicializada como
        "utilization", "minimal" y "balanced" usan la información de
        utilización-capacidad junto a un algoritmo por defecto típico (atiende
        al número de servicios por nodo), minimizando consumo y por tanto
        exhaustando la capacidad para reducir el número de servidores activos, o
        distribuyendo igualitariamente para que ninguno se sobrecargue. El valor
        "default" deshabilita el uso de información de utilización-capacidad, y
        el algoritmo es el de los score y threshold y, si 0, evitando
        sobrecarga. Ejemplo de capacidad de un nodo, requerimientos de un
        primitive y estrategia:
<ul>
<li>node node1 utilization memory="4000" cpu=8
</li>
<li>primitive xenA ocf:heartbeat:Xen utilization memory="2000" cpu=4 meta priority="10"
</li>
<li>property placement-strategy="minimal"
</li>
</ul>

</li>
<li>A la luz de este subsistema, es fácil imaginar cómo funcionan:
<ul>
<li>clústeres de alta computación: el usuario define un proceso y los
          recursos que requiere. Entonces se autentifica en el clúster y le pide
          que ejecute el proceso y le asigne tales recursos, si está autorizado
          para ello.
</li>
<li>ampliaciones de una máquina virtual en computación elástica: redefines
          el primitive de esa máquina con los nuevos requerimientos y dejas que
          actúe el sistema de localización relativa a requerimientos; si el RA de
          la vm lo soporta, entiendo que acabará de ejecutarse sólo cuando se
          haya terminado la migración en caliente de la vm al nodo destino (¿se
          puede informar del nodo destino al usar un RA con stop?); entonces se
          activará la vm en el otro nodo, donde ya está. ¿?
</li>
</ul>

</li>
</ul>

</li>
</ul>

</li>
</ul>

</li>
</ul>




<ul>
<li><b>Definición de un recurso dado su RA:</b>

<p>
    <a href="https://www.suse.com/documentation/sle_ha/book_sleha/data/cha_ha_agents.html">https://www.suse.com/documentation/sle_ha/book_sleha/data/cha_ha_agents.html</a>
</p><ul>
<li>Primitiva: id, class (clase de script RA: ocf, hb), provider (si hay
      varios), type (nombre del RA).
</li>
</ul>

</li>
</ul>




<pre class="src src-text">crm_resource --resource &lt;resourcename&gt; --query-xml
</pre>

<ul>
<li>Es posible definir templates de recursos y referenciarlos luego cual
        macro; si repiten opciones prevalencen las del primitive, no el template.
</li>
<li>Parámetros: un RA ocf es un script init LSB pero que soporta parámetros
</li>
</ul>

<p>      arbitrarios pasados como variables; éstos los puedes usar o no
      definiéndolos para cada instacia de RA. Ejem: parámetro ip de una
      instancia del RA IPAddr.
</p>


<pre class="src src-text">crm ra info [class:[provider:]]resource_agent
</pre>

<ul>
<li>Opciones y metaopciones: configura el comportamiento de ese recurso en el
      cluster: si la función del cluster es dejarlo parado o cargado, si puede
      levantarlo o bajarlo, nivel para migraciones (cada vez que falla se
      incrementa su failcount, y el migration-threshold precipita su failover),
      stickness (&gt;0 hasta +INF precipita failbacks tras failover), si múltiples
      activos&hellip;
</li>
<li>Monitores asociados: si quieres que el clúster monitorice una instancia
      de recurso has de declarar un monitor (o varios) como activo, declarar
      una frec, acciones etc. En este ejem creo que se declara un monitor para
      el estado de parado (útil si se tiene que evitar concurrencia etc) y otro
      para el resto:
       op monitor interval="300s" role="Stopped" timeout="10s" \
       op monitor interval="30s" timeout="10s"
</li>
<li>Rules.

</li>
<li>Distinto son los RA tipo STONITH, éstos (la parte sw) residen en
      /usr/lib/stonith/plugins, y se listan con stonith -L. Sobre hw posible:
      <a href="https://www.suse.com/documentation/sle_ha/book_sleha/data/sec_ha_fencing_config.html">https://www.suse.com/documentation/sle_ha/book_sleha/data/sec_ha_fencing_config.html</a>
<ul>
<li>Power Distribution Units (PDU)
</li>
<li>Uninterruptible Power Supplies (UPS)
</li>
<li>Blade Power Control Devices (if you use blades)
</li>
<li>Lights-out Devices: IBM RSA, HP iLO, Dell DRAC&hellip;
</li>
<li>(Testing Devices)
</li>
</ul>

<p>      Anotación: sus RA también permiten monitor, pero se recomienda usarlo
      cada hora/s, no cada minuto pues la parte hw puede tener problemas para
      gestionar más de una sesión a la vez, o mucho tráfico.
</p><ul>
<li>Recomendaciones para STONITH
<ul>
<li>No enchufes los equipos en paralelo.
</li>
<li>Verifica que funciona, retira la conexión en cada nodo a ver qué pasa.
</li>
<li>Stresa tus recursos y comprueba entonces sus valores de tiemout, de forma
          que los valores que declares no sean bajos y disparen fencing
          innecesarios.
</li>
<li>Usa hw apropiado.
</li>
<li>Usa más de un recurso STONITH.
</li>
<li>DLM/OCFS2 se bloqueará para siempre esperando una operación de fencing
          que nunca ocurrirá si no configuras STONITH.
</li>
<li>No es conveniente configurar startup-fencing (en tiempo de boot,
          stonith-enabled era en general) a false, porque es mejor que si al
          arranque un nodo está en un estado desconocido, actúe stonith para
          aclararlo.

</li>
</ul>

</li>
</ul>

</li>
<li>Creando otros RA
<ul>
<li>Existe Dummy, que se suele usar como plantilla.
      mkdir -p /usr/local/lib/ocf/resource.d/local
      ln -s /usr/local/lib/ocf/resource.d/local /usr/lib/ocf/resource.d/local
      cp /usr/lib/ocf/resource.d/pacemaker/Dummy /usr/local/lib/ocf/resource.d/local/myra
      (return codes <a href="https://www.suse.com/documentation/sle_ha/book_sleha/data/sec_ha_errorcodes.html">https://www.suse.com/documentation/sle_ha/book_sleha/data/sec_ha_errorcodes.html</a>)
<ul>
<li>También existe un RA "anything": man ocf_heartbeat_anything
</li>
</ul>

</li>
</ul>

</li>
</ul>




<pre class="src src-text">primitive Rest ocf:heartbeat:anything params binfile="/opt/scripts/rest.sh" \
 user="rapt01" logfile="/tmp/rest.out"      errlogfile="/tmp/rest.err" \
 op start      interval="0"   timeout="20"                             \
 op stop       interval="0"   timeout="30"                             \
 op monitor    interval="20"
</pre>

<ul>
<li>Configuración dinámica: pacemaker define Rules para hacer la configuración
    dinámica según diferentes criterios (como la hora del día)

</li>
<li>Pacemaker tiene notificaciones por (correo y) SNMP, y de ahí podemos
    monitorizarlo con nagios3. También tiene posibilidad de hacer cron con
    algunas acciones.
</li>
</ul>









<ul>
<li>Notas sobre crm shell:
<ul>
<li>Modo no interactivo: crm -f script.cli, crm &lt; script.cli, crm &lt;subcomandos&gt;
</li>
<li>Respecto al modo interactivo: crm sin subcomandos ni flags inicia modo
    interactivo. quit sale, end sube un nivel (como netsh).
<ul>
<li>cib new myNewConfig    # registra una nueva config
</li>
<li>cib reset myNewConfig  # copia live config en myNewConfig
</li>
<li>ptest                  # tras hacer cambios, puedes comprobarlos con ptest
</li>
<li>commit                 # guarda lo que hayas hecho en myNewConfig (no la usa, sólo guarda cambios)
</li>
<li>cib use live           # si quieres que crm vuelva a operar sobre live
</li>
<li>cib use myNewConfig    # si quieres que crm vuelva a operar sobre myNewConfig
                             # Lo que no sé es cómo hacer que se use myNewConfig
</li>
<li>show
</li>
</ul>

</li>
<li>templates:
<ul>
<li>resource-templates del cluster:
</li>
</ul>

</li>
</ul>

</li>
</ul>




<pre class="src src-text">crm configure rsc_template BigVM ocf:heartbeat:Xen params mem\_mgmt="true"
primitive MyVM1 @BigVM  # para usar el resource template anterior.
</pre>

<ul>
<li>configuration-templates de crm: son configuraciones completas que puedes
      usar, pero no referenciar. Al usarla te dice qué valores faltan por
      especificar y tal.
</li>
</ul>




<pre class="src src-sh">crm configure list template
crm configure template new myIntranet &lt;from<span style="color: #ffa07a;">\_</span>some<span style="color: #ffa07a;">\_</span>template&gt;
</pre>

<p>
      &hellip; <a href="https://www.suse.com/documentation/sle_ha/book_sleha/?page=/documentation/sle_ha/book_sleha/data/sec_ha_manual_config_crm.html#sec_ha_manual_config_template">https://www.suse.com/documentation/sle_ha/book_sleha/?page=/documentation/sle_ha/book_sleha/data/sec_ha_manual_config_crm.html#sec_ha_manual_config_template</a>
</p>






<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<caption></caption>
<colgroup><col class="left" />
</colgroup>
<tbody>
<tr><td class="left"><a href="http://ral-arturo.blogspot.com.es/2011/04/aproximacion-al-clustering-de-alta.html">http://ral-arturo.blogspot.com.es/2011/04/aproximacion-al-clustering-de-alta.html</a></td></tr>
<tr><td class="left"><a href="http://ral-arturo.blogspot.com.es/2011/05/i-cluster-basico-de-alta-disponibilidad.html">http://ral-arturo.blogspot.com.es/2011/05/i-cluster-basico-de-alta-disponibilidad.html</a></td></tr>
<tr><td class="left"><a href="http://ral-arturo.blogspot.com.es/2011/05/ii-cluster-basico-de-alta.html">http://ral-arturo.blogspot.com.es/2011/05/ii-cluster-basico-de-alta.html</a></td></tr>
<tr><td class="left"><a href="http://kaivanov.blogspot.com.es/2012/01/building-ha-cluster-with-pacemaker.html">http://kaivanov.blogspot.com.es/2012/01/building-ha-cluster-with-pacemaker.html</a></td></tr>
<tr><td class="left"><a href="http://taqlim.blogspot.com.es/2013/02/mysql-apache-failover-system-with.html">http://taqlim.blogspot.com.es/2013/02/mysql-apache-failover-system-with.html</a></td></tr>
<tr><td class="left"><a href="https://www.suse.com/documentation/sle_ha/book_sleha/data/sec_ha_storage_protect_fencing.html">https://www.suse.com/documentation/sle_ha/book_sleha/data/sec_ha_storage_protect_fencing.html</a></td></tr>
</tbody>
</table>



<p>
La infraestructura de clustering de RedHat usa cman como CM&amp;M compatible AIS,
fenced es su stonithd; su CRM sería rgmanager. Cman es compatible con corosync,
en este caso se carga como un plugin y parece que servicios relacionados con
sistemas de ficheros compartidos (OCFS2, GFS etc) sólo pueden comunicarse con
cman, luego es imprescindible su presencia en esos casos.
</p>

</div>

<div id="outline-container-1-1" class="outline-3">
<h3 id="sec-1-1"><span class="section-number-3">1.1</span> Ejemplos de comandos crm</h3>
<div class="outline-text-3" id="text-1-1">


</div>

<div id="outline-container-1-1-1" class="outline-4">
<h4 id="sec-1-1-1"><span class="section-number-4">1.1.1</span> Config gnl y queries al CRM:</h4>
<div class="outline-text-4" id="text-1-1-1">




<pre class="src src-text">crm_mon
crm status                              # estado del manager
crm resource status                     # estado de los recursos reg

crm configure                           # shell crm
    cib new working                     # crear backup del cib llamado working
    cib reset working                   # actualiza el backup del cib
    cib commit working                  # hace a la copia working el cib usado

crm configure show                      # configuraci&#243;n de pacemaker (crm, ra...)
crm configure show xml                  # configuraci&#243;n en xml
crm configure show &gt; /tmp/crm.xml
crm configure load update /tmp/crm.xml

crm_verify -L -V                        # verificar configuraci&#243;n

crm resource status ClusterIP           # Dice qu&#233; host asume el control de ese resource
crm resource move ClusterIP server1     # mueve ese recurso (y depends) al server1
crm resource unmove ClusterIP           # quitar esa preferencia por server1
crm resource cleanup WebSite            # To clear the failcount on a WebSite resource
crm configure delete &lt;id&gt;               # Elimina de la configuraci&#243;n ese recurso
crm resource start primitive_FS_OCFS2   # 

crm ra list ocf heartbeat               # lista todos los ra disponibles de hb y pacemaker
crm ra list ocf pacemaker
crm ra info ocf:drbd:linbit
crm ra list stonith
crm ra info stonith:external/ipmi
# ... si necesito ver lo que hacen mientras son llamados, puedo usar algo como:
# )
# ) 3&gt;&amp;1 1&gt;&amp;2 2&gt;&amp;3 1&gt;&amp;3 | tee -a /tmp/&lt;raname&gt;.ra
ocf-tester -n Apache2 -o configfile=/etc/apache2/apache2.conf -o ... /usr/lib/ocf/resource.d/heartbeat/apache
... ocf-tester necesita a xmllint (paquete libxml2-utils)

crm resource start start &lt;id&gt;           # comienza recurso &lt;id&gt; con estado start
crm migrate &lt;id&gt; node2                  # fuerza migraci&#243;n

crm resource secret myDB set passwd &lt;pw&gt;# si tienes un recurso myDB que usa
                                        # pass, puedes declararlo as&#237; para que
crm resource secret myDB show passwd    # no se liste, supongo.

crm history
crm history info
crm history detail &lt;n&gt;                  # para hacer + o - verborreico
crm history transition -1               # muestra &#250;ltima transici&#243;n
                                        # usa a gaphviz (si no, -nograph).

stonith -L                              # Lista stonith RA tambi&#233;n
stonith -t &lt;stonith-device-type&gt; -n     # lista params de un RA stonith
stonith -t &lt;stonith-device-type&gt; -h     # da help resumida
</pre>


</div>

</div>

<div id="outline-container-1-1-2" class="outline-4">
<h4 id="sec-1-1-2"><span class="section-number-4">1.1.2</span> Cluster configuration examples:</h4>
<div class="outline-text-4" id="text-1-1-2">




<pre class="src src-text">crm configure property stonith-enabled=false   # 
crm configure property no-quorum-policy=ignore # 
crm configure property enable-acl=true
</pre>

<ul>
<li>Adding a second IP address resource to the cluster:
</li>
</ul>




<pre class="src src-text">crm configure primitive ClusterIP ocf:heartbeat:IPaddr2    \
        params ip="192.168.1.97" cidr_netmask="24"         \
        op monitor interval="30" timeout="25" start-delay="0"
</pre>

<ul>
<li>Adding an Apache resource with https enabled:
</li>
</ul>




<pre class="src src-text">crm configure primitive WebSite ocf:heartbeat:apache                \
        params configfile="/etc/apache2/httpd.conf" options="-DSSL" \
        operations $id="WebSite-operations"                         \
        op start interval="0" timeout="40"                          \
        op stop interval="0" timeout="60"                           \
        op monitor interval="60" timeout="120" start-delay="0"      \
        statusurl="http://127.0.0.1/server-status/"                 \
        meta target-role="Started"
</pre>

<ul>
<li>Adding a DRBD resource:

<p>
    Note that the resource name drbd_resource="drbd_resource<sub>name</sub>" must match
    the name listed in /etc/drbd.conf and that drbd should NOT be started by
    init. <a href="http://www.drbd.org/users-guide-emb/s-pacemaker-crm-drbd-backed-service.html">http://www.drbd.org/users-guide-emb/s-pacemaker-crm-drbd-backed-service.html</a>
</p></li>
</ul>




<pre class="src src-text">crm configure primitive VolumeDRBD ocf:linbit:drbd                  \
        params drbd_resource="drbd_resource_name"                   \
        operations $id="VolumeDRBD-operations"                      \
        op start interval="0" timeout="240"                         \
        op promote interval="0" timeout="90"                        \
        op demote interval="0" timeout="90"                         \
        op stop interval="0" timeout="100"                          \
        op monitor interval="40" timeout="60" start-delay="0"       \
        op notify interval="0" timeout="90"                         \
        meta target-role="started"
crm configure primitive FileSystemDRBD ocf:heartbeat:Filesystem       \
        params device="/dev/drbd0" directory="/srv/src" fstype="ext3" \
        operations $id="FileSystemDRBD-operations"                    \
        op start interval="0" timeout="60"                            \
        op stop interval="0" timeout="60" fast_stop="no"              \
        op monitor interval="40" timeout="60" start-delay="0"         \
        op notify interval="0" timeout="60"
crm configure ms MasterDRBD VolumeDRBD                                \
        meta clone-max="2" notify="true" target-role="started"
</pre>

<ul>
<li>Dependencias: IP, Apache y Drdb primary anteriores deben ser tomados
    siempre por el mismo nodo:
</li>
</ul>




<pre class="src src-text">crm configure group Cluster ClusterIP FileSystemDRBD WebSite          \
        meta target-role="Started"
crm configure colocation WebServerWithIP inf: Cluster MasterDRBD:Master
crm configureorder StartFileSystemFirst inf: MasterDRBD:promote Cluster:start
</pre>


</div>

</div>

<div id="outline-container-1-1-3" class="outline-4">
<h4 id="sec-1-1-3"><span class="section-number-4">1.1.3</span> crm<sub>resource</sub> command</h4>
<div class="outline-text-4" id="text-1-1-3">

<p>    <a href="http://www.bersol.info/?p=24">http://www.bersol.info/?p=24</a>
</p>



<pre class="src src-text">#Comprobar qu&#233; servicios hay en un cluster:
crm_resource -L | grep Group
    Resource Group: group_1

#Comprobar en qu&#233; nodo se encuentra el servicio:
crm_resource -W -r group_1 -t group
    resource group_1is running on: emanel2

#Arrancar/Parar un servicio:
crm_resource --resource myResource  --set-parameter  target-role  --meta  --parameter-value stopped
crm_resource --resource myResource  --set-parameter  target-role  --meta  --parameter-value started
... menos verborreico, y con -H &lt;nodo&gt; (creo que posible)
crm_resource -r group_1-t group -p target_role -v started -H d1
crm_resource -r group_1-t group -p target_role -v stopped -H d1

#Migrar el servicio a otro nodo y al nodo por defecto
crm_resource -M -r group_1 -t group -H dnodo1
crm_resource -U -r group_1 -t group

#Dejar un Nodo en Standby (suelta los servicios y se pone como secundario):
/usr/lib/heartbeat/hb_standby  # &#243;&#8230; crm_resource -H d1 -v on

#Poner un Nodo OnLine (coge los servicios y se pone como primario):
/usr/lib/heartbeat/hb_takeover # &#243;&#8230; crm_resource -H d1 -v off

#Poner un nodo en standby (dejarlo offline para mantenimiento sin provocar un evento de fallo):
crm_standby
</pre>


<p>
…para chequear estado/configuracion del cluster
</p>



<pre class="src src-text">#Monitorizar el estado del Cluster:
crm_mon -i2

#Para chequear errores en el CIB&#8230;
crm_verify -LV

#Muestra el estado actual de los recursos&#8230;
crm_mon -r -1

#Chequeo exhaustivo para errores e inconsistencias en el CIB&#8230;
ciblint -L

#Nuevo chequeo del codigo del CIB ya sea online/offline (ver parametros)&#8230;
ptest

#Muestra la informaci&#243;n CIB:
cibadmin -Q

# Para para a continuaci&#243;n borrar cach&#233;:
/var/lib/heartbeat/hostcache
/var/lib/heartbeat/delhostcache
</pre>





</div>
</div>

</div>

<div id="outline-container-1-2" class="outline-3">
<h3 id="sec-1-2"><span class="section-number-3">1.2</span> Funcionalidad equivalente a ucarp</h3>
<div class="outline-text-3" id="text-1-2">

<p>La funcionalidad de ucarp: varios nodos, un servicio: IP flotante, no
interdependencias al no haber más servicios.
<a href="http://www.debian-administration.org/article/678/Virtual_IP_addresses_with_ucarp_for_high-availability">http://www.debian-administration.org/article/678/Virtual_IP_addresses_with_ucarp_for_high-availability</a>
</p>
<p>
&hellip; servida con pacemaker+corosync sería como se explica aquí:
<a href="http://albertomolina.wordpress.com/2012/03/04/sencillo-cluster-de-alta-disponilidad-con-pacemaker-y-corosync/">http://albertomolina.wordpress.com/2012/03/04/sencillo-cluster-de-alta-disponilidad-con-pacemaker-y-corosync/</a>
</p>


<pre class="src src-text">nodo1  :    10.0.0.1
nodo2  :    10.0.0.2
recurso: ip 10.0.0.11
</pre>


<pre class="src src-sh">corosync-keygen                  <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">genera /etc/corosync/authkey, para</span>
chmod 400 /etc/corosync/authkey  <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">los siguientes nodos usamos &#233;sa tx por scp</span>

vim /etc/corosync/corosync.conf
 bindnetaddr: 10.0.0.0 <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">en un escenario no trivial, la red de paso de mensajes</span>
                       <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">ser&#225; distinta al recurso FAILOVER-ADDR q gestionaremos.</span>

vim /etc/default/corosync
 <span style="color: #eedd82;">START</span>=yes

invoke-rc.d corosync restart

crm_mon
</pre>

<p>
&hellip; acaba la configuración de corosync y la inclusión de miembros del clúster;
pasamos a pacemaker y la definición del recurso:
</p>



<pre class="src src-sh"><span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Con s&#243;lo dos nodos, no puede decidir coordinadamente puesto que cuando cae</span>
<span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">uno y se va a decidir la soluci&#243;n... s&#243;lo queda uno, no hay quorum y no se</span>
<span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">har&#237;a nada: deshabilito quorum:</span>
crm configure property no-quorum-policy=ignore
<span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Deshabilito stonith</span>
crm configure property stonith-enabled=false

<span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Defino recurso FAILOVER-ADDR. La ip es la flotante, la compartida.</span>
crm configure primitive FAILOVER-ADDR ocf:heartbeat:IPaddr2 <span style="color: #ffa07a;">\</span>
    params <span style="color: #eedd82;">ip</span>=<span style="color: #ffa07a;">"10.0.0.11"</span> <span style="color: #eedd82;">nic</span>=<span style="color: #ffa07a;">"eth0"</span>                        <span style="color: #ffa07a;">\</span>
    op monitor <span style="color: #eedd82;">interval</span>=<span style="color: #ffa07a;">"10s"</span> meta is-managed=<span style="color: #ffa07a;">"true"</span>

<span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">No defino localizaci&#243;n ni dependencias al ser un escenario trivial.</span>
</pre>

<p>
&hellip; si hago ping 10.0.0.11 y anoto la mac que los devuelve, apago (o
    configuro como offline) esa máquina y vuelvo a hacer el ping, será
    respondido pero ahora por una mac distinta a la inicial y así
    progresivamente.
</p>


</div>

<div id="outline-container-1-2-1" class="outline-4">
<h4 id="sec-1-2-1"><span class="section-number-4">1.2.1</span> Añadiendo un servicio</h4>
<div class="outline-text-4" id="text-1-2-1">

<p>Añadiendo Apache2:
<a href="http://clusterlabs.org/doc/en-US/Pacemaker/1.1-plugin/html/Clusters_from_Scratch/ch06.html">http://clusterlabs.org/doc/en-US/Pacemaker/1.1-plugin/html/Clusters_from_Scratch/ch06.html</a>
</p>
<p>
<a href="http://serverfault.com/questions/418634/secure-iptables-rules-for-corosync">http://serverfault.com/questions/418634/secure-iptables-rules-for-corosync</a>
</p>
</div>
</div>
</div>

</div>

<div id="outline-container-2" class="outline-2">
<h2 id="sec-2"><span class="section-number-2">2</span> DESPLIEGUE: DRBD Primary-Secondary failover + LVM2 + Pacemaker/Corosync + LAMP</h2>
<div class="outline-text-2" id="text-2">

<p><a href="http://taqlim.blogspot.com.es/2013/02/mysql-apache-failover-system-with.html">http://taqlim.blogspot.com.es/2013/02/mysql-apache-failover-system-with.html</a>
</p>


<pre class="src src-text">Red de IP flotante: 10.0.0.0 -&gt; IP flotante: 10.0.0.1
    d1.example.org:       .11
    d2.example.org:       .12

Red para DRBD: 192.168.1.0
           d1: 192.168.1.1, /dev/sdb -&gt; vg -&gt; /mnt/drbdr0/lamp1
           d2:          .2, /dev/sdb 

Red para CM&amp;M: 192.168.0.0
           d1:          .1
           d2:          .2
</pre>

<p>
Lo mínimo sería una red para CM&amp;M, otra red para DRBD y los servicios finales
con la IP flotante. De todas formas, para testear se puede usar sólo una
también. Conviene utilizar bonding en los enlaces.
</p>
<p>
(Posibilidad de usar un nodo aparte de backup:
 <a href="http://www.howtoforge.com/drbd-8.3-third-node-replication-with-debian-etch">http://www.howtoforge.com/drbd-8.3-third-node-replication-with-debian-etch</a> )
</p>

</div>

<div id="outline-container-2-1" class="outline-3">
<h3 id="sec-2-1"><span class="section-number-3">2.1</span> NTP</h3>
<div class="outline-text-3" id="text-2-1">

<ul>
<li>En todos:
</li>
</ul>




<pre class="src src-sh">apt-get install ntp 
ntpd -q -g <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">bla bla bla o seguir el anexo del PFC</span>
</pre>

</div>

</div>

<div id="outline-container-2-2" class="outline-3">
<h3 id="sec-2-2"><span class="section-number-3">2.2</span> DRBD</h3>
<div class="outline-text-3" id="text-2-2">

<ul>
<li>Opcional: podemos crear volúmenes LVM2 sobre los que usar DRBD, o usar
  directamente los dispositivos de bloque para drbd.

</li>
<li>Instalamos. En todos:
</li>
</ul>




<pre class="src src-sh">apt-get -t squeeze-backports install drbd8-utils drbdlinks
update-rc.d drbd disable    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Si pacemaker es el asignador, que s&#243;lo sea &#233;l.</span>
<span style="color: #b0c4de;">echo</span> <span style="color: #ffa07a;">'drbd'</span> &gt;&gt; /etc/modules <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">El script init cargaba el m&#243;dulo</span>
modprobe drbd
</pre>

<p>
&hellip; el módulo está en el kernel desde 2.6.33, supongo que lo carga script init
    pero yo no voy a usar éste (creo) por ello lo puse en /etc/modules.
    la versión es 8.3, si fuese 8.4 la sintaxis variará algo.
</p>
<ul>
<li>Configuramos. En d1 (luego lo pasamos a d2 por scp):
</li>
</ul>




<pre class="src src-sh">view /etc/drbd.conf
vim /etc/drbd.d/global_common.conf  <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Opciones globales y comunes</span>
vim /etc/drbd.d/r0.res              <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Opciones por recurso DRBD</span>
</pre>


<pre class="src src-sh"><span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Content of example.org. &#201;sto hay que ponerlo entre el global y el .res</span>

<span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Par&#225;metros globales. S&#243;lo est&#225; permitida una secci&#243;n global.</span>
<span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">minor-count, dialog-refresh, disable-ip-verification and usage-count</span>
global {
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Permitir que usage.drbd.org contabilice que estamos usando el sw DRBD:</span>
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">no,yes,ask  Durante pruebas pongo no.</span>
    usage-count no;
}

<span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Opciones heredadas por todos las secciones resource posteriores.</span>
common {
  <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Syncer configura finamente el comportamiento de replicaci&#243;n:</span>
  <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">"one third of your bandwidth. It only limits the resynchronization, not the mirroring"</span>
  <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">rate, after, al-extents, use-rle, cpu-mask, verify-alg, csums-alg, c-plan-ahead, c-fill-target, c-delay-target, c-max-rate, c-min-rate and on-no-data-accessible</span>
  syncer { rate 100M; }
}

<span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Secci&#243;n para definir un recurso DRBD. M&#237;nimo tiene que declarar el protocolo</span>
<span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">y dos secciones on.</span>
resource r0 {
  <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Protocol configura cu&#225;ndo dar por terminada una escritura:</span>
  <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">C -&gt;  s&#237;ncrono, es decir cuando se ha completado en 1&#186; y 2&#186;</span>
  <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">B -&gt; as&#237;ncrono, se da por completo cuando acaba en 1&#186; y llega al buffer en 2&#186;</span>
  <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">A -&gt; as&#237;ncrono, se da por completo cuando acaba en 1&#186; y llega a buffer local TCP hacia 2&#186;</span>
  protocol C;

  <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Los handlers definen secuencias de ejecutables lanzables ante nueve</span>
  <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">eventos. El primero de la secuencia es mandar un correo sobre el</span>
  <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">problema concreto llamando a /usr/lib/drbd/notify-* (son enlaces a</span>
  <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">/usr/lib/drbd/notify.sh) y despu&#233;s intenta apagar o reiniciar por</span>
  <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">varios m&#233;todos (sysrq primero o scripts).</span>
  <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">pri-on-incon-degr, pri-lost-after-sb, pri-lost, fence-peer (formerly oudate-peer), local-io-error, initial-split-brain, split-brain, before-resync-target, after-resync-target</span>

  handlers {
    pri-on-incon-degr <span style="color: #ffa07a;">"/usr/lib/drbd/notify-pri-on-incon-degr.sh; /usr/lib/drbd/notify-emergency-reboot.sh; echo b &gt; /proc/sysrq-trigger ; reboot -f"</span>;
    pri-lost-after-sb <span style="color: #ffa07a;">"/usr/lib/drbd/notify-pri-lost-after-sb.sh; /usr/lib/drbd/notify-emergency-reboot.sh; echo b &gt; /proc/sysrq-trigger ; reboot -f"</span>;
    local-io-error <span style="color: #ffa07a;">"/usr/lib/drbd/notify-io-error.sh; /usr/lib/drbd/notify-emergency-shutdown.sh; echo o &gt; /proc/sysrq-trigger ; halt -f"</span>;
  }

  <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Opciones de ajuste de comportamiento. V&#233;ase man drbdsetup.</span>
  <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">wfc-timeout, degr-wfc-timeout, outdated-wfc-timeout, wait-after-sb, stacked-timeouts and become-primary-on </span>
  startup {
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">wfc = wait for connection timeout, pero prefijado con degr- implica </span>
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">un timeout espec&#237;fico para nodos degradados, que han sido reiniciados,</span>
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">creo.</span>
    degr-wfc-timeout 120;    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">2 minutes.</span>
  }

<span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Ante problemas de disco o de red (brain-split situations), nuestra pol&#237;tica</span>
<span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">ser&#225; simple: desconectar. Lo vemos en las dos siguientes secciones (junto a</span>
<span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">otras opciones):</span>

  <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Dispositivo de disco. Nunca &#250;sese cuando DRBD est&#225; actuando, tampoco dumpe2fs.</span>
  <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">V&#233;ase man drbdsetup</span>
  <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">on-io-error, size, fencing, use-bmbv, no-disk-barrier, no-disk-flushes, no-disk-drain, no-md-flushes, max-bio-bvecs</span>
  disk {
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Si el dispositivo de disco presenta erorres i/o</span>
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">detach, call-local-io-error o be pass_on</span>
    on-io-error   detach;
  }

  <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Ajustes relativos a la conectividad del cl&#250;ster:</span>
  <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">V&#233;ase man drbdsetup.</span>
  <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">sndbuf-size, rcvbuf-size, timeout, connect-int, ping-int, ping-timeout, max-buffers, max-epoch-size, ko-count, allow-two-primaries, cram-hmac-alg, shared-secret, after-sb-0pri, after-sb-1pri, after-sb-2pri, data-integrity-alg, no-tcp-cork, on-congestion, congestion-fill, congestion-extents</span>
  net {
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">The following lines are dedicated to handle split-brain situations</span>
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">(e.g., if one of the nodes fails).</span>
    <span style="color: #ff7f24;">#</span>
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">- Si se pierde el primario:</span>
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">P&#243;lizas: disconnect               desconecta, no sincronices</span>
    <span style="color: #ff7f24;">#          </span><span style="color: #ff7f24;">discard-younger-primary  sync desde el que era primario antes</span>
    <span style="color: #ff7f24;">#          </span><span style="color: #ff7f24;">discard-older-primary    sync desde el que se puso primario tras ca&#237;da del primario</span>
    <span style="color: #ff7f24;">#          </span><span style="color: #ff7f24;">discard-zero-changes     simplemente haz a uno el primario random o quien no escribises nada a&#250;n</span>
    <span style="color: #ff7f24;">#          </span><span style="color: #ff7f24;">discard-least-changes    haz primario a quien escribiese m&#225;s bloques</span>
    <span style="color: #ff7f24;">#          </span><span style="color: #ff7f24;">discard-node-&lt;nodename&gt;  sync de ese nodo</span>
    after-sb-0pri disconnect;
    <span style="color: #ff7f24;">#</span><span style="color: #ff7f24;">after-sb-0pri discard-younger-primary; #discard-zero-changes;</span>
    <span style="color: #ff7f24;">#</span><span style="color: #ff7f24;">after-sb-0pri discard-zero-changes; # If both secondary, just make one primary</span>
    <span style="color: #ff7f24;">#</span>
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">- Si uno queda primario y el otro no:</span>
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">P&#243;lizas: disconnect</span>
    <span style="color: #ff7f24;">#          </span><span style="color: #ff7f24;">consensus              descarta datos 2&#186; si la p&#243;liza anterior los destruir&#237;a, desconecta en otro caso</span>
    <span style="color: #ff7f24;">#          </span><span style="color: #ff7f24;">discard-secondary      descarta la versi&#243;n del 2&#186;</span>
    <span style="color: #ff7f24;">#          </span><span style="color: #ff7f24;">call-pri-lost-after-sb lo que decida la p&#243;liza anteior (0pri)</span>
    <span style="color: #ff7f24;">#          </span><span style="color: #ff7f24;">violently-as0p         0pri incluso si causa flapping. Dangerous.</span>
    after-sb-1pri disconnect;
    <span style="color: #ff7f24;">#</span><span style="color: #ff7f24;">after-sb-1pri discard-secondary; # If one primary, one is not, trust primary</span>
    <span style="color: #ff7f24;">#</span>
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">- Si quedan dos primarios:</span>
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">P&#243;liza: disconnect</span>
    <span style="color: #ff7f24;">#         </span><span style="color: #ff7f24;">call-pri-lost-after-sb  lo que decida la p&#243;liza anteior (0pri)</span>
    <span style="color: #ff7f24;">#         </span><span style="color: #ff7f24;">violently-as0p          0pri incluso si causa flapping. Dangerous.</span>
    after-sb-2pri disconnect;
    <span style="color: #ff7f24;">#</span><span style="color: #ff7f24;">after-sb-2pri call-pri-lost-after-sb; # If two primaries, make unchanged one secondary</span>

    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Si hay conflicto entre el resultado de la decisi&#243;n de resync y el rol actual:</span>
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">P&#243;liza: disconnect    </span>
    <span style="color: #ff7f24;">#         </span><span style="color: #ff7f24;">violently     permitido sincronizar con el primario. Dangerous.</span>
    <span style="color: #ff7f24;">#         </span><span style="color: #ff7f24;">call-pri-lost llama a ese script que reinicia la m&#225;quina = la pone de secundario</span>
    rr-conflict disconnect;

    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">He visto que hay alg&#250;n soporte de autenticaci&#243;n a trav&#233;s de una PSK,</span>
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">es obligatorio configurar cram-hmac-alg (cat /proc/crypto)</span>
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">y el shared-secret.</span>
    cram-hmac-alg sha1;
    shared-secret <span style="color: #ffa07a;">"mysecret"</span>;
  }

  <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Ajustes relativos a los procesos de sincronizaci&#243;n.</span>
  <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">V&#233;ase man drbdsetup.</span>
  syncer {
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">110M suele ser suficiente para un enlace Gigabit dedicado entre dos server</span>
    rate 100M;
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Extents (&#225;rea de almacenamiento contigua) alterados</span>
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">7-3843, default 127. Mayor n&#186; extent, menos updates pero m&#225;s largos.</span>
    al-extents 257;
  }

<span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Secci&#243;n para definir el dispositivo y su localizaci&#243;n. "on &lt;salida de hostname&gt;"</span>
  on d1 {
    device     /dev/drbd0;
    disk       /dev/sdb; <span style="color: #ff7f24;">#  </span><span style="color: #ff7f24;">Si lvm2 debajo, algo como /dev/mapper/VolGroup-drbd--demo</span>
    address    192.168.1.1:7788;
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">D&#243;nde guardar los metadatos; si flexible-meta-disk tiene flexibilidad por</span>
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">si usas LVM2, creo, aunque quiz&#225;s no es obligatorio seg&#250;n los ej que veo.</span>
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Interna para que use la misma device para metadatos, y la ponga al final.</span>
    flexible-meta-disk  internal;
  }

  on d2 {
    device     /dev/drbd0;
    disk       /dev/sdb;
   <span style="color: #ff7f24;">#</span><span style="color: #ff7f24;">... puesto que device y disk son iguales en d1 y d2, podr&#237;amos haberlas</span>
   <span style="color: #ff7f24;">#</span><span style="color: #ff7f24;">puesto una s&#243;la vez fuera de los bloques "on".</span>
    address    192.168.1.2:7788;
    flexible-meta-disk  internal;
  }
</pre>





<pre class="src src-sh">scp /etc/drbd.d/* d2:/etc/drbd.d/
</pre>


<p>
Alguien, antes de seguir, se quejaba de problemas de permisos
<a href="http://zeldor.biz/2011/07/drbd-masterslave-setup/">http://zeldor.biz/2011/07/drbd-masterslave-setup/</a>
</p>

<ul>
<li>Ininicialización. Creación de metadatos offline. En cada nodo:
</li>
</ul>


<p>
( si ya había un fs lo ponemos a cero:
  dd if=/dev/zero bs=512 count=512 of=/dev/sd?? )
</p>


<pre class="src src-sh">drbdadm create-md r0
invoke-rc.d drbd start <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">... &#233;sto hace que se ejecute, entre otros:</span>
 <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">drbdadm up r0 (en verdad 'all'), que a su vez es un atajo a ejecutar:</span>
 <span style="color: #ff7f24;">#  </span><span style="color: #ff7f24;">drbdadm attach r0   abre su backend de dispositivo de bloque /dev/sdb</span>
 <span style="color: #ff7f24;">#  </span><span style="color: #ff7f24;">drbdadm syncer r0   configura los par&#225;metros de sincronizaci&#243;n</span>
 <span style="color: #ff7f24;">#  </span><span style="color: #ff7f24;">drbdadm connect r0  se conecta al peer</span>
cat /proc/drbd
</pre>

<p>
&hellip; a su vez creo que ésto no hace a nadie primario porque el script ejecuta
    drbdadm &ndash;sh-b-pri all, que hace primario a quien ponga en la sección
    startup con become-primary-on, pero nosotros no tenemos puesto a nadie (éso
    lo haremos ahora a mano, y luego lo decidirá pacemaker).
</p>


<pre class="src src-sh">drbdadm role r0                <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">nos dir&#225; efectivamente en qu&#233; rol est&#225;</span>
drbd-overview                  <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">mini reporte del estado</span>

ls -l /dev/drbd0
</pre>

<ul>
<li>Primera sincronización. Forzamos a d1 como primario, lo cual comienza
  la replicación (de los metadatos que ya existen). Además ya podemos
  efectuar las acciones de creación el volumen LVM2 y/o el FS desde
  d1 al ser el primario.
</li>
</ul>




<pre class="src src-sh">drbdadm -- --overwrite-data-of-peer primary r0
drbdadm role r0
drbd-overview
</pre>

<p>
&hellip; comienza a sincronizar la metadata. Lo hará a la velocidad definida
    en el .conf: syncer { rate 100M; } &hellip; pero se puede transitoriamente
    cambiar online con:
</p>


<pre class="src src-sh">drbdsetup /dev/drbd0 syncer -r 10M <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">por ejem</span>
</pre>


<p>
Notas:
</p>
<ul>
<li>Salida de drbd-overview o cat /proc/drbd
   <a href="http://www.drbd.org/users-guide/ch-admin.html#s-check-status">http://www.drbd.org/users-guide/ch-admin.html#s-check-status</a>
</li>
</ul>




<pre class="src src-text">0: cs:StandAlone ro:Primary/Unknown ds:UpToDate/DUnknown   r----
   ns:0 nr:0 dw:0 dr:148 al:0 bm:0 lo:0 pe:0 ua:0 ap:0 ep:1 wo:b oos:73728
</pre>

<ul>
<li>cs = connection state. Hay muchos
<ul>
<li>Unconnected: estado transitorio de intento de conexión, puede llevar a:
</li>
<li>WFconnection: esperando a que el peer aparezca
</li>
<li>WFreportparams: cnx a nivel de tcp, esperando pqt a nivel de app
</li>
<li>Connected.
</li>
<li>Disconnecting: estado transitorio, desemboca en:
</li>
<li>Standalone, sin red por cualquier razón, incluída la descnx manual.
</li>
</ul>

</li>
<li>ro = roles. Primary, Secondary, Unknown(se ref siempre al peer, que está desconn)
</li>
<li>ds = disk states
<ul>
<li>attatching: estado transitorio mientras que lee la metadata
</li>
<li>consistent: datos consistentes, cuando te conectes se decidirá:
</li>
<li>uptodate
</li>
<li>outdated
</li>
<li>dunknown: sin conexión con el peer.
</li>
<li>inconsistent
</li>
<li>diskless
</li>
<li>attatching
</li>
<li>failed
</li>
</ul>

</li>
<li>i/o indicators:
<ul>
<li>r(running), s(suspended)
</li>
<li>otros (cada guión es otro)
</li>
</ul>

</li>
<li>performance indicators (2ª línea)
<ul>
<li>ns/nr: network send/receive
</li>
<li>dw/dr: disk w/r
</li>
<li>oos: nº kB out of sync
</li>
</ul>

</li>
</ul>



<ul>
<li>Si no tuviésemos a pacemaker y sus RA, para moverlo de d1 a d2 habría que:
<ul>
<li>En d1:
</li>
</ul>

</li>
</ul>




<pre class="src src-sh">umount /mnt/drbdr0/lamp1             <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">desmontarlo de donde lo est&#233;</span>
vgchange -aen datavg                 <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">desactivar el volumen LVM2 (sgt apartado)</span>
drbdadm secondary r0                 <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">ya d1 no puede ser primario drbd</span>
drbdadm role r0                      <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Comprobamos que cambi&#243; el role</span>
</pre>

<ul>
<li>En d2:
</li>
</ul>




<pre class="src src-sh">drbdadm primary r0                   <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">d2 debe ser primario drbd</span>
drbdadm role r0                      <span style="color: #ff7f24;">#</span>
vgchange -aey datavg                 <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">activamos y por fin montamos:</span>
mount /dev/datavg/lamp1 /mnt/drbdr0/lamp1
</pre>

<ul>
<li>Levantar manualmente y poner en modo primario o secundario:
</li>
</ul>




<pre class="src src-sh">drbdadm attach  r0
drbdadm syncer  r0
drbdadm connect r0  <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">... estos 3 equivalen a drbdadm up r0</span>
drbdadm primary r0  <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">y en el otro  drbdadm secondary r0</span>
</pre>

<ul>
<li>Split-Brain Solution:
   <a href="http://www.drbd.org/users-guide/s-resolve-split-brain.html">http://www.drbd.org/users-guide/s-resolve-split-brain.html</a>
</li>
</ul>




<pre class="src src-text">Con drbdadm role r0 vemos: primary/unknown y secondary/unknown
                           luego se han desconectado por algo.

En syslog vemos:   block drbd0: self F25FFA5E40EBB804:...
                   block drbd0: peer F153F8E499CEDFEE:...
                   block drbd0: uuid_compare()=100 by rule 90
                   block drbd0: Split-Brain detected, dropping connection!
                   ...
</pre>

<p>
   Con drbd-overview vemos: StandAlone/Standalone o WFConnection/Standalone lo
   que implica que o ambos han descubierto a la vez la situación y
   desconectado, o uno lo hizo antes.
</p><ul>
<li>Eliges el nodo víctima (el que perderá sus modificaciones; por ejemplo el
     que estaba en modo secundario cuando sucedió el problema, pienso). Ahí:
</li>
</ul>




<pre class="src src-sh">drbdadm disconnect r0
drbdadm secondary r0
drbdadm -- --discard-my-data connect r0
</pre>

<ul>
<li>El otro nodo es el sobreviviente, ahí haces:
</li>
</ul>




<pre class="src src-sh">drbdadm connect r0
</pre>

<p>
     &hellip; el valor de cs (conection state) pasará a SyncTarget y luego a
     Connected.
</p>

<p>
--
</p>
<p>
&hellip; ahora podemos formatear "mkfs.ext4 /dev/drbd0" o usar LVM2 primero:
</p>
</div>

</div>

<div id="outline-container-2-3" class="outline-3">
<h3 id="sec-2-3"><span class="section-number-3">2.3</span> LVM2</h3>
<div class="outline-text-3" id="text-2-3">




<pre class="src src-text">apt-get install lvm2
</pre>


<p>
Si no instalo además las extensiones de clúster clvm, cada operación de
administración LVM2 que (previa desactivación de lvm2) haga en un host tendré
que realizarla también en el otro para que la metadata de volúmenes esté
sincronizada. Un ejem de instalación con clvm lo pongo en el ejemplo M-M.
</p>
<p>
Realmente, usar LVM hace la gestión muy parecida a la de AFS: reservar un
espacio de /vicepa para crear un volumen que montar donde hiciera falta, es
ahora reservar un espacio de /dev/drbd0 para crear un volumen que montar donde
haga falta. Si en AFS se hacía un volumen raiz afs y un volumen raiz de nuestra
celda, aquí / nos provee el raiz del servidor y /mnt/drbdr0 sería donde
montamos el raiz del recurso DRBD.
</p>
<ul>
<li>Seguimos en nodo primario de drbd, d1:
</li>
</ul>




<pre class="src src-sh">drbdadm role r0       <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">(debe estar en Primary o el siguiente comando fallar&#225;</span>
                      <span style="color: #ff7f24;">#  </span><span style="color: #ff7f24;">incluso aunque /dev/drbd0 exista )</span>
pvcreate /dev/drbd0   <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">inicializo ese disco para usarlo con LVM2</span>
pvdisplay             <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">para que todo vaya bien, el UUID que muestre</span>
                      <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">este pvdisplay debe ser el mismo que el que</span>
                      <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">se muestre en otros nodos (podr&#237;amos ir ahora</span>
                      <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">quiz&#225;s a d2 y comprobarlo).</span>
</pre>

<ul>
<li>En todos:
</li>
</ul>




<pre class="src src-sh">apt-get install lvm2
vim /etc/lvm/lvm.conf
 filter = [ <span style="color: #ffa07a;">"a|drbd.*|"</span>, <span style="color: #ffa07a;">"r|.*|"</span> ]
 write_cache_state = 0
</pre>

<p>
&hellip; el filtro (array de exp regulares para 'a'ceptar o 'r'echazar dispositivos)
    sirve para rechazar dispositivos que estén inicializados con pvcreate, y se
    aceptan los que casen con las regexp precedidas con 'a' o los que no casen
    con las regexp 'a' o 'r'. pvdisplay y vgscan permite comprobar el filtro en
    cli. Supongo que con este filtro puedes avisar si se rompe algún disco etc.
</p>
<p>
&hellip; write_cache_state deshabilita la cache lvm, además borramos ésta para
    evitar que, por una activación a destiempo (no master) o lo que sea se
    utilice una información que a lo mejor ha cambiado.
</p>


<pre class="src src-sh">rm -rf /etc/lvm/cache/.cache
</pre>



<ul>
<li>De nuevo en el drbd primay, d1:
</li>
</ul>




<pre class="src src-sh">vgcreate data1vg /dev/drbd0
lvcreate --size 10M -n root.r0 data1vg
lvcreate --size 15M -n r0.etc data1vg
lvcreate --size 50M -n r0.varlibmysql data1vg
lvcreate --size 10M -n r0.varwww data1vg

vgchange -aey data1vg                   <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">"-a" activa el volumen para montarlo</span>
                                        <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">si no permanece irreconocido por</span>
                                        <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">ese subsistema del kernel.</span>
                                        <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">"-ae" avisa de que si se usa clustering</span>
                                        <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">se ha de activar de forma exclusiva</span>
                                        <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">en un s&#243;lo (el que sea pero uno,</span>
                                        <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">entiendo) mientras que "-al" lo</span>
                                        <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">activar&#237;a s&#243;lo localmente.</span>
                                        <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">La "y" es de yes, si fuese n ser&#237;a no.</span>
                                        <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">No s&#233; hasta qu&#233; punto es necesario</span>
                                        <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">en un despliegue no master-master,</span>
                                        <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">pero lo he visto en tuto master-slave</span>
</pre>

<p>
&hellip; entiendo que al activar el volumen, lvm2 busca volúmenes en los
    dispositivos indicados por el filtro del lvm.conf, lo que encuentre lo deja
    escrito en su .cache y crea los ficheros bajo /dev.  Ésto implica que en el
    otro nodo d2, sólo apareceran esos ficheros bajo /dev cuando se active ese
    volumen a mano o por pacemaker. La cache borrada no se reconstruye por la
    opción write<sub>cache</sub><sub>state</sub>=0. Así, aunque la replicación drbd se lleve a cabo
    ok, no veremos aún en d2 los ficheros bajo /dev.
</p>


<pre class="src src-sh">mkfs.ext4 /dev/data1vg/root.r0
mkfs.ext4 /dev/data1vg/r0.etc
mkfs.ext4 /dev/data1vg/r0.varlibmysql
mkfs.ext4 /dev/data1vg/r0.varwww
</pre>


<pre class="src src-sh"><span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">ejem de extender alguna:</span>
 lvextend --size +1M /dev/mapper/data1vg-r0.varwww <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">la extiende 1M</span>
 e2fsck -f /dev/data1vg/r0.varwww         <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">la checkeamos y extendemos tb el fs</span>
 resize2fs /dev/data1vg/r0.varwww
 <span style="color: #ff7f24;">#</span><span style="color: #ff7f24;">lvremove /dev/data1vg/&lt;bla&gt;...para borrarlo...</span>


drbdadm dstate r0    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">otra forma de mostrar estado</span>
</pre>

<ul>
<li>En todos:
</li>
</ul>




<pre class="src src-sh">mkdir -p /mnt/drbdr0
</pre>

</div>

</div>

<div id="outline-container-2-4" class="outline-3">
<h3 id="sec-2-4"><span class="section-number-3">2.4</span> Apache2, MySQL, Phpmyadmin</h3>
<div class="outline-text-3" id="text-2-4">

<ul>
<li>En todos:
</li>
</ul>




<pre class="src src-sh">apt-get install mysql-server apache2 phpmyadmin
</pre>

<p>
 &hellip; responder con los defaults.
</p>


<pre class="src src-text">mysql root pw: root
phpmyadmin sobre web server: apache2
phpmyadmin user: admin
                 para luego acceder a http://localhost/phpmyadmin/setup/index.php
phpmyadmin user pw: admin
                    ... user y pass los guarda en /etc/phpmyadmin/htpasswd.setup
phpmyadmin access db: unix socket
phpmyadmin db username: phpmyadmin
phpmyadmin db username: phpmyadmin
phpmyadmin db: phpmyadmin
</pre>


<p>
<a href="http://dev.mysql.com/doc/refman/5.1/en/ha-drbd.html">http://dev.mysql.com/doc/refman/5.1/en/ha-drbd.html</a>
</p>


<pre class="src src-sh">invoke-rc.d mysql stop
invoke-rc.d apache2 stop
update-rc.d mysql disable
update-rc.d apache2 disable

a2enmod status
</pre>

<ul>
<li>En el drbd primario, d1:
</li>
</ul>




<pre class="src src-sh">mkdir -p /mnt/drbdr0
mount /dev/data1vg/root.r0 /mnt/drbdr0
mkdir -p /mnt/drbdr0/etc
mount /dev/data1vg/r0.etc /mnt/drbdr0/etc
mkdir -p /mnt/drbdr0/etc/mysql
mkdir -p /mnt/drbdr0/etc/apache2
mkdir -p /mnt/drbdr0/etc/phpmyadmin
mkdir -p /mnt/drbdr0/var/www
mount /dev/data1vg/r0.varwww /mnt/drbdr0/var/www
mkdir -p /mnt/drbdr0/var/lib/mysql
mount /dev/data1vg/r0.varlibmysql /mnt/drbdr0/var/lib/mysql



cp -aR /var/lib/mysql/* /mnt/drbdr0/var/lib/mysql
cp /etc/mysql/my.cnf /etc/mysql/my.cnf_predrbd
cp -aR /etc/mysql/* /mnt/drbdr0/etc/mysql/
cat &lt;&lt;EOF &gt; /mnt/drbdr0/etc/mysql/conf.d/fx-listen.cnf
<span style="color: #ffff00; font-weight: bold;">####fx:</span>
<span style="color: #ffff00; font-weight: bold;">[mysqld]</span>
<span style="color: #ffff00; font-weight: bold;">bind-address            = 0.0.0.0</span>
<span style="color: #ffff00; font-weight: bold;">####endfx</span>
<span style="color: #ffff00; font-weight: bold;">EOF</span>

chown -R mysql:mysql /mnt/drbdr0/var/lib/mysql
</pre>

<p>
&hellip; ojo, está claro que en todos los ordenadores el uid y gid de mysql debe ser
    el mismo.
</p>




<pre class="src src-sh">cp -Ra /etc/apache2/* /mnt/drbdr0/etc/apache2
cp -Ra /var/www/* /mnt/drbdr0/var/www/
cp -Ra /etc/phpmyadmin/* /mnt/drbdr0/etc/phpmyadmin/



umount /mnt/drbdr0/var/lib/mysql
umount /mnt/drbdr0/var/www
umount /mnt/drbdr0/etc
umount /mnt/drbdr0
</pre>



<p>
Ahora podríamos corregir los paths en los .conf (que deberían ser locales),
algo así&hellip;
</p>


<pre class="src src-text">***(no h&#225;gase)***
vim /etc/mysql/my.cnf
 datadir = /mnt/drbdr0/var/lib/mysql
vim /etc/apache2/sites-available/default
 DocumentRoot /mnt/drbdr0/var/www/default
</pre>


<p>
&hellip; pero mejor hacemos los ficheros originales links a los destinos en drbd y
    ponemos esos links también bajo gestión de pacemaker gracias a drbdlinks o
    el RA "links":
</p>
</div>

</div>

<div id="outline-container-2-5" class="outline-3">
<h3 id="sec-2-5"><span class="section-number-3">2.5</span> Gestión de enlaces</h3>
<div class="outline-text-3" id="text-2-5">


</div>

<div id="outline-container-2-5-1" class="outline-4">
<h4 id="sec-2-5-1"><span class="section-number-4">2.5.1</span> Una posibilidad es el sw drbdlinks. Ejem para LAMP:</h4>
<div class="outline-text-4" id="text-2-5-1">

<p>  This can be used to manage links or mount-binds for /etc/apache,
  /var/lib/{pg,my}sql etc that need to appear as if they are local to the
  system when running applications after a drbd shared partition has been
  mounted.
</p>
<p>
  When run with "start" as the mode, drbdlinks will rename the existing
  files/directories, and then make symbolic links into the DRBD partition.
  "stop" does the reverse.  By default, the rename appends ".drbdlinks" to the
  name, but this can be overridden.
</p>
<p>
  The advantage over creating static symbolic links is that package updates
  often require that directories point at real files, so updates can often fail
  if you do not have the shared storage mounted.
</p>
<p>
  drbdlinks also supports multiple instances of links, in the case of
  active/active clusters. For example, if you have MySQL running in one
  resource group, and Apache running in another, you can use the "-c
  anothercnf.conf" switch to specify a configuration file for each resource
  group.
</p>
<p>
  <a href="http://www.tummy.com/Community/software/drbdlinks/">http://www.tummy.com/Community/software/drbdlinks/</a>
</p>
<ul>
<li>En d1 para luego usar scp a d2:
</li>
</ul>




<pre class="src src-sh">vim /etc/drbdlinks.conf  <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">es autoexplicativ</span>
</pre>


<pre class="src src-sh">mountpoint(<span style="color: #ffa07a;">'/mnt/drbdr0/'</span>)
link(<span style="color: #ffa07a;">'/var/www'</span>,<span style="color: #ffa07a;">'/mnt/drbdr0/var/www'</span>)
link(<span style="color: #ffa07a;">'/etc/apache2'</span>,<span style="color: #ffa07a;">'/mnt/drbdr0/etc/apache2'</span>)
link(<span style="color: #ffa07a;">'/var/lib/mysql'</span>,<span style="color: #ffa07a;">'/mnt/drbdr0/var/lib/mysql'</span>)
link(<span style="color: #ffa07a;">'/etc/mysql'</span>,<span style="color: #ffa07a;">'/mnt/drbdr0/etc/mysql'</span>)
link(<span style="color: #ffa07a;">'/etc/phpmyadmin'</span>,<span style="color: #ffa07a;">'/mnt/drbdr0/etc/phpmyadmin'</span>)
<span style="color: #ff7f24;">#</span><span style="color: #ff7f24;">link('...</span>
<span style="color: #ff7f24;">#</span><span style="color: #ff7f24;">usebindmount(1)</span>
<span style="color: #ff7f24;">#</span><span style="color: #ff7f24;">link('ahora se usar&#225; mount bind....</span>
<span style="color: #ff7f24;">#</span><span style="color: #ff7f24;">usebindmount(0)</span>
<span style="color: #ff7f24;">#</span><span style="color: #ff7f24;">link('ya se volver&#225; a usar ln -s ...</span>
</pre>

<p>
Queda para después configurar el RA de drbdlinks para pacemaker.
</p>
<p>
scp /etc/drbdlinks.conf d2:/etc/drbdlinks.conf
</p>
</div>

</div>

<div id="outline-container-2-5-2" class="outline-4">
<h4 id="sec-2-5-2"><span class="section-number-4">2.5.2</span> Alternativa: utilizar el RA SYMLINK de pacemaker luego.</h4>
<div class="outline-text-4" id="text-2-5-2">

<p>   <a href="http://www.sebastien-han.fr/blog/2012/11/01/manage-your-symlin-with-pacemaker/">http://www.sebastien-han.fr/blog/2012/11/01/manage-your-symlin-with-pacemaker/</a>
</p>
<p>
   What does the RA do? Given a configured rule "/path/file, symlink, suffix"
   in pacemaker, it basically it checks if a file called &lt;file&gt; in &lt;path&gt;
   exists, if it does it simply rename &lt;file&gt;.&lt;suffix&gt; after the symlink is
   created. If it does not it directly creates &lt;symlink&gt;.
</p>
<p>
   Funciona igual, pero no necesita scripts python, es autocontenido en
   pacemaker.
</p>
</div>
</div>

</div>

<div id="outline-container-2-6" class="outline-3">
<h3 id="sec-2-6"><span class="section-number-3">2.6</span> Corosync</h3>
<div class="outline-text-3" id="text-2-6">

<ul>
<li>En todos los nodos:
</li>
</ul>




<pre class="src src-sh">apt-get -t squeeze-backports install corosync
</pre>

<p>
&hellip; y NO update-rc.d corosync disable porque es precisamente corosync el que
    levanta a pacemaker y por tanto al resto.
</p>
<ul>
<li>Configuración de corosync. En d1:
</li>
</ul>




<pre class="src src-sh">vim /etc/corosync/corosync.conf
</pre>


<pre class="src src-sh"><span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Configuration options for the totem protocol.</span>
totem {
        <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Versi&#243;n del fichero de configuraci&#243;n.</span>
        version: 2

        <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">How long before declaring a token lost (ms)</span>
        token: 3000

        <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">How many token retransmits before forming a new configuration</span>
        token_retransmits_before_loss_const: 10

        <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">How long to wait for join messages in the membership protocol (ms)</span>
        join: 60

        <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">How long to wait for consensus to be achieved before starting a new round of membership configuration (ms)</span>
        consensus: 3600

        <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Turn off the virtual synchrony filter</span>
        vsftype: none

        <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Number of messages that may be sent by one processor on receipt of the token</span>
        max_messages: 20

        <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Limit generated nodeids to 31-bits (positive signed integers)</span>
        clear_node_high_bit: yes

        <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Disable encryption</span>
        secauth: off

        <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">How many threads to use for encryption/decryption</span>
        threads: 0

        <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Optionally assign a fixed node id (integer)</span>
        <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">nodeid: 1234</span>

        <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">This specifies the mode of redundant ring, which may be none, active, or passive.</span>
        rrp_mode: none

        interface {
                <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">ID de esta iface para el Redundant Ring Protocol. Empieza en 0.</span>
                ringnumber: 0
                <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Iface con IPv4 o IPv6:</span>
                bindnetaddr: 192.168.0.0
                <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Avoid 224.x.x.x because this is a "config" multicast address.</span>
                <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">http://goo.gl/fn0MW</span>
                <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">http://www.cisco.com/en/US/tech/tk828/technologies_white_paper09186a00802d4643.shtml</span>
                <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">TODO: comprobar si esa IP mcast se pone como iface virt de la</span>
                <span style="color: #ff7f24;">#       </span><span style="color: #ff7f24;">de la declarada para el bindnetaddr anterior:</span>
                mcastaddr: 226.94.1.1
                <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Puerto UDP</span>
                mcastport: 5405
                <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Si us&#225;semos "transport udpu" y no us&#225;semos mcastaddr, creo que</span>
                <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">habr&#237;a que crear secciones:</span>
                <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">member {</span>
                <span style="color: #ff7f24;">#        </span><span style="color: #ff7f24;">memberaddr: 10.0.0.2</span>
                <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">}</span>
                <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">member {</span>
                <span style="color: #ff7f24;">#        </span><span style="color: #ff7f24;">memberaddr: 10.0.0.3</span>
                <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">}</span>
                <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">...</span>
        }
}

amf {  
        mode: disabled
}

service {
        <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Load the Pacemaker Cluster Resource Manager</span>
        ver:       0
        name:      pacemaker
        <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">TODO: implica &#233;so que pacemaker no trae script init?</span>
}

aisexec {
        user:   root
        group:  root
}

<span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Configuration options for logging</span>
logging {
        fileline: off
        to_stderr: yes
        to_logfile: yes
        logfile: /var/log/corosync/corosync.log
        to_syslog: yes
        syslog_facility: daemon
        debug: off
        timestamp: on
        logger_subsys {
                subsys: AMF
                debug: off
                tags: enter|leave|trace1|trace2|trace3|trace4|trace6
        }
}
</pre>



<pre class="src src-sh">corosync-keygen <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">http://sublimated.wordpress.com/2007/08/28/not-enough-random-bytes-available/</span>

scp /etc/corosync/authkey       d2:/etc/corosync
scp /etc/corosync/corosync.conf d2:/etc/corosync
</pre>

<ul>
<li>En todos:
</li>
</ul>




<pre class="src src-sh"><span style="color: #b0c4de;">echo</span> &#8220;<span style="color: #eedd82;">START</span>=yes&#8221; &gt; /etc/default/corosync
invoke-rc.d corosync start

tail -f /var/log/corosync/corosync.log
</pre>


</div>

</div>

<div id="outline-container-2-7" class="outline-3">
<h3 id="sec-2-7"><span class="section-number-3">2.7</span> Pacemaker</h3>
<div class="outline-text-3" id="text-2-7">


</div>

<div id="outline-container-2-7-1" class="outline-4">
<h4 id="sec-2-7-1"><span class="section-number-4">2.7.1</span> instalación</h4>
<div class="outline-text-4" id="text-2-7-1">

<ul>
<li>En todos:
</li>
</ul>




<pre class="src src-sh">apt-get -t squeeze-backports install pacemaker curl
</pre>

<p>
&hellip; 30 MB con deps (perl libs&hellip;)
&hellip; creo que curl/wget es necesario para consultar mod_status de apache
</p>

<p>
  Pacemaker debe ser levantado por corosync:
</p>


<pre class="src src-sh">invoke-rc.d corosync stop
invoke-rc.d corosync start
ps aux | grep pace

ls /var/lib/heartbeat

pacemakerd --features
pacemakerd --help
crm_mon --help
crm_mon --daemonize --as-html /var/www/crm_mon.html

crm --help
crm configure show
crm configure show xml
crm configure shownode &lt;maquina1&gt;
crm_verify -L

crm ra classes
crm ra list
</pre>




<ul>
<li>Previo: Creo que el RA de apache tiene un problema. En todos:
</li>
</ul>




<pre class="src src-sh">vim /usr/lib/ocf/resource.d/heartbeat/apache
</pre>


<pre class="src src-sh"><span style="color: #ff7f24;">#</span><span style="color: #ff7f24;">!/bin/</span><span style="color: #00ffff;">bash</span><span style="color: #ff7f24;"> -x</span>
...
: ${<span style="color: #eedd82;">OCF_FUNCTIONS_DIR</span>=${<span style="color: #eedd82;">OCF_ROOT</span>}/lib/heartbeat}
<span style="color: #ff7f24;">####</span><span style="color: #ff7f24;">fx:</span>
(
<span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">&#201;sto es lo que arregla el problema, de hecho este RA intenta buscarlo</span>
<span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">en el file que das de config (/etc/apache2/apache2.conf) pero ah&#237; no</span>
<span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">est&#225;, sino en otro que se incluye (/etc/apache2/ports.conf).</span>
<span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Espero que en testing &#233;sto est&#233; arreglado.</span>
<span style="color: #eedd82;">Listen</span>=localhost:80
<span style="color: #ff7f24;">#</span>
<span style="color: #ff7f24;">#</span><span style="color: #ff7f24;">Si el script lo ejecuto a mano para inspeccionar el problema, tengo</span>
<span style="color: #ff7f24;">#</span><span style="color: #ff7f24;">que definir OCF_ROOT aqu&#237;, seg&#250;n veo en la traza. As&#237;:</span>
<span style="color: #ff7f24;">#</span><span style="color: #ff7f24;">if test -z "${OCF_ROOT}"; then</span>
<span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">OCF_ROOT=/usr/lib/ocf/</span>
<span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">OCF_FUNCTIONS_DIR=${OCF_ROOT}/lib/heartbeat</span>
<span style="color: #ff7f24;">#</span><span style="color: #ff7f24;">fi</span>
<span style="color: #ff7f24;">#</span><span style="color: #ff7f24;">... otra posibilidad es utilizar el script ocf-tester</span>
<span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">ocf-tester -n Apache2 -o configfile=/etc/apache2/apache2.conf -o ... /usr/lib/ocf/resource.d/heartbeat/apache</span>
<span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">... ocf-tester necesita a xmllint (paquete libxml2-utils)</span>
<span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">man ocf_heartbeat_apache</span>
<span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">man ocf-tester</span>
<span style="color: #ff7f24;">####</span><span style="color: #ff7f24;">endfx</span>
...
<span style="color: #ff7f24;">####</span><span style="color: #ff7f24;">fx:</span>
)2&gt;/tmp/a2e
<span style="color: #ff7f24;">####</span><span style="color: #ff7f24;">endfx</span>
</pre>

</div>

</div>

<div id="outline-container-2-7-2" class="outline-4">
<h4 id="sec-2-7-2"><span class="section-number-4">2.7.2</span> configuración general</h4>
<div class="outline-text-4" id="text-2-7-2">

<ul>
<li>En d1 por ejemplo:
</li>
</ul>




<pre class="src src-sh">crm_mon --one-shot -V <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">cluster state. equiv a "crm_mon -1f -V"</span>
</pre>

<p>
 Si no hay más de dos nodos, modificamos la propiedad general:
</p>


<pre class="src src-sh">crm configure property no-quorum-policy=<span style="color: #ffa07a;">"ignore"</span>
crm configure property expected-quorum-votes=<span style="color: #ffa07a;">"1"</span>
</pre>

</div>

</div>

<div id="outline-container-2-7-3" class="outline-4">
<h4 id="sec-2-7-3"><span class="section-number-4">2.7.3</span> STONITH</h4>
<div class="outline-text-4" id="text-2-7-3">

<p>Nota: en clústeres sin concurrencias de escrituras, se podían usar mecanismos
de fencing a nivel de recurso especiales tal como sfex. En este sistema se
reserva una pequeña partición que almacena locks, los cuales han de reservarse
para usar el FS; entonces sfex (que se configura con su primitive en pacemaker)
asegura que sólo haya un lock concedido, además de renovarlos para evitar
inaniciones etc.
 <a href="https://www.suse.com/documentation/sle_ha/book_sleha/data/sec_ha_storageprotection_exstoract.html">https://www.suse.com/documentation/sle_ha/book_sleha/data/sec_ha_storageprotection_exstoract.html</a>
</p>


<p>
 TODO: discutir deshabilitar stonith. Es necesario para garantizar integridad
       de datos con w compartidas sobre un único o replicado medio, pero
       necesita una parte hw que no tengo. No puede ser, avisan, un power
       switch que esté conectado a la misma fuente de pontencia q los nodos; ni
       pueden depender de un mensaje ssh desde del nodo para saber si está activo.
       Lo + imp del hw SONITH es que logre diferenciar si el nodo está inactivo
       o activo pero sin conectividad.
       <a href="http://clusterlabs.org/doc/crm_fencing.html">http://clusterlabs.org/doc/crm_fencing.html</a>
</p>
<p>
        crm configure property stonith-enabled="false"
</p>
<p>
       Si sí disponemos de hw (o usamos simuladores), obtenemos sus param y
       conf un recurso:
</p>


<pre class="src src-sh">stonith_admin --list-installed
stonith_admin --metadata --agent &lt;type_seg&#250;n_cmd_anterior&gt;
</pre>


<pre class="src src-text">crm configure primitive ... v&#233;ase man stonithd para usar o no estos param:
   1. pcmk_host_map
   2. pcmk_host_list, pcmk_host_check
   3. pcmk_host_argument
</pre>

<p>
        Ejem con driver stonith:external/ssh, sólo para testing:
</p>


<pre class="src src-text">primitive st_ssh stonith:external/ssh params hostlist="node1 node2"
clone fencing st-ssh
</pre>

<p>
        A veces, con STONITH, es preferible no usar clones y, en su lugar,
        crear una primitiva para cada lugar (y usar location para que el
        primitive con el proceso que monitoriza el nodo A, no se ejecute en
        nodo A)
</p>
<p>
          "In this example, location constraints are used for the following
           reason: There is always a certain probability that the STONITH
           operation is going to fail. Therefore, a STONITH operation on the
           node which is the executioner as well is not reliable. If the node
           is reset, it cannot send the notification about the fencing
           operation outcome. The only way to do that is to assume that the
           operation is going to succeed and send the notification
           beforehand. But if the operation fails, problems could
           arise. Therefore, by convention, stonithd refuses to kill its host"
</p>
<p>
          "The choice of which construct to use for configuration depends on
           several factors: nature of the fencing device, number of hosts
           managed by the device, number of cluster nodes, or personal
           preference".
</p>
<p>
        Ejem con driver stonith:null (sólo para testing) y location:
</p>


<pre class="src src-text">primitive stonithnull1 stonith:null params hostlist="d1"
primitive stonithnull2 stonith:null params hostlist="d2"
location l-st-node1 stonithnull1 -inf: d1
location l-st-node2 stonithnull2 -inf: d2
</pre>

<p>
        Ejem IBM RSA lights-out device (driver stonith:external/ibmrsa-telnet):
</p>


<pre class="src src-text">primitive st-ibmrsa-1 stonith:external/ibmrsa-telnet params nodename=d1 ipaddr=192.168.0.1 userid=USERID passwd=PASSW0RD
primitive st-ibmrsa-2 stonith:external/ibmrsa-telnet params nodename=d2 ipaddr=192.168.0.2 userid=USERID passwd=PASSW0RD
# st-ibmrsa-1 can run anywhere but on d1
location l-st-node1 st-ibmrsa-1 -inf: d1
# st-ibmrsa-2 can run anywhere but on d2
location l-st-node2 st-ibmrsa-2 -inf: d2
</pre>


<p>
       Test:
</p>


<pre class="src src-sh">stonith_admin --reboot nodename (previo: mejor parar antes cluster en esa m&#225;quina)
</pre>

<p>
       Tips: <a href="http://www.hastexo.com/resources/hints-and-kinks">http://www.hastexo.com/resources/hints-and-kinks</a>
</p>
</div>

</div>

<div id="outline-container-2-7-4" class="outline-4">
<h4 id="sec-2-7-4"><span class="section-number-4">2.7.4</span> Configuración de Pacemaker para failover:</h4>
<div class="outline-text-4" id="text-2-7-4">



<p>
  nota: drbd8-utils trae el RA de Linbit que usar, no el viejo proveído por
        heartbeat.
</p>
<p>
 # Para prevenir que un recurso se mueva tras la recuperación de su
   localización anterior o por otros motivos. Por defecto pacemaker cuenta que
   esa movilidad tiene coste cero y por tanto los mueve aplicando criterios de
   balanceo de carga u otros. El stickiness se puede configurar por RA o con un
   default que suele ser suficiente:
</p>


<pre class="src src-sh">crm configure rsc_defaults resource-stickiness=100
</pre>

<p>
DRBD no se puede desactivar en el nodo no activo, sino que debe
quedar activo pero en estado slave; sin embargo los demás sí se desactivan en
el nodo no activo. Por tanto, sólo DRBD tiene asociado un clon, y además éste
es de tipo multiestado, m-s. Lo vemos ahora:
</p>



<pre class="src src-text">crm
 cib new conf20130407
  configure
   # IP flotante:
   primitive ClusterIP1 ocf:heartbeat:IPaddr2 params ip="10.0.0.1" cidr_netmask="32" op monitor interval="30s"
   # Storage:
    # DRBD level: http://www.drbd.org/users-guide/ch-pacemaker.html
   primitive DRBDr0 ocf:linbit:drbd params drbd_resource="r0" op monitor interval="29s" role="Master" op monitor interval="31s" role="Slave"
      # ms es un multistate resource, un clon con dos modos de
      # funcionamiento, master y slave:
   ms DRBDr0_Clone DRBDr0 meta master-max="1" master-node-max="1" clone-max="2" clone-node-max="1" notify="true"
    # LVM2 level: lvm2 encima de drbd
    #             (al usar clvm en despliegues m-m, se a&#241;ade un RA adicional a &#233;sto)
   primitive LVMvg1 ocf:heartbeat:LVM params volgrpname="data1vg" exclusive="true" op start interval="0" timeout="30" op stop interval="0" timeout="30"
    # FS level:
    # ... si DRBD estuviese tambi&#233;n sobre LVM2, device ser&#237;a m&#225;s algo como
    #     "/dev/drbd/by-res/wwwdata"; en mi s&#243;lo hay LVM sobre DRBD:
   primitive FSroot_r0 ocf:heartbeat:Filesystem params device="/dev/data1vg/root.r0" directory="/mnt/drbdr0" fstype="ext4"
   primitive FSr0_etc ocf:heartbeat:Filesystem params device="/dev/data1vg/r0.etc" directory="/mnt/drbdr0/etc" fstype="ext4"
   primitive FSr0_varwww ocf:heartbeat:Filesystem params device="/dev/data1vg/r0.varwww" directory="/mnt/drbdr0/var/www" fstype="ext4"
   primitive FSr0_varlibmysql ocf:heartbeat:Filesystem params device="/dev/data1vg/r0.varlibmysql" directory="/mnt/drbdr0/var/lib/mysql" fstype="ext4"
   # DRBDlinks:
   primitive LINKS_Drbdlinks heartbeat:drbdlinks op monitor interval="10s" meta target-role="Started"
     # Alternativa: RA SYSLINKS:
     # primitive p_sym_leseb ocf:heartbeat:symlink \
     #   params target="/mnt/drbd/util1/leseb" link="/etc/leseb" backup_suffix=".active"
   # MySQL:
   primitive MySQL ocf:heartbeat:mysql
   # Apache2: (necesita /server-status de mod_status operativo)
   primitive Apache2 ocf:heartbeat:apache params configfile=/etc/apache2/apache2.conf op monitor interval=1min statusurl="http://127.0.0.1/server-status/"
   #primitive Apache2 ocf:heartbeat:apache params configfile="/etc/apache2/apache2.conf" options="-DSSL" operations $id="WebSite-operations" op start interval="0" timeout="40" op stop interval="0" timeout="60" op monitor interval="60" timeout="120" start-delay="0" statusurl="http://127.0.0.1/server-status/" meta target-role="Started"

   # Location, Colocation, Order: (Scratch guide)
   colocation LVMvg1_on_DRBDr0 inf: LVMvg1 DRBDr0_Clone:Master
   order LVMvg1-after-DRBDr0 inf: DRBDr0_Clone:promote LVMvg1:start
     # Para el fs creo un grupo; n&#243;tese que el orden en que incluyo los
     # recursos del grupo es su start order.
   group FS FSroot_r0 FSr0_etc FSr0_varwww FSr0_varlibmysql
   colocation FS_on_DRBDr0 inf: FS LVMvg1  # DRBDr0_Clone:Master
   #order FS-after-DRBDr0 inf: DRBDr0_Clone:promote FS:start
   #http://oss.clusterlabs.org/pipermail/pacemaker/2010-February/004864.html
   order FS-after-LVMvg1 inf: LVMvg1 FS
   colocation LINKS_Drbdlinks-with-FS inf: LINKS_Drbdlinks FS
   order LINKS_Drbdlinks-after-FS inf: FS LINKS_Drbdlinks
   colocation MySQL-with-FS inf: MySQL FS
   order MySQL-after-fs inf: FS MySQL
   colocation ClusterIP1-with-MySQL inf: ClusterIP1 MySQL
   order ClusterIP1-after-MySQL inf: MySQL ClusterIP1
   colocation Apache2-with-ClusterIP1 inf: Apache2 ClusterIP1
   order Apache2-after-ClusterIP1 inf: ClusterIP1 Apache2
   location prefer-d1 Apache2 50: d1
    # o tb: location prefer-d1 DRBD_Clone rule role="master" 100: #uname eq node1
   # ... es posible que todos los colocation se puedan resumir con:
   #     group LAMP DRBDr0 LVMvg1 FS Links MySQL Apache2 ClusterIP1
   #     colocation LAMP-with-DRBDr0_Clone inf: LAMP DRBDr0_Clone:Master
   #     ... o incluso mejor esa &#250;ltima por:
   #         order LAMP-after-DRBDr0_Clone inf: DRBDr0_Clone:promote LAMP:start
   # ... si us&#225;semos -inf en las colocation, indicamos que nunca est&#233;n juntos
   # ... order significa "carga a, luego b; para b, luego a" pero la acci&#243;n
   #     (cargar o= start) se puede cambiar a la de promoci&#243;n con promote y
   #     significa "promote a, luego carga b; para b, despromociona a". El usar
   #     promote lo he visto entre drbd y fs en el howto oficial clusterlabs.org,
   #     al interponer lvm he usado el promote entre drbd y lvm como he
   #     visto en http://www.drbd.org/users-guide/s-lvm-pacemaker.html y
   #     http://oss.clusterlabs.org/pipermail/pacemaker/2010-February/004864.html
   #     y aqu&#237; prefiere usar promote para todos:
   #     http://coisasdoit.blogspot.com.es/2012/07/testing-linux-cluster-debian-squeeze.html
   #     ... m&#225;s sobre todos los tipos de ordering:
   #     http://clusterlabs.org/mwiki/images/d/d6/Ordering_Explained.pdf
   # ... usamos location si alguna es preferible por ser una m&#225;quina m&#225;s
   #     potente etc. Pienso que si pones varios location para tales
   #     recursos, pero &#233;stos tienen un orden, ganar&#237;a siempre el de menor
   #     orden&#191;?

   edit primitive &lt;tal&gt; ... si quiero modificar alguna (te pone un vim)
  commit
  end
 cib use live
 cib commit conf20130407
 quit
</pre>


<pre class="src src-sh">crm_verify -L -V
ip addr show | grep -B2 10.0.0.1
</pre>


<pre class="src src-sh">crm configure show
crm resource status Apache2
man ocf_heartbeat_apache
</pre>



<pre class="src src-text">OLD (no h&#225;gase):
   group WebServer ClusterIP WebFS Links DBase WebSite
   colocation WebServer-with-ms_ro inf: WebServer ms_r0:Master
   order WebServer-after-ms_ro inf: ms_r0:promote WebServer:start
   location prefer-fo2 WebServer 50: fo2
   -
   colocation col_sym_with_util1 inf: p_sym_leseb ms_drbd_util1:Master
   order ord_sym_after_util1 inf: ms_drbd_util1:promote p_sym_leseb:start
</pre>

</div>
</div>

</div>

<div id="outline-container-2-8" class="outline-3">
<h3 id="sec-2-8"><span class="section-number-3">2.8</span> Test failover</h3>
<div class="outline-text-3" id="text-2-8">

<p>Anotación:
Parece que hay un script 'ocf-tester' para rular los RA con environment:
</p>


<pre class="src src-text">ocf-tester -n Apache2 -o configfile=/etc/apache2/apache2.conf -o ... /usr/lib/ocf/resource.d/heartbeat/apache
man ocf_heartbeat_apache
man ocf-tester
... ocf-tester necesita a xmllint (paquete libxml2-utils)
</pre>


<ul>
<li>En d1:
</li>
</ul>




<pre class="src src-sh">crm node standby <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Si pones este nodo en standby, los recursos se asignan a otro nodo:</span>
crm_mon -1f -V   <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">tambi&#233;n es lo que hago antes de un invoke-rc.d corosync stop</span>

crm node online  <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Si lo vuelves a poner online, los recursos vuelven a d1 (default):</span>
crm_mon -1f -V

crm_standby -D -U <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Poner en standby no &#233;ste sino el otro nodo</span>
crm_mon -1f -V

crm resource move WebServer d3 <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">switchover; mueve ese recurso a ese nodo manual</span>
crm_mon -1f -V

crm resource unmove WebServer <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">devuelve el control de asignaciones a pacemaker</span>


crm_resource -r -p &lt;bla&gt; -v stopped <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Stop resource</span>
crm_resource -r -p &lt;bla&gt; -v started <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Start resource</span>
</pre>

</div>

</div>

<div id="outline-container-2-9" class="outline-3">
<h3 id="sec-2-9"><span class="section-number-3">2.9</span> Test phpmyadmin</h3>
<div class="outline-text-3" id="text-2-9">

<ul>
<li>Desde el master:
</li>
</ul>




<pre class="src src-sh">mysql -h 127.0.0.1 -u root -p
</pre>


<pre class="src src-sql"><span style="color: #00ffff;">CREATE</span> DATABASE testdb <span style="color: #98fb98;">CHARACTER</span> <span style="color: #00ffff;">SET</span> utf8 <span style="color: #00ffff;">COLLATE</span> utf8_unicode_ci;
<span style="color: #00ffff;">GRANT</span> <span style="color: #00ffff;">ALL</span> <span style="color: #00ffff;">PRIVILEGES</span> <span style="color: #00ffff;">ON</span> testdb.* <span style="color: #00ffff;">TO</span> test@<span style="color: #ffa07a;">'%'</span> IDENTIFIED <span style="color: #00ffff;">BY</span> <span style="color: #ffa07a;">'test'</span>;
<span style="color: #00ffff;">CREATE</span> <span style="color: #00ffff;">TABLE</span> <span style="color: #87cefa;">testdb.testt</span> ( id <span style="color: #98fb98;">int</span>(10) <span style="color: #00ffff;">not</span> <span style="color: #00ffff;">null</span>, a <span style="color: #98fb98;">varchar</span>(30) <span style="color: #00ffff;">not</span> <span style="color: #00ffff;">null</span>, <span style="color: #00ffff;">primary</span> <span style="color: #00ffff;">key</span> (id), <span style="color: #00ffff;">key</span> (a) );
</pre>



<p>
Ahora crearemos un profile para que phpmyadmin sepa de nuestro server mysql y
haga de iface a él:
</p>


<pre class="src src-sh">vim /mnt/drbdr0/etc/phpmyadmin/config.inc.php
</pre>


<pre class="src src-php">...
<span style="color: #ff7f24;">////</span><span style="color: #ff7f24;">fx:</span>
<span style="color: #ff7f24;">//</span><span style="color: #ff7f24;">http://wiki.phpmyadmin.net/pma/Config</span>

<span style="color: #ff7f24;">// </span><span style="color: #ff7f24;">ClusterMySQL:</span>
$<span style="color: #eedd82;">cfg</span>[<span style="color: #ffa07a;">'Servers'</span>][$<span style="color: #eedd82;">i</span>][<span style="color: #ffa07a;">'verbose'</span>] = <span style="color: #ffa07a;">'ClusterMySQL'</span>;
$<span style="color: #eedd82;">cfg</span>[<span style="color: #ffa07a;">'Servers'</span>][$<span style="color: #eedd82;">i</span>][<span style="color: #ffa07a;">'host'</span>] = <span style="color: #ffa07a;">'10.0.0.1'</span>;
$<span style="color: #eedd82;">cfg</span>[<span style="color: #ffa07a;">'Servers'</span>][$<span style="color: #eedd82;">i</span>][<span style="color: #ffa07a;">'port'</span>] = <span style="color: #ededed; background-color: #000000; font-weight: bold;">3306</span>;
$<span style="color: #eedd82;">cfg</span>[<span style="color: #ffa07a;">'Servers'</span>][$<span style="color: #eedd82;">i</span>][<span style="color: #ffa07a;">'connect_type'</span>] = <span style="color: #ffa07a;">'tcp'</span>;
$<span style="color: #eedd82;">cfg</span>[<span style="color: #ffa07a;">'Servers'</span>][$<span style="color: #eedd82;">i</span>][<span style="color: #ffa07a;">'extension'</span>] = <span style="color: #ffa07a;">'mysqli'</span>;
$<span style="color: #eedd82;">cfg</span>[<span style="color: #ffa07a;">'Servers'</span>][$<span style="color: #eedd82;">i</span>][<span style="color: #ffa07a;">'auth_type'</span>] = <span style="color: #ffa07a;">'cookie'</span>;
$<span style="color: #eedd82;">cfg</span>[<span style="color: #ffa07a;">'Servers'</span>][$<span style="color: #eedd82;">i</span>][<span style="color: #ffa07a;">'user'</span>] = <span style="color: #ffa07a;">'root'</span>;
$<span style="color: #eedd82;">cfg</span>[<span style="color: #ffa07a;">'Servers'</span>][$<span style="color: #eedd82;">i</span>][<span style="color: #ffa07a;">'password'</span>] = <span style="color: #ffa07a;">''</span>;

<span style="color: #ff7f24;">// </span><span style="color: #ff7f24;">Otro</span>
$<span style="color: #eedd82;">i</span>++;
<span style="color: #ff7f24;">//</span><span style="color: #ff7f24;">...</span>

<span style="color: #ff7f24;">// </span><span style="color: #ff7f24;">Default:</span>
$<span style="color: #eedd82;">cfg</span>[<span style="color: #ffa07a;">'ServerDefault'</span>] = <span style="color: #ededed; background-color: #000000; font-weight: bold;">2</span>;
<span style="color: #ff7f24;">////</span><span style="color: #ff7f24;">endfx</span>
</pre>


<ul>
<li>Desde cualquiera:
</li>
</ul>




<pre class="src src-sh">w3m http://10.0.0.1/phpmyadmin/ <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">o con elinks que tiene frames</span>
</pre>

<p>
 úsese test:test
</p>

</div>

<div id="outline-container-2-9-1" class="outline-4">
<h4 id="sec-2-9-1"><span class="section-number-4">2.9.1</span> nota via web:</h4>
<div class="outline-text-4" id="text-2-9-1">

<ul>
<li>Phpmyadmin también permite crear profiles a través de formularios; está
  explicado en /usr/share/doc/phpmyadmin/README.Debian.gz en Configuration.
  Desde el master:
</li>
</ul>




<pre class="src src-sh">/usr/sbin/pma-configure <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">permite que /etc/phpmyadmin/config exista y sea writeable por www-data</span>

w3m http://localhost/phpmyadmin/setup/index.php admin:admin (lo de debconf)
</pre>


<pre class="src src-text">name of this server: ClusterMySQL
hostname: example.org
port: 3306 
socket: 10.0.0.1
use ssl: no
conn: tcp
php ext: mysqli
compress: no
auth type: cookie (http://wiki.phpmyadmin.net/pma/Auth_types)
 (si config, luego te pide el root:rootpw de mysql)
&lt;save&gt;
&lt;save button&gt;
</pre>


<pre class="src src-sh">mv /etc/phpmyadmin/config/config.inc.php /etc/phpmyadmin/
</pre>

<p>
 (de hecho si haces el mv contrario, el formulario web te permitiría editarlo)
 &hellip; pero el save no me ha valido y no me ha aparecido nada, creo, así que
     le he dado también al botón de download, y he copiado lo que me ha
     interesado en el config.inc.php.
 &lt;download button&gt;
</p>


<pre class="src src-sh">/usr/sbin/pma-secure    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">impide que /etc/phpmyadmin sea writeable por www-data</span>
</pre>

</div>
</div>
</div>

</div>

<div id="outline-container-3" class="outline-2">
<h2 id="sec-3"><span class="section-number-2">3</span> Anotaciones sobre Stacked DRBD para Primary-multiSlave</h2>
<div class="outline-text-2" id="text-3">

<p><a href="http://www.drbd.org/users-guide/s-three-way-repl.html">http://www.drbd.org/users-guide/s-three-way-repl.html</a>
<a href="http://www.drbd.org/users-guide/s-three-nodes.html">http://www.drbd.org/users-guide/s-three-nodes.html</a>
<a href="http://www.drbd.org/users-guide/s-pacemaker-stacked-resources.html">http://www.drbd.org/users-guide/s-pacemaker-stacked-resources.html</a>
</p>
<p>
DRBD no tiene soporte para más de dos nodos [*], aunque provee un mecanismo
para conseguir algo así entre un par y un tercer nodo, o entre 2 o más
pares&hellip; aunque con limitaciones. La principal limitación es que a nivel de
CRM, de pacemaker, cada par debe estar en un cluster distinto :P. Otra es la
sobrecarga de diseño y configuración. Otra quizás la eficiencia. También es
mencionable que (y lo discuto luego) sólo parece servir para
Primary-multiSlave, sólo un primario de nuevo.
</p>


<pre class="src src-text">[*] (Tampoco el mdadm del raid de linux soportaba clusterizaci&#243;n alguna: no s&#233;
    d&#243;nde hay alternativa, pues).
    http://fghaas.wordpress.com/2009/09/16/alternatives-to-drbd/
</pre>

<p>
Si un despliegue DRBD que llamaremos par-A queremos sincronizarlo con una 3ª
máquina u otro par (par-B), creamos en par-A un recurso DRBD adicional a r0, y
que será de tipo stacked, es decir en ambos nodos de par-A su drbd.conf
declararía:
</p><ol>
<li>un recurso r0 con una sección "on" que declara la IP y la device /dev/drbd0
</li>
</ol>

<p>que usar para un nodo de par-A, y otra sección "on" similar para el otro nodo
de par-A. Y 2) también declara un recurso DRBD adicional de nombre r0-S el cual
declara una sección "on" con la device (p.e /dev/drbd10) y la IP del tercer
nodo remoto y una sección (ahora no es otra on) "stacked-on-top-of r0", la cual
asigna a este recurso apilado un dispositivo (p.e. /dev/drbd10) y una IP
flotante (véase luego al hablar de pacemaker) accesible por el tercer nodo;
también hay que usar siempre "meta-disk internal" en los recursos apilados.
<a href="http://www.drbd.org/users-guide/s-three-nodes.html">http://www.drbd.org/users-guide/s-three-nodes.html</a>
</p>
<p>
Para inicializarlo, primero inicializamos r0 y cuando todo ok, vamos a un nodo
de par-A donde r0 esté activado como primario (usando drbdadm up r0 o
pacemaker). Entonces inicializamos r0-S en ese mismo nodo y lo ponemos de
primario:
</p>
<p>
drbdadm create-md &ndash;stacked r0-S # Inicializo metadata de stacked.
drbdadm up &ndash;stacked r0-S        # Activo y
 drbdadm primary &ndash;stacked r0-S  # pongo en modo primario en este nodo.
</p>
<p>
(
</p>
<p>
 En el tercer nodo, o el segundo par en su caso, habría que hacer ésto también
 ahora, sólo que sin &ndash;stacked si no es allí un recurso apilado -caso de en un
 3er nodo-, o con ello -caso de en un segundo par porque usará un
 apilado-. Después no hace falta otro comando para la 1ª sync como hacíamos en
 r0 creo, pero también he visto usarlo así que:
 drbdadm &ndash;stacked &ndash; &ndash;overwrite-data-of-peer primary r0-S
</p>
<p>
)
</p>

<p>
El recurso r0-S, lo usamos normalmente (montamos /dev/drbd10&hellip;) mientras que
el no stacked no. Ésto puede que no sea ninguna tontería e intervenga a la hora
de decidir cuántos primarios acabas teniendo para ofrecer servicios (ésto tiene
forma de raíces de árbol: no es lo mismo que puedas entrar por abajo -múltiples
raicillas- que por arriba -un sólo tallo-). Básicamente diría que sí que se
pueden tener varios puntos de acceso si puedes tener varias raíces en paralelo
(a-la "olivas de 3 estacas") lo que ocurriría si pudieras crear varios
"stacked-on-top-of r0" paralelos. El problema es que podría ocurrir que quedase
un sólo nodo de primario de todos los apilados paralelos, luego si hay más de
un punto de acceso al disco duro, podría haber corrupción. Quizás no, o quizás
sí y la solucíon sea usar un shared FS, o quizás simplemente drbd no permite
stacked paralelos&hellip; no lo sé.
</p>
<p>
Si no, sólo podría haber un primario por muchos pares que tuvieses. De todas
formas, puesto que tenemos la limitación de que cada par tiene que estar en un
clúster distinto, tampoco es muy interesante el caso de más de 2 nodos.
</p>
<p>
Además, llegando ya a la configuración del clúster, tenemos que hacer cumplir
al stacked estos requisitos con pacemaker:
<a href="http://www.drbd.org/users-guide/s-pacemaker-stacked-resources.html">http://www.drbd.org/users-guide/s-pacemaker-stacked-resources.html</a>
</p><ul>
<li>que r0-S lo definamos, al igual que r0, con un primitive y con clon
   master-slave.
</li>
<li>colocación: que r0-S se levante como master en el nodo donde se haya hecho
   master r0.
</li>
<li>orden: que r0-S se levante tras la promoción de r0.
</li>
<li>que r0-S lleve asociado un recurso IP flotante (el que nombré) para hacer en
   todo momento accesible remotamente al master de r0-S, y entonces: dicha IP
   se colocará donde el master r0 y se activará también antes de r0-S.
</li>
</ul>


<p>
El tercer nodo que tiene que replicarse lo tiene fácil sabiendo la IP flotante:
se configura normalmente con un recurso sólo y no stacked (supongo que debe
llamarse r0-S como remotamente, eso sí) y ya está. Claro, ese r0 en una sección
"on" tendrá su IP pública, bien, pero en la otra sección "on" tendrá la IP
flotante del r0-S de par-A.
</p>
<p>
Si no es un tercer nodo sino un par-B, par-B se configura igual que par-A, sólo
cambian obligatoriamente las IP. Al configurarse igual, tanto par-A como par-B
ofrecen una IP flotante, IPfA e IPfB. Es claro que en la sección on de r0-S de
par-A se usa IPfB, y en sección on de r0-S de par-B se usa IPfB.
</p>
<p>
Si tuviésemos un par adicional, par-C: la respuesta depende de si se pueden
tener apilaciones paralelas o no.
</p>
<p>
 Begin OLD (ésto es de cuando pensaba que se montaba el recurso base, no el apilado):
  Si tenemos un par-C, supongo que habría que apilar otro recurso drdb en cada
  par, una especie de r0-S-S con "stacked-on-top-of r0-S". Para reducir el
  número de recursos, podríamos crear un camino de replicación, en serie,
  parA-parB-parC y por tanto sólo par-B tendría el segundo recurso stacked.
  Para más de 3 pares el proceso es el mismo: apilar para que haya sync todos
  con todos (cualquier combinación de todos los pares cogidos de dos en dos), o
  disminuir la sobrecarga de configuración creando <span style="text-decoration:underline;">un</span> camino lineal basado en
  tríos de pares, o crear <span style="text-decoration:underline;">varios</span> caminos lineales a placer&hellip;
 End OLD.
</p>
<p>
nota: en la replicación de los stacked, podemos elegir de nuevo el mecanismo
      que quereamos A, B o C.
</p>





<pre class="src src-text">Red de IP flotante: 10.0.0.0 -&gt; IP flotante: 10.0.0.1
    d1.example.org:       .11
    d2.example.org:       .12
    d3.example.org:       .13
    d4.example.org:       .14

Red para DRBD: 192.168.1.0
           d1: 192.168.1.1, /dev/sdb -&gt; vg -&gt; /mnt/drbdr0/lamp1
           d2:          .2, /dev/sdb 
           d3:          .3, /dev/sdb 
           d4:          .4, /dev/sdb 
           IP flotante r0-S d1-d2: 192.168.1.200
           IP flotante r0-S d3-d4: 192.168.1.201
           ... la IP flotante podr&#237;a estar en otra red como la 10.0.0.0,
               o en una nueva, todo depende de en cu&#225;l son accesibles ambos
               pares entre s&#237; y si conviene por razones capacidad del enlace.


Red para CM&amp;M: 192.168.0.0
           d1:          .1
           d2:          .2
           d3:          .3
           d4:          .4
</pre>

<p>
<a href="http://www.howtoforge.com/drbd-8.3-third-node-replication-with-debian-etch">http://www.howtoforge.com/drbd-8.3-third-node-replication-with-debian-etch</a>
</p>
</div>

</div>

<div id="outline-container-4" class="outline-2">
<h2 id="sec-4"><span class="section-number-2">4</span> DESPLIEGUE: DRBD M-M + cLVM2 + OCFS2 + Pacemaker/Corosync + LAMP</h2>
<div class="outline-text-2" id="text-4">


<ul>
<li>Lo haré sobre debian7.
</li>
</ul>


<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<caption></caption>
<colgroup><col class="left" />
</colgroup>
<tbody>
<tr><td class="left"><a href="http://doc.opensuse.org/products_new/draft/SLE-HA/SLE-ha/index.html">http://doc.opensuse.org/products_new/draft/SLE-HA/SLE-ha/index.html</a></td></tr>
<tr><td class="left"><a href="http://crunchtools.com/kvm-cluster-with-drbd-gfs2/">http://crunchtools.com/kvm-cluster-with-drbd-gfs2/</a></td></tr>
<tr><td class="left"><a href="http://henroo.wordpress.com/2011/08/23/kvm-with-gfs2-shared-storage-on-clustered-lvm/">http://henroo.wordpress.com/2011/08/23/kvm-with-gfs2-shared-storage-on-clustered-lvm/</a></td></tr>
<tr><td class="left"><a href="http://blogs.oucs.ox.ac.uk/jamest/2011/06/03/replicating-block-devices-with-drbd/">http://blogs.oucs.ox.ac.uk/jamest/2011/06/03/replicating-block-devices-with-drbd/</a></td></tr>
<tr><td class="left"><a href="http://rackerhacker.com/2011/02/13/dual-primary-drbd-with-ocfs2/">http://rackerhacker.com/2011/02/13/dual-primary-drbd-with-ocfs2/</a></td></tr>
</tbody>
</table>




<ul>
<li>Realmente quiero usar DRBD M-M?
  <a href="http://drbd.10923.n7.nabble.com/Reasons-not-to-use-allow-two-primaries-with-DRDB-td11807.html">http://drbd.10923.n7.nabble.com/Reasons-not-to-use-allow-two-primaries-with-DRDB-td11807.html</a>
</li>
</ul>



<ul>
<li>Nota: al usar DRBD M-M, MySQL tal como en el esquema no M-M sería un problema
        no? O éso también lo soluciona el interponer un shared FS.

</li>
<li>Nota: antes de hacer el mkfs, pacemaker debe tener registrados a los RA de
        DRBD, DLM y OCFS2 (el primero con su ms, los otros con sus clones) y
        Stonith. Sólo con éstos activos se puede proceder al mkfs y a
        continuación registrar el RA para FS que monta/desmonta.

</li>
<li>Nota: el que DRBD vaya a funcionar master-master no significa que dejemos de
        usar el clon de tipo "ms", ya que van a seguir existiendo dos estados en
        el recurso; eso sí, explicitamos que puede haber hasta dos en estado
        master.

<p>
        Respecto a las colocaciones, al activarse todos los clones no hay que
        definirlas, a excepción de algunos casos especiales: DLM, O2CB, FS, que
        deben colocarse donde DRBD esté en estado master; entiendo entonces que
        es más un mecanismo para desactivarlos donde DRBD pueda tener algún
        problema, mientras que si hay dos master, pueden colocarse en ambos. (por
        cierto los colocation los hace con los clones/ms, no sé si es lo mismo
        que hacerlo con los primitive).
</p>
</li>
<li>Nota: Los clones de cualquier recurso pueden usar master-max (cuántas copias
        pueden promocionarse a master) y master-node-max (cuántas copias del
        recurso se pueden promocionar a maestro en un sólo nodo). Por defecto es
        1 en ambos.

</li>
<li>Nota: En la primitiva de DRBD para pacemaker: the master-max=2 meta variable
        enables dual-Master mode for a Pacemaker master/slave set. This requires
        that allow-two-primaries is also set to yes in the DRBD
        configuration. Otherwise, Pacemaker will flag a configuration error
        during resource validation. Además fencing no es suficiente: stonith!
<ul>
<li>En bloque disk: fencing resource-and-stonith
</li>
<li>En handlers: outdate-peer "/usr/lib/drbd/stonith<sub>admin</sub>-fence-peer.sh"
</li>
</ul>

</li>
</ul>



</div>

<div id="outline-container-4-1" class="outline-3">
<h3 id="sec-4-1"><span class="section-number-3">4.1</span> Enlaces, Bonding, /etc/hosts, NTP, puppet|cfengine|csync2</h3>
<div class="outline-text-3" id="text-4-1">


<p>
eth0 192.168.1.1  -&gt; drbd
eth1 10.0.3.15    -&gt; inet
eth2 192.168.0.1  -&gt; no usada
eth3 10.0.0.11    -&gt; servicios
</p>
<p>
Al menos dos enlaces entre cada máquina, ethernet de 100Mb/s o más y
 bonded. Por defecto se aconseja mode=active-backup.
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<caption></caption>
<colgroup><col class="left" />
</colgroup>
<tbody>
<tr><td class="left"><a href="http://tech.kulish.com/2011/11/30/debian-squeeze-802-3ad/">http://tech.kulish.com/2011/11/30/debian-squeeze-802-3ad/</a></td></tr>
<tr><td class="left"><a href="https://help.ubuntu.com/community/UbuntuBonding?action=show&amp;redirect=LinkAggregation">https://help.ubuntu.com/community/UbuntuBonding?action=show&amp;redirect=LinkAggregation</a></td></tr>
<tr><td class="left"><a href="http://www.howtogeek.com/52068/how-to-setup-network-link-aggregation-802-3ad-on-ubuntu/">http://www.howtogeek.com/52068/how-to-setup-network-link-aggregation-802-3ad-on-ubuntu/</a></td></tr>
</tbody>
</table>


<p>
El equipo de red debe soportar multicasting.
</p>
<p>
Quizás un power switch, un dispositivo que pueda ordenar a un equipo apagarlo y
encenderlo de nuevo. Se podría considerar a ésto la parte hw de SONITH.
</p>
<p>
Es muy importante que los nodos sean accesibles por nombre, para evitar
problemas se usa /etc/hosts (adicionalmente a DNS pero primero se mira allí).
</p>
<p>
Se puede usar sw como CSYNC2 para sincronizar fichero en el cluster; usa
rsync. It maintains a database of modified files so it is able to handle
deletion of files and file modification conflicts. Es una alternativa simple a
puppet o cfengine.
</p>



</div>

<div id="outline-container-4-1-1" class="outline-4">
<h4 id="sec-4-1-1"><span class="section-number-4">4.1.1</span> iSCSI (Storage Area Network)</h4>
<div class="outline-text-4" id="text-4-1-1">

<p>Si vas a usar iscsi o FB para tener una SAN de la que montan los nodos
directamente.
</p>
<p>
r/iscsi
</p>
<p>
Si quieres dotarla de replicación adicional, usas DRBD también.
</p>
<p>
Si va a haber acceso w concurrentes distribuídos (de dif nodos), se formaterá
usando un FS compartido como OCFS2 o GFS2, pudiendo usar cLVM2 para particionar
dinámicamente.
</p>
</div>
</div>

</div>

<div id="outline-container-4-2" class="outline-3">
<h3 id="sec-4-2"><span class="section-number-3">4.2</span> NTP</h3>
<div class="outline-text-3" id="text-4-2">

<ul>
<li>En todos:
</li>
</ul>




<pre class="src src-sh">apt-get install ntp
ntpd -q -g <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">bla bla bla o seguir el anexo del PFC</span>
</pre>

</div>

</div>

<div id="outline-container-4-3" class="outline-3">
<h3 id="sec-4-3"><span class="section-number-3">4.3</span> DRBD (Storage Replication)</h3>
<div class="outline-text-3" id="text-4-3">

<p><a href="http://www.drbd.org/users-guide-8.3/">http://www.drbd.org/users-guide-8.3/</a>
</p><ul>
<li>Anotación: mdadm no tiene extensiones de clúster; no podemos hacer raid1 con
  mdadm etc. Sí es posible que cada equipo tenga dos discos duros sobre los que
  se hace raid1 <span style="text-decoration:underline;">local</span> y el dispositivo resultante es la fuente para DRBD:
  <a href="http://ubanov.wordpress.com/2009/08/10/instalacion-servidores-con-ocfs2-drbd-lvm2-raid1-sobre-debian-howto-install/">http://ubanov.wordpress.com/2009/08/10/instalacion-servidores-con-ocfs2-drbd-lvm2-raid1-sobre-debian-howto-install/</a>
</li>
<li>DRBD = raid1 (mirroring) a nivel de bloque entre dispositivos en red para
  conseguir alta disponibilidad por un mecanismo de failover. Si cae la
  tecnología HA designa a otro nodo como "Active node" (failover) y cuando se
  recupere sincroniza los bloques modificados y designa de nuevo como activo al
  primigenio (failback) o no. Agregación de enlaces (802.3ad) puede ser una
  buena forma de asegurarse alta tasa de transmisión para la replicación.
</li>
<li>DRDB no comparte el medio, sino que lo replica luego: no es un SPF, si falla
  la comunicación y cada nodo se tomase a sí mismo como master (split brain) no
  habría corrupción así que te evitas una fencing policy, las operaciones de
  lectura son locales vs SAN o NAS.
</li>
<li>Primary-primary mode = modo master-master (no tiene modo multi-master).
  Además en este modo el FS debe ser especial: no ext4 sino gfs, ocfs2&hellip; o
  bien se usa LVM encima y su funcionalidad de snapshots (pues las escrituras
  las hace siempre por duplicado, luego podemos leer del original y del
  snapshot).
<ul>
<li>La configuración master-master permite usar funcionalidades de
    load-balancing built-in de drbd8.
</li>
</ul>

</li>
<li>Synchronous mirroring = protocol C in DRBD speak = modo transaccional.
  También permite modo asíncrono pero no leo cuál sería su nombre.
</li>
<li>The DRBD OCF resource agent provides Master/Slave capability. Había uno
  antiguo para heartbeat, úsese mejor el OCF de linbit. Por otro lado, es vital
  que sólo el RA maneje a DRBD, luego hay que quitar los scripts de inicio de
  DRBD (muy importante en situaciones de split brain).
</li>
<li>DRBD en modo m/s + MySQL es posible. Hace falta que mysql esté declarado en
  un grupo que sólo tras la elección del master drbd se active y asocie (al
  elegido). Yo tenía dudas de usar drbd con rdbms, pero se supone que sólo hay
  un rdbms activo, los otros no escriben ni leeen.
</li>
</ul>



<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<caption></caption>
<colgroup><col class="left" />
</colgroup>
<tbody>
<tr><td class="left"><a href="http://ubanov.wordpress.com/2009/08/10/instalacion-servidores-con-ocfs2-drbd-lvm2-raid1-sobre-debian-howto-install/">http://ubanov.wordpress.com/2009/08/10/instalacion-servidores-con-ocfs2-drbd-lvm2-raid1-sobre-debian-howto-install/</a></td></tr>
<tr><td class="left"><a href="https://wiki.ubuntu.com/ClusterStack/LucidTesting#Pacemaker.2C_drbd8_and_OCFS2_or_GFS2">https://wiki.ubuntu.com/ClusterStack/LucidTesting#Pacemaker.2C_drbd8_and_OCFS2_or_GFS2</a></td></tr>
<tr><td class="left"><a href="http://www.gossamer-threads.com/lists/drbd/users/20044">http://www.gossamer-threads.com/lists/drbd/users/20044</a></td></tr>
</tbody>
</table>


<ul>
<li>En todos
</li>
</ul>




<pre class="src src-sh">apt-get install drbd8-utils
update-rc.d clvm disable    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Si pacemaker es el asignador, que s&#243;lo sea &#233;l.</span>
<span style="color: #b0c4de;">echo</span> <span style="color: #ffa07a;">'drbd'</span> &gt;&gt; /etc/modules <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">El script init cargaba el m&#243;dulo, luego necesito &#233;sto</span>
modprobe drbd
</pre>

<ul>
<li>Configuramos. En d1 (luego lo pasamos a d2 por scp):
</li>
</ul>




<pre class="src src-sh">view /etc/drbd.conf
vim /etc/drbd.d/global_common.conf  <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Opciones globales y comunes</span>
</pre>


<pre class="src src-sh"> <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Par&#225;metros globales. S&#243;lo est&#225; permitida una secci&#243;n global.</span>
 <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">minor-count, dialog-refresh, disable-ip-verification and usage-count</span>
global {
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Permitir que usage.drbd.org contabilice que estamos usando el sw DRBD:</span>
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">no,yes,ask  Durante pruebas pongo no.</span>
    usage-count no;
}
 <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Opciones heredadas por todos las secciones resource posteriores.</span>
common {
  <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Syncer configura finamente el comportamiento de replicaci&#243;n:</span>
  <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">rate, after, al-extents, use-rle, cpu-mask, verify-alg, csums-alg, c-plan-ahead, c-fill-target, c-delay-target, c-max-rate, c-min-rate and on-no-data-accessible</span>
  <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">syncer { rate 100M; }</span>
  <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Los handlers definen secuencias de ejecutables lanzables ante nueve</span>
  <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">eventos. El primero de la secuencia es mandar un correo sobre el</span>
  <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">problema concreto llamando a /usr/lib/drbd/notify-* (son enlaces a</span>
  <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">/usr/lib/drbd/notify.sh) y despu&#233;s intenta apagar o reiniciar por</span>
  <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">varios m&#233;todos (sysrq primero o scripts).</span>
  <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">pri-on-incon-degr, pri-lost-after-sb, pri-lost, fence-peer (formerly oudate-peer), local-io-error, initial-split-brain, split-brain, before-resync-target, after-resync-target</span>

  handlers {
    pri-on-incon-degr <span style="color: #ffa07a;">"/usr/lib/drbd/notify-pri-on-incon-degr.sh; /usr/lib/drbd/notify-emergency-reboot.sh; echo b &gt; /proc/sysrq-trigger ; reboot -f"</span>;
    pri-lost-after-sb <span style="color: #ffa07a;">"/usr/lib/drbd/notify-pri-lost-after-sb.sh; /usr/lib/drbd/notify-emergency-reboot.sh; echo b &gt; /proc/sysrq-trigger ; reboot -f"</span>;
    local-io-error <span style="color: #ffa07a;">"/usr/lib/drbd/notify-io-error.sh; /usr/lib/drbd/notify-emergency-shutdown.sh; echo o &gt; /proc/sysrq-trigger ; halt -f"</span>;
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Seg&#250;n http://www.drbd.org/users-guide-8.3/ch-ocfs2.html es muy imp:</span>
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">"All cluster file systems require fencing - not only via the DRBD</span>
    <span style="color: #ff7f24;">#  </span><span style="color: #ff7f24;">resource, but STONITH! A faulty member must be killed"</span>
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Y a continuaci&#243;n declara la l&#237;nea:</span>
    <span style="color: #ff7f24;">#</span><span style="color: #ff7f24;">outdate-peer "/usr/lib/drbd/make-sure-the-other-node-is-confirmed-dead.sh"</span>
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">... pero ese script no existe, es s&#243;lo un ejem de la doc; pero alguien</span>
    <span style="color: #ff7f24;">#     </span><span style="color: #ff7f24;">de Linbit escribi&#243; el contenido en otro que s&#237; viene en debian:</span>
    <span style="color: #ff7f24;">#</span><span style="color: #ff7f24;">http://lists.linbit.com/pipermail/drbd-user/2012-March/018081.html</span>
    <span style="color: #ff7f24;">####</span><span style="color: #ff7f24;">fx:</span>
    outdate-peer <span style="color: #ffa07a;">"/usr/lib/drbd/stonith_admin-fence-peer.sh"</span>;
    <span style="color: #ff7f24;">####</span><span style="color: #ff7f24;">endfx</span>
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Nota: hay otro que suena a pacemaker, "/usr/lib/drbd/crm-fence-peer.sh",</span>
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">pero aqu&#237; dicen que _no_ es para dual primary:</span>
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">http://lists.linbit.com/pipermail/drbd-user/2012-March/018083.html</span>
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">En otros sitios he le&#237;do "/sbin/kill-other-node.sh" porque la gu&#237;a para</span>
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">8.4 cambia el anterior a &#233;ste, quiz&#225;s es la misma historia, pero uso 8.3.</span>
  }
}

<span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Lo fundamental para permitir dos primarios son las opciones:</span>
<span style="color: #ff7f24;">#  </span><span style="color: #ff7f24;">- net { allow-two-primaries; ... }</span>
<span style="color: #ff7f24;">#  </span><span style="color: #ff7f24;">- startup { become-primary-on both; ... } # si no obligatorio, deseable.</span>
</pre>


<pre class="src src-sh">vim /etc/drbd.d/r0.res
</pre>


<pre class="src src-sh"> <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Secci&#243;n para definir un recurso DRBD. M&#237;nimo tiene que declarar el protocolo</span>
 <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">y dos secciones on.</span>
resource r0 {
  <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Protocol configura cu&#225;ndo dar por terminada una escritura:</span>
  <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">C -&gt;  s&#237;ncrono, es decir cuando se ha completado en 1&#186; y 2&#186;</span>
  <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">B -&gt; as&#237;ncrono, se da por completo cuando acaba en 1&#186; y llega al buffer en 2&#186;</span>
  <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">A -&gt; as&#237;ncrono, se da por completo cuando acaba en 1&#186; y llega a buffer local TCP hacia 2&#186;</span>
  protocol C;

  <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Opciones de ajuste de comportamiento. V&#233;ase man drbdsetup.</span>
  <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">wfc-timeout, degr-wfc-timeout, outdated-wfc-timeout, wait-after-sb, stacked-timeouts and become-primary-on</span>
  startup {
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Qu&#233; nodo deber&#237;a ser promocionado a primario, puede ser both o &lt;nodename&gt;</span>
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Cuando no especificas qui&#233;n, quedan en modo secundario y se entiendehasta</span>
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">que el que un cluster resource manager lo decidir&#225;.</span>
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">En un despliegue master-master, debe ser become-primary-on both</span>
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">****</span>
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">No es recomendable activarlo durante la inicializaci&#243;n,</span>
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">es mejor dejarlo inactiva hasta que la primera sincronizaci&#243;n se haya</span>
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">completado:</span>
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">****</span>
    <span style="color: #ff7f24;">#</span><span style="color: #ff7f24;">become-primary-on both;</span>

    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Tiempo para concluir que hay un timeout en la conexi&#243;n; es 0 (inf) por</span>
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">defecto porque el script de inicio intenta esperar a que est&#233;n activos</span>
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">los drbd y que el cluster no se encuentre luego un split-brain al inicio.</span>
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Default es 0 (inf). El manual de suse usa 1, tambi&#233;n he visto 300 muy frecmente</span>
    wfc-timeout 1;
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">wfc = wait for connection timeout, pero prefijado con degr- implica </span>
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">un timeout espec&#237;fico para nodos degradados, que han sido reiniciados,</span>
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">creo. El manual de suse usa 1, tambi&#233;n he visto 10 o 120</span>
    degr-wfc-timeout 1;
  }

<span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Ante problemas de disco o de red (brain-split situations), nuestra pol&#237;tica</span>
<span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">ser&#225; simple: desconectar. Lo vemos en las dos siguientes secciones (junto a</span>
<span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">otras opciones):</span>

  <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Dispositivo de disco. Nunca &#250;sese cuando DRBD est&#225; actuando, tampoco dumpe2fs.</span>
  <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">V&#233;ase man drbdsetup</span>
  <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">on-io-error, size, fencing, use-bmbv, no-disk-barrier, no-disk-flushes, no-disk-drain, no-md-flushes, max-bio-bvecs</span>
  disk {
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Si el dispositivo de disco presenta erorres i/o</span>
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">detach, call-local-io-error o be pass_on</span>
    on-io-error   detach;

    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Pol&#237;tica de fencing: dont-care, resource-only, resource-and-stonith.</span>
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Seg&#250;n http://www.drbd.org/users-guide-8.3/ch-ocfs2.html:</span>
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">"All cluster file systems require fencing - not only via the DRBD</span>
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">resource, but STONITH! A faulty member must be killed"</span>
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">y a&#241;ade que hay que usar:</span>
    <span style="color: #ff7f24;">####</span><span style="color: #ff7f24;">fx:</span>
    fencing resource-and-stonith;
    <span style="color: #ff7f24;">####</span><span style="color: #ff7f24;">endfx</span>
  }




  <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Ajustes relativos a la conectividad del cl&#250;ster:</span>
  <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">sndbuf-size, rcvbuf-size, timeout, connect-int, ping-int, ping-timeout, max-buffers, max-epoch-size, ko-count, allow-two-primaries, cram-hmac-alg, shared-secret, after-sb-0pri, after-sb-1pri, after-sb-2pri, data-integrity-alg, no-tcp-cork, on-congestion, congestion-fill, congestion-extents</span>
  <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Seg&#250;n http://www.drbd.org/users-guide-8.3/s-ocfs2-create-resource.html,</span>
  <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">la p&#243;liza para M-M ser&#237;a:</span>
  net {
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">****</span>
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">No es recomendable activar allow-two-primaries durante la inicializaci&#243;n,</span>
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">es mejor dejarlo inactiva hasta que la primera sincronizaci&#243;n se haya</span>
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">completado:</span>
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">****</span>
    <span style="color: #ff7f24;">#</span><span style="color: #ff7f24;">allow-two-primaries;</span>
    <span style="color: #ff7f24;">#</span><span style="color: #ff7f24;">... - n&#243;tese que permite que haya dos primarios, pero no es necesario</span>
    <span style="color: #ff7f24;">#    </span><span style="color: #ff7f24;">que constantemente tengan ese rol ambos;</span>
    <span style="color: #ff7f24;">#    </span><span style="color: #ff7f24;">- y n&#243;tese tambi&#233;n que en situaciones de split-brain sigue siendo</span>
    <span style="color: #ff7f24;">#    </span><span style="color: #ff7f24;">un problema que ambos sean primarios.</span>

    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">The following lines are dedicated to handle split-brain situations</span>
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">(e.g., if one of the nodes fails). Pongo los valores que dice en el</span>
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">manual para master-master y ocfs2:</span>
    <span style="color: #ff7f24;">#</span>
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Fencing a nivel de recurso DRBD:</span>
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">http://www.drbd.org/users-guide/s-configure-split-brain-behavior.html</span>
    <span style="color: #ff7f24;">#</span>
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">- Si se pierde el primario:</span>
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">P&#243;lizas: disconnect               desconecta, no sincronices</span>
    <span style="color: #ff7f24;">#          </span><span style="color: #ff7f24;">discard-younger-primary  sync desde el que era primario antes</span>
    <span style="color: #ff7f24;">#          </span><span style="color: #ff7f24;">discard-older-primary    sync desde el que se puso primario tras ca&#237;da del primario</span>
    <span style="color: #ff7f24;">#          </span><span style="color: #ff7f24;">discard-zero-changes     simplemente haz a uno el primario random o quien no escribises nada a&#250;n</span>
    <span style="color: #ff7f24;">#          </span><span style="color: #ff7f24;">discard-least-changes    haz primario a quien escribiese m&#225;s bloques</span>
    <span style="color: #ff7f24;">#          </span><span style="color: #ff7f24;">discard-node-&lt;nodename&gt;  sync de ese nodo</span>
    <span style="color: #ff7f24;">#</span><span style="color: #ff7f24;">after-sb-0pri discard-younger-primary; #discard-zero-changes;</span>
    <span style="color: #ff7f24;">#</span><span style="color: #ff7f24;">after-sb-0pri disconnect;</span>
    after-sb-0pri discard-zero-changes; <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">If both secondary, just make one primary:</span>
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">DRBD detected a split-brain scenario, but none of the nodes think they&#8217;re a</span>
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">primary. DRBD will take the newest modifications and apply them to the node</span>
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">that didn&#8217;t have any changes.</span>

    <span style="color: #ff7f24;">#</span>
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">- Si uno queda primario y el otro no:</span>
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">P&#243;lizas: disconnect</span>
    <span style="color: #ff7f24;">#          </span><span style="color: #ff7f24;">consensus              descarta datos 2&#186; si la p&#243;liza anterior los destruir&#237;a, desconecta en otro caso</span>
    <span style="color: #ff7f24;">#          </span><span style="color: #ff7f24;">discard-secondary      descarta la versi&#243;n del 2&#186;</span>
    <span style="color: #ff7f24;">#          </span><span style="color: #ff7f24;">call-pri-lost-after-sb lo que decida la p&#243;liza anteior (0pri)</span>
    <span style="color: #ff7f24;">#          </span><span style="color: #ff7f24;">violently-as0p         0pri incluso si causa flapping. Dangerous.</span>
    <span style="color: #ff7f24;">#</span><span style="color: #ff7f24;">after-sb-1pri disconnect;</span>
    after-sb-1pri discard-secondary; <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">If one primary, one is not, trust primary</span>
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">DRBD detected a split-brain scenario, but one node is the primary and the</span>
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">other is the secondary. In this case, DRBD will decide that the secondary</span>
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">node is the victim and it will sync data from the primary to the secondary</span>
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">automatically.</span>

    <span style="color: #ff7f24;">#</span>
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">- Si quedan dos primarios pero hay brain-split:</span>
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">P&#243;liza: disconnect</span>
    <span style="color: #ff7f24;">#         </span><span style="color: #ff7f24;">call-pri-lost-after-sb  lo que decida la p&#243;liza anteior (0pri)</span>
    <span style="color: #ff7f24;">#         </span><span style="color: #ff7f24;">violently-as0p          0pri incluso si causa flapping. Dangerous.</span>
    <span style="color: #ff7f24;">#</span><span style="color: #ff7f24;">after-sb-2pri call-pri-lost-after-sb; # If two primaries, make unchanged one secondary:</span>
    <span style="color: #ff7f24;">#</span>
    after-sb-2pri disconnect;
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">DRBD detected a split-brain scenario, but it can&#8217;t figure out which node</span>
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">has the right data. It tries to protect the consistency of both nodes by</span>
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">disconnecting the DRBD volume entirely. You&#8217;ll have to tell DRBD which node</span>
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">has the valid data in order to reconnect the volume. Use extreme caution if</span>
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">you find yourself in this scenario.</span>

    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Si hay conflicto entre el resultado de la decisi&#243;n de resync y el rol actual:</span>
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">P&#243;liza: disconnect    </span>
    <span style="color: #ff7f24;">#         </span><span style="color: #ff7f24;">violently     permitido sincronizar con el primario. Dangerous.</span>
    <span style="color: #ff7f24;">#         </span><span style="color: #ff7f24;">call-pri-lost llama a ese script que reinicia la m&#225;quina = la pone de secundario</span>
    <span style="color: #ff7f24;">#</span><span style="color: #ff7f24;">rr-conflict disconnect;</span>

    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">He visto que hay alg&#250;n soporte de autenticaci&#243;n a trav&#233;s de PSK,</span>
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">es obligatorio configurar cram-hmac-alg (cat /proc/crypto)</span>
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">y el shared-secret.</span>
    cram-hmac-alg sha1;
    shared-secret <span style="color: #ffa07a;">"mysecret"</span>;
  }

  <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Ajustes relativos a los procesos de sincronizaci&#243;n.</span>
  <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">V&#233;ase man drbdsetup.</span>
  syncer {
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">110M suele ser suficiente para un enlace Gigabit dedicado entre dos</span>
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">server. El manual de suse recomienda 1/3 de la tasa de transmisi&#243;n</span>
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">disponible.</span>
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Ojo, este rate afecta a la resincronizaci&#243;n, no al mirroring</span>
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">(entiendo que &#233;so quiere decir que afecta a metadatos drbd, no a datos)</span>

    rate 100M;
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Extents (&#225;rea de almacenamiento contigua) alterados</span>
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">7-3843, default 127. Mayor n&#186; extent, menos updates pero m&#225;s largos.</span>
    al-extents 257;
  }

<span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Secci&#243;n para definir el dispositivo y su localizaci&#243;n. "on &lt;salida de hostname&gt;"</span>
<span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">(si queremos utilizar IP hay que usar la opci&#243;n "floating" en su lugar).</span>
<span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">He visto dar al device un segundo arg, "minor 0" como minor number del device</span>
  on d1 {
    device     /dev/drbd0;
    disk       /dev/sdb; <span style="color: #ff7f24;">#  </span><span style="color: #ff7f24;">Si lvm2 debajo, algo como /dev/mapper/VolGroup-drbd--demo</span>
    address    192.168.1.1:7788;
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">D&#243;nde guardar los metadatos; si flexible-meta-disk tiene flexibilidad por</span>
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">si usas LVM2, creo.</span>
    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Interna para que use la misma device para metadatos, y la ponga al final.</span>
    <span style="color: #ff7f24;">#</span><span style="color: #ff7f24;">flexible-meta-disk  internal;</span>
    meta-disk internal;
  }

  on d2 {
    device     /dev/drbd0;
    disk       /dev/sdb;
   <span style="color: #ff7f24;">#</span><span style="color: #ff7f24;">... puesto que device y disk son iguales en d1 y d2, podr&#237;amos haberlas</span>
   <span style="color: #ff7f24;">#</span><span style="color: #ff7f24;">puesto una s&#243;la vez fuera de los bloques "on".</span>
    address    192.168.1.2:7788;
    <span style="color: #ff7f24;">#</span><span style="color: #ff7f24;">flexible-meta-disk  internal;</span>
    meta-disk internal;
  }
}
</pre>


<pre class="src src-sh">drbdadm dump all       <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">comprueba sintaxis</span>
scp /etc/drbd.d/* d2:/etc/drbd.d/
</pre>

<ul>
<li>En d1:
</li>
</ul>

<p>Inicializo los metadatos drbd
</p>


<pre class="src src-sh">drbdadm -- --ignore-sanity-checks create-md r0 <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">"-- --ignore-...-checks" es del manual suse</span>
</pre>


<pre class="src src-text">Writing meta data...
initializing activity log
NOT initialized bitmap
New drbd meta data block successfully created.
</pre>


<pre class="src src-sh">invoke-rc.d drbd start <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">... &#233;sto hace que se ejecute, entre otros:</span>
 <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">drbdadm up r0 (en verdad 'all'), que a su vez es un atajo a la sec:</span>
 <span style="color: #ff7f24;">#  </span><span style="color: #ff7f24;">drbdadm attach r0   abre su backend de dispositivo de bloque /dev/sdb</span>
 <span style="color: #ff7f24;">#  </span><span style="color: #ff7f24;">drbdadm syncer r0   configura los par&#225;metros de sincronizaci&#243;n</span>
 <span style="color: #ff7f24;">#  </span><span style="color: #ff7f24;">drbdadm connect r0  se conecta al peer</span>
cat /proc/drbd
</pre>

<ul>
<li>En d2:
</li>
</ul>




<pre class="src src-sh">drbdadm -- --ignore-sanity-checks create-md r0 <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">"-- --ignore-...-checks" es del manual suse</span>

invoke-rc.d drbd restart
cat /proc/drbd
</pre>

<ul>
<li>En d1:
</li>
</ul>




<pre class="src src-sh"><span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Comienzo proceso de resync, si el disco duro estaba usado previamente</span>
<span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">hago antes un dd if=/dev/zero of=/dev/sdb count=10000</span>
drbdadm -- --overwrite-data-of-peer primary r0

drbdadm primary r0                   <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Haz de primario</span>
drbdadm role r0                      <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Comprobamos que cambi&#243; el role</span>
</pre>

<p>
&hellip; acabada la primera sync, ya puedo activar allow-two-primaries:
</p>


<pre class="src src-sh">vim /etc/drbd.d/r0.res
</pre>


<pre class="src src-sh"> ...
 startup {
    become-primary-on both;
...
 net {
    allow-two-primaries;
 ...
</pre>


<pre class="src src-sh">scp /etc/drbd.d/* d2:/etc/drbd.d/
</pre>

<ul>
<li>En d2
</li>
</ul>




<pre class="src src-sh">invoke-rc.d drbd restart
drbdadm role r0                      <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Comprobamos el role, si no es prim:</span>
drbdadm primary r0                   <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">haz de primario t&#250; tambi&#233;n. O primary all.</span>
</pre>


<p>
TODO: (la primera vez al menos) me falló el ponerse los dos primarios tras el
      restart, creo; tuve que hacer stop y luego start o manual.
</p>

<p>
Algo de tuning y troubleshooting en el manual Suse:
</p><ul>
<li><a href="https://www.suse.com/documentation/sle_ha/book_sleha/data/sec_ha_drbd_tuning.html">https://www.suse.com/documentation/sle_ha/book_sleha/data/sec_ha_drbd_tuning.html</a>
</li>
<li><a href="https://www.suse.com/documentation/sle_ha/book_sleha/data/sec_ha_drbd_trouble.html">https://www.suse.com/documentation/sle_ha/book_sleha/data/sec_ha_drbd_trouble.html</a>
</li>
</ul>



<ul>
<li>Anotación: Salida de drbd-overview o cat /proc/drbd
   <a href="http://www.drbd.org/users-guide/ch-admin.html#s-check-status">http://www.drbd.org/users-guide/ch-admin.html#s-check-status</a>
</li>
</ul>




<pre class="src src-text">0: cs:StandAlone ro:Primary/Unknown ds:UpToDate/DUnknown   r----
   ns:0 nr:0 dw:0 dr:148 al:0 bm:0 lo:0 pe:0 ua:0 ap:0 ep:1 wo:b oos:73728
</pre>

<ul>
<li>cs = connection state. Hay muchos
<ul>
<li>Unconnected: estado transitorio de intento de conexión, puede llevar a:
</li>
<li>WFconnection: esperando a que el peer aparezca
</li>
<li>WFreportparams: cnx a nivel de tcp, esperando pqt a nivel de app
</li>
<li>Connected.
</li>
<li>Disconnecting: estado transitorio, desemboca en:
</li>
<li>Standalone, sin red por cualquier razón, incluída la descnx manual.
</li>
</ul>

</li>
<li>ro = roles. Primary, Secondary, Unknown(se ref siempre al peer, que está desconn)
</li>
<li>ds = disk states
<ul>
<li>attatching: estado transitorio mientras que lee la metadata
</li>
<li>consistent: datos consistentes, cuando te conectes se decidirá:
</li>
<li>uptodate
</li>
<li>outdated
</li>
<li>dunknown: sin conexión con el peer.
</li>
<li>inconsistent
</li>
<li>diskless
</li>
<li>attatching
</li>
<li>failed
</li>
</ul>

</li>
<li>i/o indicators:
<ul>
<li>r(running), s(suspended)
</li>
<li>otros (cada guión es otro)
</li>
</ul>

</li>
<li>performance indicators (2ª línea)
<ul>
<li>ns/nr: network send/receive
</li>
<li>dw/dr: disk w/r
</li>
<li>oos: nº kB out of sync
</li>
</ul>

</li>
</ul>


</div>

</div>

<div id="outline-container-4-4" class="outline-3">
<h3 id="sec-4-4"><span class="section-number-3">4.4</span> cLVM2</h3>
<div class="outline-text-3" id="text-4-4">

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<caption></caption>
<colgroup><col class="left" />
</colgroup>
<tbody>
<tr><td class="left"><a href="https://www.suse.com/documentation/sle_ha/book_sleha/?page=/documentation/sle_ha/book_sleha/data/book_sleha.html">https://www.suse.com/documentation/sle_ha/book_sleha/?page=/documentation/sle_ha/book_sleha/data/book_sleha.html</a></td></tr>
<tr><td class="left"><a href="https://www.suse.com/documentation/sle_ha/book_sleha/?page=/documentation/sle_ha/book_sleha/data/sec_ha_ocfs2_create_service.html#proc_ocfs2_resources">https://www.suse.com/documentation/sle_ha/book_sleha/?page=/documentation/sle_ha/book_sleha/data/sec_ha_ocfs2_create_service.html#proc_ocfs2_resources</a></td></tr>
</tbody>
</table>


<p>
cLVM2 son extensiones de clustering para LVM2. Es así otra tecnología para
permitir almacenamiento compartido, pero en este caso es sólo para la parte de
particionado permitiendo que este tipo de metadatos se propaguen por los nodos
(y si redimensionas desde un nodo, que los otros dejen de funcionar con el
viejo perfil y se acojan al nuevo). Compatible con pacemaker. LVM2 cogía unos
PV (phisical volumes: discos o particiones físicas que administra sólo LVM2) y
los agrupaba como VG (Grupos de Volúmenes) para poder ofrecer con ellos LV, los
volúmenes lógicos que montas, redimensionas etc. También de un LV puedes hacer
un snapshot o SLV. Si usas el modo primario-primario de DRBD, éste se puede
interponer como PV de LVM2 permitiendo luego crear un VG de dispositivos drbd y
de ahí LV en el cluster, pudiendo redimensionar ya online etc; la otra
posibilidad, menos interesante es que DRBD usa un LV ya creado de backend, lo
cual no necesita un LVM clusterizable ya, sería una curiosidad excepto si
decimos que esos LV se pueden usar para que DRBD los ofrezca luego como PV DRBD
en el clúster.
</p><table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<caption></caption>
<colgroup><col class="left" />
</colgroup>
<tbody>
<tr><td class="left"><a href="http://www.drbd.org/users-guide-8.3/s-lvm-drbd-as-pv.html">http://www.drbd.org/users-guide-8.3/s-lvm-drbd-as-pv.html</a></td></tr>
<tr><td class="left"><a href="http://www.drbd.org/users-guide-8.3/s-nested-lvm.html">http://www.drbd.org/users-guide-8.3/s-nested-lvm.html</a></td></tr>
</tbody>
</table>



<p>
No necesitas las extensiones de clúster cuando sólo tienes un master drbd, al
igual que no necesitabas a OCFS2 en ese caso.
</p>
<p>
3 componentes:
</p><ul>
<li>LVM2
</li>
<li>cLVM2
</li>
<li>DLM
</li>
</ul>


<p>
Hay dos problemas con clvm: 1) en debian 6 no trae soporte para corosync, sólo
para cman; ésto se resuelve recompilando. Y 2), hay un bug no resuelto en
debian 6 ni 7 por el que no viene con un RA ocf:
<a href="http://bugs.debian.org/cgi-bin/bugreport.cgi?bug=597713">http://bugs.debian.org/cgi-bin/bugreport.cgi?bug=597713</a>
sólo se me ocurre bajar el de allí. Vamos con el primer problema:
</p>
<ul>
<li>En d1:
</li>
</ul>




<pre class="src src-sh"><span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Si debian 7, instalamos:</span>
apt-get install clvm  <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">probablemente se instala ya corosync</span>

<span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Si debian 6, recompilar&#237;amos antes de instalar, pero no hay garant&#237;as:</span>
<span style="color: #ff7f24;">#</span><span style="color: #ff7f24;">mkdir /usr/src/clvm</span>
<span style="color: #ff7f24;">#</span><span style="color: #ff7f24;">cd /usr/src/clvm</span>
<span style="color: #ff7f24;">#</span><span style="color: #ff7f24;">apt-get install libcorosync-dev</span>
<span style="color: #ff7f24;">#</span><span style="color: #ff7f24;">apt-get source dpkg-dev clvm</span>
<span style="color: #ff7f24;">#</span><span style="color: #ff7f24;">cd lvm2-*</span>
<span style="color: #ff7f24;">#</span><span style="color: #ff7f24;">vim debian/rules</span>
<span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">... --with-clvmd=all (intentar&#237;a autodetectar cman,corosync,openais)</span>
<span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Y comprobamos que tenemos un RA para clvm:</span>
<span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">ls /usr/lib/...</span>
<span style="color: #ff7f24;">#</span><span style="color: #ff7f24;">apt-get install debhelper automake libcman-dev libdlm-dev libreadline5-dev libselinux1-dev libudev-dev pkg-config quilt</span>
<span style="color: #ff7f24;">#</span><span style="color: #ff7f24;">unset CFLAGS; unset CXXFLAGS; dpkg-buildpackage -D</span>
<span style="color: #ff7f24;">#</span><span style="color: #ff7f24;">dpkg -i ../clvm*deb</span>
<span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Luego habr&#237;a que comentar los if que eval&#250;an si est&#225; cman y su estado,</span>
<span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">en el script init.</span>
</pre>


<pre class="src src-sh">update-rc.d clvm disable
</pre>

<p>
Luego hay que activar cLVM con el tipo de bloque 3:
</p>


<pre class="src src-sh">vim /etc/lvm/lvm.conf
    <span style="color: #ff7f24;">####</span><span style="color: #ff7f24;">fx:</span>
    <span style="color: #ff7f24;">#</span><span style="color: #ff7f24;">&#183;locking_type = 1</span>
    locking_type = 3     <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">para que lvm use clustered locking. &#201;sto se puede</span>
                         <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">cambiar desde la cli con:</span>
                         <span style="color: #ff7f24;">#  </span><span style="color: #ff7f24;">lvmconf --enable-cluster</span>
    <span style="color: #ff7f24;">####</span><span style="color: #ff7f24;">endfx</span>
 ...
 <span style="color: #ff7f24;">####</span><span style="color: #ff7f24;">fx:</span>
    <span style="color: #ff7f24;">#</span><span style="color: #ff7f24;">&#183;filter = [ "a/.*/" ]</span>
    filter = [ <span style="color: #ffa07a;">"a|drbd.*|"</span>, <span style="color: #ffa07a;">"r|.*|"</span> ]
 <span style="color: #ff7f24;">####</span><span style="color: #ff7f24;">endfx</span>
 ...
    <span style="color: #ff7f24;">####</span><span style="color: #ff7f24;">fx:</span>
    <span style="color: #ff7f24;">#</span><span style="color: #ff7f24;">&#183;write_cache_state = 1</span>
    write_cache_state = 0
    <span style="color: #ff7f24;">####</span><span style="color: #ff7f24;">endfx</span>
</pre>

<p>
Quizás puede faltar este directorio bajo /run
</p>


<pre class="src src-sh">mkdir -p /var/run/lvm
vim /etc/init.d/clvm
<span style="color: #ff7f24;">####</span><span style="color: #ff7f24;">fx:</span>
<span style="color: #ff7f24;">#</span><span style="color: #ff7f24;">Quiz&#225;s falte tambi&#233;n &#233;sto en los scripts de corosync.</span>
<span style="color: #ff7f24;">#</span><span style="color: #ff7f24;">https://bugs.launchpad.net/ubuntu/+source/lvm2/+bug/959218</span>
mkdir -p /var/run/lvm
<span style="color: #ff7f24;">####</span><span style="color: #ff7f24;">endfx</span>
...
</pre>


<p>
When several devices seemingly share the same physical volume signature (as can
be the case for multipath devices or DRBD), it is recommended to explicitly
configure the devices which LVM2 scans for PVs. La línea anterior "a"cepta las
de la forma /dev/drbd&hellip; y "r"echaza el resto.
</p>
<p>
&hellip; write<sub>cache</sub><sub>state</sub> deshabilita la cache lvm, además borramos ésta para
    evitar que, por una activación a destiempo (no master) o lo que sea se
    utilice una información que a lo mejor ha cambiado. El manual de suse para
    primary-primary no dice nada de ésto, pero me sigue pareciendo razonable
    y por éso o lo quito en el conf anterior, o lo dejo y además hago:
</p>


<pre class="src src-sh">rm -rf /etc/lvm/cache/.cache
</pre>

<p>
&hellip; invoke-rc.d clvm start no funcionará porque aún no está levantado corosync etc
</p>
<ul>
<li>En d2:
</li>
</ul>




<pre class="src src-sh">apt-get -f install clvm
scp d1:/etc/lvm/lvm.conf /etc/lvm/lvm.conf
scp d1:/etc/init.d/clvm /etc/init.d/clvm
update-rc.d clvm disable
rm -rf /etc/lvm/cache/.cache
mkdir -p /var/run/lvm
</pre>

<p>
&hellip; invoke-rc.d clvm start no lo levantará porque aún no está levantado corosync etc
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<caption></caption>
<colgroup><col class="left" />
</colgroup>
<tbody>
<tr><td class="left"><a href="http://www.advogato.org/person/lmb/diary.html?start=104">http://www.advogato.org/person/lmb/diary.html?start=104</a></td></tr>
<tr><td class="left"><a href="https://www.suse.com/documentation/sle_ha/book_sleha/?page=/documentation/sle_ha/book_sleha/data/book_sleha.html">https://www.suse.com/documentation/sle_ha/book_sleha/?page=/documentation/sle_ha/book_sleha/data/book_sleha.html</a></td></tr>
<tr><td class="left"><a href="http://doc.opensuse.org/products/draft/SLE-HA/SLE-ha-guide_sd_draft/cha.ha.config.example.html">http://doc.opensuse.org/products/draft/SLE-HA/SLE-ha-guide_sd_draft/cha.ha.config.example.html</a></td></tr>
<tr><td class="left"><a href="http://www.drbd.org/users-guide-8.3/ch-lvm.html">http://www.drbd.org/users-guide-8.3/ch-lvm.html</a></td></tr>
</tbody>
</table>




</div>

<div id="outline-container-4-4-1" class="outline-4">
<h4 id="sec-4-4-1"><span class="section-number-4">4.4.1</span> Respecto al script RA ocf</h4>
<div class="outline-text-4" id="text-4-4-1">

<p>El paquete no trae un script RA, así que:
<a href="http://bugs.debian.org/cgi-bin/bugreport.cgi?bug=597713">http://bugs.debian.org/cgi-bin/bugreport.cgi?bug=597713</a>
</p><ul>
<li>En d1:
</li>
</ul>




<pre class="src src-sh">mkdir -p /usr/lib/ocf/resource.d/lvm2/ <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">el path compatible con el configure primitive de suse que usar&#233; en pacemaker, y el que usa ubun</span>
touch /usr/lib/ocf/resource.d/lvm2/clvmd
chmod a+x /usr/lib/ocf/resource.d/lvm2/clvmd
vim      /usr/lib/ocf/resource.d/lvm2/clvmd
</pre>


<pre class="src src-sh"><span style="color: #ff7f24;">#</span><span style="color: #ff7f24;">!/bin/</span><span style="color: #00ffff;">bash</span>

<span style="color: #ff7f24;">####</span><span style="color: #ff7f24;">fx:</span>
mkdir -p /var/run/lvm
<span style="color: #ff7f24;">####</span><span style="color: #ff7f24;">endfx</span>

<span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">OCF initialization</span>
<span style="color: #b0c4de;">.</span> ${<span style="color: #eedd82;">OCF_ROOT</span>}/resource.d/heartbeat/.ocf-shellfuncs
<span style="color: #b0c4de;">.</span> /lib/lsb/init-functions

<span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Parameter defaults</span>
: ${<span style="color: #eedd82;">OCF_RESKEY_CRM_meta_globally_unique</span>:=<span style="color: #ffa07a;">"false"</span>}
: ${<span style="color: #eedd82;">OCF_RESKEY_daemon_timeout</span>:=<span style="color: #ffa07a;">"80"</span>}
: ${<span style="color: #eedd82;">OCF_RESKEY_daemon_options</span>:=<span style="color: #ffa07a;">"-d 0"</span>}

<span style="color: #eedd82;">exec</span>=<span style="color: #ffa07a;">"/usr/sbin/clvmd"</span>

<span style="color: #00ffff;">function</span> check_status() {
    <span style="color: #b0c4de;">local</span> <span style="color: #eedd82;">pid</span>=$(<span style="color: #fa8072;">pidofproc</span> $<span style="color: #eedd82;">exec</span>)
    [ -n <span style="color: #ffa07a;">"$pid"</span> ] || <span style="color: #00ffff;">return</span> $<span style="color: #eedd82;">OCF_NOT_RUNNING</span>
    <span style="color: #00ffff;">return</span> $<span style="color: #eedd82;">OCF_SUCCESS</span>
}

clvmd_start() {
    <span style="color: #b0c4de;">local</span> <span style="color: #eedd82;">elapsed</span>=0 <span style="color: #eedd82;">base</span>=$(<span style="color: #fa8072;">basename</span> $<span style="color: #eedd82;">exec</span>)
    ocf_log info <span style="color: #ffa07a;">"Starting $OCF_RESOURCE_INSTANCE"</span>
    <span style="color: #00ffff;">if</span> [ ! -e <span style="color: #ffa07a;">"$exec"</span> ]; <span style="color: #00ffff;">then</span>
        ocf_log err <span style="color: #ffa07a;">"Required binary not found: $exec"</span>
        <span style="color: #00ffff;">return</span> $<span style="color: #eedd82;">OCF_ERR_INSTALLED</span>
    <span style="color: #00ffff;">fi</span>
    /sbin/start-stop-daemon --start --nicelevel 0 --quiet --chdir <span style="color: #ffa07a;">"$PWD"</span> --exec $<span style="color: #eedd82;">exec</span> -- <span style="color: #ffa07a;">"$OCF_RESKEY_daemon_options"</span>
    <span style="color: #00ffff;">case</span> <span style="color: #ffa07a;">"$?"</span><span style="color: #00ffff;"> in</span>
        1)
            ocf_log warn <span style="color: #ffa07a;">"$base already running with pid $(</span><span style="color: #fa8072;">pidof</span><span style="color: #ffa07a;"> $exec)."</span>
            <span style="color: #00ffff;">return</span> $<span style="color: #eedd82;">OCF_SUCCESS</span>
            ;;
        3)
            ocf_log err <span style="color: #ffa07a;">"Unable to start $base."</span>
            <span style="color: #00ffff;">return</span> $<span style="color: #eedd82;">OCF_ERR_GENERIC</span>
            ;;
    <span style="color: #00ffff;">esac</span>

    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">wait for the daemon to start</span>
    <span style="color: #00ffff;">while</span> ! check_status; <span style="color: #00ffff;">do</span>
        <span style="color: #00ffff;">if</span> (( ++elapsed &gt; OCF_RESKEY_daemon_timeout )); <span style="color: #00ffff;">then</span>
            ocf_log err <span style="color: #ffa07a;">"$base invocation returned without error but is still not started after $OCF_RESKEY_daemon_timeout seconds."</span>
            <span style="color: #00ffff;">return</span> $<span style="color: #eedd82;">OCF_ERR_GENERIC</span>
        <span style="color: #00ffff;">fi</span>
        sleep 1
    <span style="color: #00ffff;">done</span>

    ocf_log info <span style="color: #ffa07a;">"$base started."</span>
    <span style="color: #00ffff;">return</span> $<span style="color: #eedd82;">OCF_SUCCESS</span>
}

clvmd_stop() {
    ocf_log info <span style="color: #ffa07a;">"Stopping $OCF_RESOURCE_INSTANCE"</span>
    <span style="color: #b0c4de;">local</span> <span style="color: #eedd82;">elapsed</span>=0 <span style="color: #eedd82;">base</span>=$(<span style="color: #fa8072;">basename</span> $<span style="color: #eedd82;">exec</span>)
    /sbin/start-stop-daemon --stop --quiet --exec $<span style="color: #eedd82;">exec</span>
    <span style="color: #00ffff;">case</span> <span style="color: #ffa07a;">"$?"</span><span style="color: #00ffff;"> in</span>
        1)
            ocf_log info <span style="color: #ffa07a;">"No need to stop $base as it isn't running."</span>
            <span style="color: #00ffff;">return</span> $<span style="color: #eedd82;">OCF_NOT_RUNNING</span>
            ;;
        3)
            ocf_log err <span style="color: #ffa07a;">"Stopping $base with SIG$signal returned an error. Fail."</span>
            <span style="color: #00ffff;">return</span> $<span style="color: #eedd82;">OCF_ERR_GENERIC</span>
            ;;
    <span style="color: #00ffff;">esac</span>

    <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">wait for the daemon to stop</span>
    <span style="color: #00ffff;">while</span> check_status; <span style="color: #00ffff;">do</span>
        <span style="color: #00ffff;">if</span> (( ++elapsed &gt; OCF_RESKEY_daemon_timeout )); <span style="color: #00ffff;">then</span>
            ocf_log err <span style="color: #ffa07a;">"$base is still not stopped after $OCF_RESKEY_daemon_timeout seconds. Killing it."</span>
            <span style="color: #00ffff;">if</span> ! /sbin/start-stop-daemon --stop --oknodo --signal KILL --exec $<span style="color: #eedd82;">exec</span> ; <span style="color: #00ffff;">then</span>
                ocf_log err <span style="color: #ffa07a;">"Unable to kill $base: $output"</span>
                <span style="color: #00ffff;">return</span> $<span style="color: #eedd82;">OCF_ERR_GENERIC</span>
            <span style="color: #00ffff;">fi</span>
            ocf_log warn <span style="color: #ffa07a;">"$base killed. Yep - killed it dead."</span>
        <span style="color: #00ffff;">fi</span>
        sleep 1
    <span style="color: #00ffff;">done</span>

    ocf_log info <span style="color: #ffa07a;">"$base stopped."</span>
    <span style="color: #00ffff;">return</span> $<span style="color: #eedd82;">OCF_SUCCESS</span>
}

clvmd_monitor() {
    clvmd_validate
    check_status
}

clvmd_usage() {
    <span style="color: #b0c4de;">echo</span> <span style="color: #ffa07a;">"usage: $0 {start|stop|monitor|validate-all|meta-data}"</span>
    <span style="color: #b0c4de;">echo</span> <span style="color: #ffa07a;">"  Expects to have a fully populated OCF RA-compliant environment set."</span>
    <span style="color: #b0c4de;">echo</span> <span style="color: #ffa07a;">"  In particular, a value for OCF_ROOT."</span>
}

clvmd_validate() {
    <span style="color: #00ffff;">case</span> ${<span style="color: #eedd82;">OCF_RESKEY_CRM_meta_globally_unique</span>} <span style="color: #00ffff;">in</span>
        yes|Yes|true|True|1) 
            ocf_log err <span style="color: #ffa07a;">"$OCF_RESOURCE_INSTANCE must be configured with the gloablly_unique=false meta attribute"</span>
            <span style="color: #00ffff;">exit</span> $<span style="color: #eedd82;">OCF_ERR_CONFIGURED</span>
            ;;
    <span style="color: #00ffff;">esac</span>
    <span style="color: #00ffff;">return</span> $<span style="color: #eedd82;">OCF_SUCCESS</span>
}

meta_data() {
    cat &lt;&lt;END
<span style="color: #ffff00; font-weight: bold;">&lt;?xml version="1.0"?&gt;</span>
<span style="color: #ffff00; font-weight: bold;">&lt;!DOCTYPE resource-agent SYSTEM "ra-api-1.dtd"&gt;</span>
<span style="color: #ffff00; font-weight: bold;">&lt;resource-agent name="clvmd"&gt;</span>
<span style="color: #ffff00; font-weight: bold;">    &lt;version&gt;1.0&lt;/version&gt;</span>
<span style="color: #ffff00; font-weight: bold;">    &lt;longdesc lang="en"&gt;</span>
<span style="color: #ffff00; font-weight: bold;">        This is a clvmd Resource Agent.</span>
<span style="color: #ffff00; font-weight: bold;">        It starts clvmd as anonymous clones.</span>
<span style="color: #ffff00; font-weight: bold;">    &lt;/longdesc&gt;</span>
<span style="color: #ffff00; font-weight: bold;">    &lt;shortdesc lang="en"&gt;clvmd resource agent&lt;/shortdesc&gt;</span>

<span style="color: #ffff00; font-weight: bold;">    &lt;parameters&gt;</span>
<span style="color: #ffff00; font-weight: bold;">        &lt;parameter name="daemon_timeout" unique="0"&gt;</span>
<span style="color: #ffff00; font-weight: bold;">            &lt;longdesc lang="en"&gt;</span>
<span style="color: #ffff00; font-weight: bold;">                Number of seconds to allow the control daemon to come up and down</span>
<span style="color: #ffff00; font-weight: bold;">            &lt;/longdesc&gt;</span>
<span style="color: #ffff00; font-weight: bold;">            &lt;shortdesc lang="en"&gt;Daemon Timeout&lt;/shortdesc&gt;</span>
<span style="color: #ffff00; font-weight: bold;">            &lt;content type="string" default="80"/&gt;</span>
<span style="color: #ffff00; font-weight: bold;">        &lt;/parameter&gt;</span>
<span style="color: #ffff00; font-weight: bold;">        &lt;parameter name="daemon_options" unique="0"&gt;</span>
<span style="color: #ffff00; font-weight: bold;">            &lt;longdesc lang="en"&gt;</span>
<span style="color: #ffff00; font-weight: bold;">                Options to clvmd. Refer to clvmd.8 for detailed descriptions.</span>
<span style="color: #ffff00; font-weight: bold;">            &lt;/longdesc&gt;</span>
<span style="color: #ffff00; font-weight: bold;">            &lt;shortdesc lang="en"&gt;Daemon Options&lt;/shortdesc&gt;</span>
<span style="color: #ffff00; font-weight: bold;">            &lt;content type="string" default="-d0"/&gt;</span>
<span style="color: #ffff00; font-weight: bold;">        &lt;/parameter&gt;</span>
<span style="color: #ffff00; font-weight: bold;">    &lt;/parameters&gt;</span>

<span style="color: #ffff00; font-weight: bold;">    &lt;actions&gt;</span>
<span style="color: #ffff00; font-weight: bold;">        &lt;action name="start"         timeout="90" /&gt;</span>
<span style="color: #ffff00; font-weight: bold;">        &lt;action name="stop"          timeout="100" /&gt;</span>
<span style="color: #ffff00; font-weight: bold;">        &lt;action name="monitor"       timeout="20" depth="0"/&gt;</span>
<span style="color: #ffff00; font-weight: bold;">        &lt;action name="meta-data"     timeout="5" /&gt;</span>
<span style="color: #ffff00; font-weight: bold;">        &lt;action name="validate-all"  timeout="30" /&gt;</span>
<span style="color: #ffff00; font-weight: bold;">    &lt;/actions&gt;</span>
<span style="color: #ffff00; font-weight: bold;">&lt;/resource-agent&gt;</span>
<span style="color: #ffff00; font-weight: bold;">END</span>
    <span style="color: #00ffff;">return</span> $<span style="color: #eedd82;">OCF_SUCCESS</span>
}

<span style="color: #00ffff;">case</span> $<span style="color: #eedd82;">__OCF_ACTION</span><span style="color: #00ffff;"> in</span>
    meta-data)
        meta_data
        ;;
    start)
        clvmd_start
        ;;
    stop)
        clvmd_stop
        ;;
    monitor)
        clvmd_monitor
        ;;
    validate-all)
        clvmd_validate
        ;;
    usage|<span style="color: #b0c4de;">help</span>)    
        clvmd_usage
        <span style="color: #00ffff;">exit</span> $<span style="color: #eedd82;">OCF_SUCCESS</span>
        ;;
    *)
        clvmd_usage &gt;&amp;2
        <span style="color: #00ffff;">exit</span> $<span style="color: #eedd82;">OCF_ERR_UNIMPLEMENTED</span>
        ;;
<span style="color: #00ffff;">esac</span>

<span style="color: #00ffff;">exit</span> $?
</pre>


<ul>
<li>En d2
</li>
</ul>




<pre class="src src-sh">mkdir -p /usr/lib/ocf/resource.d/lvm2
scp d1:/usr/lib/ocf/resource.d/lvm2/clvmd /usr/lib/ocf/resource.d/lvm2/clvmd
</pre>

</div>
</div>

</div>

<div id="outline-container-4-5" class="outline-3">
<h3 id="sec-4-5"><span class="section-number-3">4.5</span> Corosync</h3>
<div class="outline-text-3" id="text-4-5">

<ul>
<li>En todos:
</li>
</ul>




<pre class="src src-sh">apt-get install corosync
</pre>

<p>
&hellip; ya está instalado de cuando clvm.
</p>
<p>
    Corosync es el que levanta a pacemaker y, así, al resto.
</p>
<ul>
<li>Configuración de corosync. En d1:
</li>
</ul>




<pre class="src src-sh">vim /etc/corosync/corosync.conf
</pre>


<pre class="src src-sh"><span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Configuration options for the totem protocol.</span>
totem {
        <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Versi&#243;n del fichero de configuraci&#243;n.</span>
        version: 2

        <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">How long before declaring a token lost (ms)</span>
        token: 3000

        <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">How many token retransmits before forming a new configuration</span>
        token_retransmits_before_loss_const: 10

        <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">How long to wait for join messages in the membership protocol (ms)</span>
        join: 60

        <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">How long to wait for consensus to be achieved before starting a new round of membership configuration (ms)</span>
        consensus: 3600

        <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Turn off the virtual synchrony filter</span>
        vsftype: none

        <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Number of messages that may be sent by one processor on receipt of the token</span>
        max_messages: 20

        <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Limit generated nodeids to 31-bits (positive signed integers)</span>
        clear_node_high_bit: yes

        <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Disable encryption</span>
        secauth: off

        <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">How many threads to use for encryption/decryption</span>
        threads: 0

        <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Optionally assign a fixed node id (integer)</span>
        <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">nodeid: 1234</span>

        <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">This specifies the mode of redundant ring, which may be none, active, or passive.</span>
        rrp_mode: none

        interface {
                <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">ID de esta iface para el Redundant Ring Protocol. Empieza en 0.</span>
                ringnumber: 0
                <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Iface con IPv4 o IPv6:</span>
<span style="color: #ff7f24;">####</span><span style="color: #ff7f24;">fx:</span>
                bindnetaddr: 192.168.0.0
<span style="color: #ff7f24;">####</span><span style="color: #ff7f24;">endfx</span>
                <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Avoid 224.x.x.x because this is a "config" multicast address.</span>
                <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">http://goo.gl/fn0MW</span>
                <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">http://www.cisco.com/en/US/tech/tk828/technologies_white_paper09186a00802d4643.shtml</span>
                <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">TODO: comprobar si esa IP mcast se pone como iface virt de la</span>
                <span style="color: #ff7f24;">#       </span><span style="color: #ff7f24;">de la declarada para el bindnetaddr anterior:</span>
                mcastaddr: 226.94.1.1
                <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Puerto UDP</span>
                mcastport: 5405
                <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Si us&#225;semos "transport udpu" y no us&#225;semos mcastaddr, creo que</span>
                <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">habr&#237;a que crear secciones:</span>
                <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">member {</span>
                <span style="color: #ff7f24;">#        </span><span style="color: #ff7f24;">memberaddr: 10.0.0.2</span>
                <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">}</span>
                <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">member {</span>
                <span style="color: #ff7f24;">#        </span><span style="color: #ff7f24;">memberaddr: 10.0.0.3</span>
                <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">}</span>
                <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">...</span>
        }
}

amf {  
        mode: disabled
}

service {
        <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Load the Pacemaker Cluster Resource Manager</span>
        ver:       0
        name:      pacemaker
        <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Pacemaker no trae script init.</span>
}

aisexec {
        user:   root
        group:  root
        <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Es el demonio de openais, que usar&#225; alg&#250;n componente de OCFS2</span>
}

<span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Configuration options for logging</span>
logging {
        fileline: off
        to_stderr: yes
<span style="color: #ff7f24;">####</span><span style="color: #ff7f24;">fx:</span>
        to_logfile: yes
        logfile: /var/log/corosync/corosync.log
<span style="color: #ff7f24;">####</span><span style="color: #ff7f24;">endfx</span>
        to_syslog: yes
        syslog_facility: daemon
        debug: off
        timestamp: on
        logger_subsys {
                subsys: AMF
                debug: off
                tags: enter|leave|trace1|trace2|trace3|trace4|trace6
        }
}
</pre>



<pre class="src src-sh">corosync-keygen <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">http://sublimated.wordpress.com/2007/08/28/not-enough-random-bytes-available/</span>

scp /etc/corosync/authkey       d2:/etc/corosync
scp /etc/corosync/corosync.conf d2:/etc/corosync
</pre>

<ul>
<li>En todos:
</li>
</ul>




<pre class="src src-sh"><span style="color: #b0c4de;">echo</span> <span style="color: #eedd82;">START</span>=yes &gt;&gt; /etc/default/corosync
invoke-rc.d corosync start

tail -f /var/log/corosync/corosync.log
</pre>


<pre class="src src-text">corosync [MAIN  ] Completed service synchronization, ready to provide service.
</pre>

</div>

</div>

<div id="outline-container-4-6" class="outline-3">
<h3 id="sec-4-6"><span class="section-number-3">4.6</span> Pacemaker. OCFS2: instalación y requerimientos. Recursos previos a FS</h3>
<div class="outline-text-3" id="text-4-6">

<ul>
<li>En todos:
</li>
</ul>




<pre class="src src-sh">apt-get install pacemaker curl
</pre>

<p>
 &hellip; 30 MB con deps (perl libs&hellip;). curl/wget es necesario, creo, para
     (que el RA correspondiente pueda) consultar mod<sub>status</sub> de apache.
</p>
<p>
  Pacemaker debe ser levantado por corosync:
</p>


<pre class="src src-sh">invoke-rc.d corosync stop
invoke-rc.d corosync start
ps aux | grep pace

ls /var/lib/heartbeat

pacemakerd --features
pacemakerd --help
crm_mon --help <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">crm_mon --daemonize --as-html /var/www/crm_mon.html</span>

crm --help
crm configure show
crm configure show xml
crm configure show node d1
crm_verify -L

crm ra classes
crm ra list
</pre>

<ul>
<li>Previo: Creo que el RA de apache tiene un problema. En todos:
</li>
</ul>

<p>&hellip;
</p>

</div>

<div id="outline-container-4-6-1" class="outline-4">
<h4 id="sec-4-6-1"><span class="section-number-4">4.6.1</span> Variables globales de la CIB</h4>
<div class="outline-text-4" id="text-4-6-1">

<ul>
<li>En d1 por ejemplo:
 Si no hubiese más de dos nodos, modificamos la propiedad general:
</li>
</ul>




<pre class="src src-sh">crm configure property no-quorum-policy=<span style="color: #ffa07a;">"ignore"</span>
crm configure property expected-quorum-votes=<span style="color: #ffa07a;">"1"</span>
</pre>

<p>
  Para prevenir que un recurso se mueva tras la recuperación de su
   localización anterior o por otros motivos. Por defecto pacemaker cuenta que
   esa movilidad tiene coste cero y por tanto los mueve aplicando criterios de
   balanceo de carga u otros. El stickiness se puede configurar por RA o con un
   default que suele ser suficiente:
</p>


<pre class="src src-sh">crm configure rsc_defaults resource-stickiness=100
</pre>

</div>

</div>

<div id="outline-container-4-6-2" class="outline-4">
<h4 id="sec-4-6-2"><span class="section-number-4">4.6.2</span> Configuración de STONITH</h4>
<div class="outline-text-4" id="text-4-6-2">

<p>Permite estar 100% seguros de que un nodo no tiene acceso a los datos a pesar
de perder la comunicación con él, para ello apaga el nodo. La falta de
comunicación puede suceder por fallos en la red o en el nodo, y el dispositivo
STONITH debe poder diferenciarlos y, por ello, no puede por ejem:
</p><ul>
<li>no puede depender su alimentación de la del nodo.
</li>
<li>no puede depender la comunicación STONITHdevice y nodo de SSH.
</li>
<li>etc
</li>
</ul>




<pre class="src src-sh">stonith_admin --list-installed                             <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">drivers</span>
stonith_admin --metadata --agent &lt;type_seg&#250;n_cmd_anterior&gt; <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">params de config</span>
</pre>

<p>
<a href="http://clusterlabs.org/doc/en-US/Pacemaker/1.1-plugin/html/Clusters_from_Scratch/_configuring_stonith.html">http://clusterlabs.org/doc/en-US/Pacemaker/1.1-plugin/html/Clusters_from_Scratch/_configuring_stonith.html</a>
</p>


<pre class="src src-text">crm cib new stonith
 configure primitive ...
</pre>

<p>
    Véase man stonithd para usar o no estos param:
</p><ol>
<li>pcmk_host_map
</li>
<li>pcmk_host_list, pcmk_host_check
</li>
<li>pcmk_host_argument
</li>
</ol>




<pre class="src src-text">crm cib commit stonith
</pre>


<pre class="src src-sh">crm configure property stonith-enabled=<span style="color: #ffa07a;">"true"</span>

<span style="color: #ff7f24;">#</span><span style="color: #ff7f24;">stonith_admin --reboot &lt;nodename&gt; # test stonith functionality</span>
                                   <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">(previo: mejor p&#225;rese antes cluster en esa m&#225;quina)</span>
</pre>

<p>
 Ejem con driver stonith:external/ssh, sólo para testing:
</p>


<pre class="src src-text">primitive st_ssh stonith:external/ssh params hostlist="node1 node2"
clone fencing st-ssh
</pre>

<p>
 &hellip;versión no con clone sino con localización de distintos primitive.
 (entiendo que no se puede definir location con clones -tengo la duda de los
 clones tipo "globally unique" pero bueno-, de forma que hay que crear el
 recurso para cada nodo, dos primitive pues)
</p>
<p>
   "In this example, location constraints are used for the following reason:
    There is always a certain probability that the STONITH operation is going
    to fail. Therefore, a STONITH operation on the node which is the
    executioner as well is not reliable. If the node is reset, it cannot send
    the notification about the fencing operation outcome. The only way to do
    that is to assume that the operation is going to succeed and send the
    notification beforehand. But if the operation fails, problems could
    arise. Therefore, by convention, stonithd refuses to kill its host"
</p>
<p>
   "The choice of which construct to use for configuration depends on several
   factors: nature of the fencing device, number of hosts managed by the
   device, number of cluster nodes, or personal preference".
</p>


<pre class="src src-text">crm configure
 primitive STH_SSH-1 stonith:external/ssh params hostlist="d1"
 primitive STH_SSH-2 stonith:external/ssh params hostlist="d2"
 location ssh1-no-en-d1 STH_SSH-1 -inf: d1
 location ssh2-no-en-d2 STH_SSH-2 -inf: d2
commit
exit
</pre>



<p>
Tips: <a href="http://www.hastexo.com/resources/hints-and-kinks">http://www.hastexo.com/resources/hints-and-kinks</a>
</p>
</div>

</div>

<div id="outline-container-4-6-3" class="outline-4">
<h4 id="sec-4-6-3"><span class="section-number-4">4.6.3</span> Recurso ClusterIP1</h4>
<div class="outline-text-4" id="text-4-6-3">




<pre class="src src-sh"><span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">IP flotante:</span>
crm configure primitive ClusterIP1 ocf:heartbeat:IPaddr2 params <span style="color: #eedd82;">ip</span>=<span style="color: #ffa07a;">"10.0.0.1"</span> <span style="color: #eedd82;">cidr_netmask</span>=<span style="color: #ffa07a;">"32"</span> op monitor <span style="color: #eedd82;">interval</span>=<span style="color: #ffa07a;">"30s"</span>
</pre>

</div>

</div>

<div id="outline-container-4-6-4" class="outline-4">
<h4 id="sec-4-6-4"><span class="section-number-4">4.6.4</span> OCFS2: instalación y requerimientos</h4>
<div class="outline-text-4" id="text-4-6-4">

<ul>
<li><a href="https://wiki.ubuntu.com/ClusterStack/LucidTesting#Pacemaker.2C_drbd8_and_OCFS2_or_GFS2">https://wiki.ubuntu.com/ClusterStack/LucidTesting#Pacemaker.2C_drbd8_and_OCFS2_or_GFS2</a>
</li>
<li><a href="http://www.hastexo.com/resources/hints-and-kinks/ocfs2-pacemaker-debianubuntu">http://www.hastexo.com/resources/hints-and-kinks/ocfs2-pacemaker-debianubuntu</a>
</li>
<li><a href="http://www.drbd.org/users-guide-8.3/ch-ocfs2.html">http://www.drbd.org/users-guide-8.3/ch-ocfs2.html</a>
</li>
<li>En ambos:
</li>
</ul>




<pre class="src src-sh">apt-get install libdlm3 ocfs2-tools dlm-pcmk ocfs2-tools-pacemaker openais drbd8-utils libdlmcontrol3
update-rc.d o2cb disable
<span style="color: #ff7f24;">#</span><span style="color: #ff7f24;">update-rc.d ocfs2 disable # &#233;ste no hay que deshabilitarlo, porque se dedica a:</span>
</pre>


<pre class="src src-text">- ocfs2-tools-pacemaker -&gt; /usr/sbin/ocfs2_controld.pcmk, el cual depende
                           de libcoroipcc, libcrmcluster, libstonithd. As&#237;, es
                           un demonio que permite comunicar a ocfs2 con
                           corosync y ser controlado por pacemaker,
                           sustituyendo as&#237; las funcionalidades CRM y CM&amp;ML
                           nativas de los m&#243;dulos del kernel de OCFS2, por
                           corosync/pacemaker.
- dlm-pcmk              -&gt; /usr/sbin/dlm_controld.pcmk, depende tambi&#233;n de
                           las anteriores (excepto libstonithd). Mismo
                           razonamiento: demonio que permite comunicar al DLM
                           con corosync y ser controlado por pacemaker.
                           Nota libdlm3-pacemaker: era de ubun pero all&#237;
                           dlm-pcmk acab&#243; siendo su reemplazo (por si
                           la veo en alg&#250;n tuto viejo).
- libdlm3               -&gt; librer&#237;a con rutinas para usar el m&#243;dulo DLM en
                           kernel. Por cierto que seg&#250;n veo hay varios
                           m&#243;dulos, uno es del propio ocfs2, y otro est&#225;
                           en la rama "fs", no s&#233; si se relacionan entre
                           s&#237; uno gen&#233;rico y otro iface ocfs2
                           http://lwn.net/Articles/136308/)
- openais               -&gt; Integrar Ocfs2 con corosync/pacemaker necesita
                           el servicio openais_ckpt. El paquete openais
                           instala con un bloque service { } el fichero:
                           /etc/corosync/service.d/ckpt-service
- ocfs2-tools           -&gt; utilidades mkfs, fsck, etc.
</pre>

<ul>
<li>Start on boot: no&lt;default&gt;. Elijo no porque ésto lo controla pacemaker, no
  script init. Además, y en contra de lo que dice la guía de lucid, un yes hace
  que el script /etc/init.d/o2cb configure el uso de la iface nativa, no
  pacemaker.
</li>
</ul>


<p>
( En GFS2 es parecido, hay un dlm_controld en gfs-pcmk, un gfs_controld en
  dlm-pcmk, y unas utilidades en gfs-tools. Si usases corosync con cman en vez
  de con pacemaker, el paquete cman traería su versión de esos mismos programas ).
</p>
<p>
Recordemos que si necesitas un FS compartido (OCFS2, GFS2), éste necesitaba
usar un Distributed Lock Manager para gestionar las escrituras de todos los
participantes.
</p>
<p>
Por otro lado, OCFS2 puede clusterizarse 1) usando su módulo propio para
"CM&amp;ML" (paso de mensajes y membresía entre los nodos, DLM,&hellip;) y su módulo
propio para "CRM" (gestión de recursos, es decir montar y desmontar el fs);
estos módulos se configurarían en su /etc/ocfs2/cluster.conf. Pero OCFS2
también puede clusterizarse 2) utilizando, en lugar de su soporte propio, unos
demonios que habilitarían delegar ambas funcionalidades a corosync y pacemaker
respectivamente. Así pacemaker tiene estos recursos para OCFS2:
</p>
<ul>
<li>un RA al demonio dlm_controld.pcmk que era la interfaz corosync-DLM:
  ocf:pacemaker:controld.

<p>
  (por cierto que sólo necesitas un DLM en el clúster aunque haya varios
  servicios que lo necesiten -OCFS2, CLVM&hellip;-; sí lo debes clonear, dado que
  ofrece a cada nodo los servicios de CM&amp;ML etc, evidentemente. A este
  respecto, también es confuso que la documentación del RA dice, sin embargo,
  que -al menos el RA- es para, o lo necita, OCFS2. Es confuso, pero el demonio
  en sí, el paquete dlm-pcmk, no dice nada de especificidades de OCFS2, además
  de, como acabo de decir en este párrafo, CLVM usaría el mismo clon de este
  RA).
</p>
</li>
<li>un RA para el demonio ocfs2_controld.pcmk es ocf:ocfs2:o2cb (si no
  usas corosync/pacemaker ese demonio no existe, y son los módulos del kernel
  los que autónomamente cumplirían las funciones de node manager, heartbeat
  agent, network agent etc y en espacio de user sólo habría una utilidad de
  control o2cb_ctl).

</li>
<li>y el RA para montar/desmontar los fs, y que es el clásico (no específico de
  ocfs2) cf:heartbeat:Filesystem. La CLAVE AL CONFIGURAR OCFS2 es que sólo
  puedes formatear el sistema de ficheros que este resource monta o desmonta
  tras tener funcionando su soporte de cluster, en este caso STONITH y los dos
  RA anteriores de pacemaker, pues los módulos OCFS2 en el kernel necesitan de
  los servicios que proveen esos demonios, una vez que hemos optado por
  pacemaker en lugar del soporte nativo de clusterización de aquellos.

<p>
  "Before you can create OCFS2 volumes, you must configure the following
   resources:
</p><ul>
<li>DLM (cloneado como dijimos)
</li>
<li>O2CB (cloneado también: igual que los módulos del kernel para ocfs2 los
     cargas en todos, este recurso iface CRM y CM&amp;ML también).
</li>
<li>and a STONITH resource (shared fs = fencing!)"

</li>
</ul>

<p>   Entonces será luego cuando formateemos y registremos la primitiva FS que lo
   montaría, ahora vamos con esos 3 recursos.
</p></li>
</ul>


<p>
Tests preliminares; precisan que corosync esté cargado:
</p>


<pre class="src src-sh"><span style="color: #ff7f24;">## </span><span style="color: #ff7f24;">DLM:</span>
modprobe -v dlm
modprobe -v lock_dlm
modprobe -v configfs
/usr/sbin/dlm_controld.pcmk -q 0 -D <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">-h para help</span>

crm ra info ocf:pacemaker:controld
ocf-tester -n DLM -o <span style="color: #eedd82;">daemon</span>=<span style="color: #ffa07a;">"/usr/sbin/dlm_controld.pcmk"</span> /usr/lib/ocf/resource.d/pacemaker/controld

<span style="color: #ff7f24;">## </span><span style="color: #ff7f24;">O2CB:</span>
modprobe -v ocfs2_stackglue
modprobe -v ocfs2_stack_user
<span style="color: #b0c4de;">echo</span> pcmk &gt; /sys/fs/ocfs2/cluster_stack <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">si no usa la iface nativa en vez de pacemaker</span>
modprobe -v ocfs2
modprobe -v configfs
<span style="color: #ff7f24;">#</span><span style="color: #ff7f24;">(hay otros pero son para ocfs2 sin pacemaker etc ocfs2_stack_o2cb ocfs2_dlmfs)</span>
<span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">modprobe -r ocfs2; modprobe -r ocfs2_stack_user</span>
/usr/sbin/ocfs2_controld.pcmk -D <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">-h para help</span>

crm ra info ocf:pacemaker:o2cb
ocf-tester -n O2CB /usr/lib/ocf/resource.d/pacemaker/o2cb

<span style="color: #ff7f24;">#</span><span style="color: #ff7f24;">killall ... si es necesario.</span>

<span style="color: #ff7f24;">#</span><span style="color: #ff7f24;">xargs --null --max-args=1 echo &lt; /proc/`pidof /usr/sbin/corosync`/environ | less</span>
<span style="color: #ff7f24;">#</span><span style="color: #ff7f24;">vim /usr/sbin/aisexec</span>
<span style="color: #ff7f24;">##</span><span style="color: #ff7f24;">!/bin/sh</span>
<span style="color: #ff7f24;">#</span><span style="color: #ff7f24;">export COROSYNC_DEFAULT_CONFIG_IFACE="openaisserviceenableexperimental:corosync_parser"</span>
<span style="color: #ff7f24;">#</span><span style="color: #ff7f24;">corosync "$@"</span>


</pre>

<p>
strace /usr/sbin/aisexec -f
strace -o /tmp/dlm.txt /usr/sbin/dlm<sub>controld</sub>.pcmk -D
ocf-tester -n O2CB /usr/lib/ocf/resource.d/pacemaker/o2cb
</p>
</div>

</div>

<div id="outline-container-4-6-5" class="outline-4">
<h4 id="sec-4-6-5"><span class="section-number-4">4.6.5</span> Anotaciones sobre el recurso IPAddr2, IP flotante, en primary-primary: estrategia "load sharing" con clone</h4>
<div class="outline-text-4" id="text-4-6-5">

<ul>
<li>El recurso IPAddr2 para la IP virtual, tiene esta problemática cuando, usando
  primary-primary, queremos que los servicios en cada nodo sirvan
  concurrentemente:
<ul>
<li>Si creamos dos IP una localizada en cada nodo, perdemos la tolerancia a
     fallos del lado del servidor.
</li>
<li>Si dejamos una sóla IP, no se aprovecha la posibilidad de escrituras
     concurrentes en ambos nodos.
</li>
<li>Si dejamos una sóla IP pero clonada, se eliminan los dos problemas
     anteriores pero aparecen otros:
<ul>
<li>Dado el descubrimiento del host que registra la IP mediante ARP, cómo
       controlar las colisiones y los mecanismos ARP para evitar éstas.

<p>
       Se usa una técnica de balanceo llamada "load sharing", con soporte en
       NETFILTER para automatizarlo, más un demonio que hace de heartbeat,
       creo. En esta técnica los paquetes llegan a todos los nodos (una
       multicast arp), pero está predefinido quién procesará el paquete en
       capas superiores, véase gestión de sesión:
</p>
</li>
<li>Gestionar la sesión.

<p>
       Parece que el sistema asocia una comunicación a un nodo a través de la
       IP y puerto de origen (lo que indicas en el primitive).
</p></li>
</ul>

</li>
</ul>

</li>
</ul>


<p>
&hellip; vamos a ver qué hace el RA IPAddr2 (es un script bash):
</p>
<ul>
<li>El RA IPAddr2 comienza con una función de inicialización.
     Inicializa desde las variables OCF
<ul>
<li>BASE_IP es la ip virtual del cluster
</li>
<li>NIC, NETMASK, BRDCAST. Pero después ejecuta un pequeño comando, findif,
       pasándole BASE_IP. Si devuelve (al menos algún?) resultados los usa para
       rellenarlas, si no devuelve sale.
</li>
<li>IF_LABEL es el alias de la iface, por ejem si el NIC es eth0, el
       IF_LABEL puede ser eth0:1.
</li>
<li>IF_MAC, que será compartida. Excepto si usas un hub, debiera ser
       multicast. Si no se la das, él crea una multicast. Esa mac se ha de
       enviar a todos porque todos deben tener la misma MAC.
</li>
<li>ARP_INTERVAL/REPEAT/BACKGROUND/NETMASK. ARP_NETMASK tiene como default a
       fffffff.
</li>
<li>LVS_SUPPORT
</li>
<li>IP_INC_GLOBAL se le asigna OCF_RESKEY_CRM_meta_clone_max. Entonces si
       este clone_max es mayor que uno y $OCF_RESKEY_unique_clone_address no
       está definido, se entiende que estamos en un escenario de load sharing,
       es decir de IP virtual cloneada; ésto el script lo almacena seteando
       IP_CIP=yes. Además 1) Se establece IP_CIP_HASH a sourceip-sourceport o
       lo que configurases en el primitive, 2) Si no le configuraste (es lo
       típico) en el primitive la mac multicast, la crea haciendo el md5sum del
       triplete ip netmask bdcast y extrayendo los primeros 12 nibles y
       asegurándose que el primero es impar, ya que entonces esa MAC ES
       MULTICAST 3) establece el fichero de seguimiento de conexiones asociado
       a la cadena especial de iptables "CLUSTERIP" y la IP virtual que usamos:
       IP_CIP_FILE es igual por tanto a /proc/net/ipt_CLUSTERIP/$BASEIP

</li>
</ul>

<p>     Además de inicializar esas variables, se asegura de que somos root, no
     permite usar LVS si se está usando también load sharing (el script detecta
     la variable IP_CIP=yes).
</p>
</li>
<li>Después, se define la función ip_served, que intenta distinguir si ya nos
  estábamos ocupando en el pasado de esa BASEIP o no:
<ul>
<li>esa ip debe pertenecer a alguna iface, si no es que no la estamos
       sirviendo: devuelve no.
</li>
<li>si, sabiendo por el check anterior que esa ip está en una iface, no
       usamos ip cloneadas (IP_CIP está vacía), devuelve que te ocupas de ella,
       ok. Una excepción: si tienes la ip pero está en la iface 'lo*' y está
       activado el soporte lvs (si está en 'lo*' es porque eres un server real,
       no un balanceador).
</li>
<li>si es una ip en una iface y usamos ip cloneadas (IP_CIP=yes): si no
       existe el fichero conntrack (IP_CIP_FILE) devolvemos partial, si sí
       existe miramos en él a ver si estamos, si no estamos devolvemos
       partial2. Lo que mira es ésto (IP_INC_NO es 1 o
       OCF_RESKEY_CRM_meta_clone +1):
</li>
</ul>

</li>
</ul>




<pre class="src src-text">if egrep -q "(^|,)${IP_INC_NO}(,|$)" $IP_CIP_FILE ; then echo ok; fi
</pre>

<ul>
<li>Cuando se ejecuta el RA con start, se llama a la función ip<sub>start</sub>. Ésta
  llama a la anterior ip<sub>served</sub>. Si ip<sub>served</sub> responde que ok, pues no se
  hace nada más. Si responde que no se hace:
<ul>
<li>Habida una ip de cluster se usa ip_conntrack con la regla:
</li>
</ul>

</li>
</ul>




<pre class="src src-sh">$<span style="color: #eedd82;">IPTABLES</span> -I INPUT -d $<span style="color: #eedd82;">BASEIP</span> -i $<span style="color: #eedd82;">NIC</span> -j CLUSTERIP <span style="color: #ffa07a;">\</span>
                --new <span style="color: #ffa07a;">\</span>
                --clustermac $<span style="color: #eedd82;">IF_MAC</span> <span style="color: #ffa07a;">\</span>
                --total-nodes $<span style="color: #eedd82;">IP_INC_GLOBAL</span> <span style="color: #ffa07a;">\</span>
                --local-node $<span style="color: #eedd82;">IP_INC_NO</span> <span style="color: #ffa07a;">\</span>
                --hashmode $<span style="color: #eedd82;">IP_CIP_HASH</span>
</pre>

<ul>
<li>CLUSTERIP es el nombre de una cadena predefinida (véase EXTENSIONS en la
       man) y ofrece parte del mecanismo para compartir una ip y mac multicast
       entre los oredenadores de un cluster sin que haya un balanceador
       delante. Así, por un lado todos los paquetes de entrada se reciben por
       todos si hay un hub, pero también si hay un switch pues éste registra
       una dirección multicast que le obliga a copiarlos; y por otro lado cada
       nodo registra con ip<sub>conntrack</sub> si está sirviendo una conexión o no.
       Está claro que todos los nodos participantes están enlazados a nivel 2,
       que esta técnica de balanceo permite: no tener balanceador y evitar
       (excepto cuando tienes varios) un SPF; no ahorra bitrate usado en la
       línea (llega a todos), pero no todos lo procesan, lo "suben", por lo que
       sí ahorra procesamiento en las máquinas y procesos servidores (balanceo
       de nivel 4, con estado nivel 3, sin modificar a ningún nivel).

<p>       
       La dificultad está pues en que sólo uno procese la comunicación (en el
       caso de TCP, sólo uno responde un SYN/ACK ante el primer SYN y el
       tráfico posterior) y que la decisión de quíen lo hace se tome
       instantáneamente. Ambos se consiguen repartiendo a priori quién se ocupa
       de qué IP's. Cuando esta técnica de load sharing se implementa
       manualmente, ese acuerdo se puede hacer definiendo las ip pares o
       impares si hay dos nodos, las que acaben en 1,2,3 si hay 3&hellip; pero si
       usamos la cadena CLUSTERIP podemos pasar &ndash;hashmode y hará algo parecido
       teniendo en cuenta la ip de origen, ip y puerto de origen, o ip y puerto
       de origen más puerto de destino. La aplicación del filtro a todo tráfico
       puede no ser conveniente (y si queremos hacer un ssh a una de las
       máquinas?), entonces añadiríamos otras reglas iptables a mano etc
       <a href="http://lartc.org/autoloadbalance.html">http://lartc.org/autoloadbalance.html</a>
</p>
<p>
       También hay soporte para que, si un nodo cae, los otros se enteran y
       asumen su parte. Soporte por parte del script, véase sendarp ahora
       después, no por parte de la cadena CLUSTERIP, entiendo - a pesar de lo
       que sugiere en <a href="http://www.rkeene.org/projects/info/wiki/102">http://www.rkeene.org/projects/info/wiki/102</a>)
</p></li>
</ul>



<p>
       Luego la función init añade la iface (lo de la mac multicast sólo se le
       da a la regla de iptables en chain CLUSTERIP, no ahora, pero esa cadena
       ya sabrá qué hacer puesto que también le das la ip que acabamos de
       registrar). Si tenemos soporte LVS, el que esta IP esté activada
       implicaría que estamos haciendo de balanceador y no de servidor real;
       entonces lo que hace es eliminar el alias en "lo" si lo había.
</p>
<p>
       Por último ejecuta sendarp (excepto si la iface es lo* o ib* -
       infiniband -) y el demonio lo hace a un ritmo regular: "Run send<sub>arp</sub> to
       note peers about new mac address. repeatinterval-ms: timing, in
       milliseconds of sending arp packets -&gt; For each ARP announcement
       requested, a pair of ARP packets is sent, an ARP request, and an ARP
       reply. Creo que se refiere a que había dos modalidades de
       ARP-announcement, como request (lo que recomienda el rfc) y como replay,
       y por seguridad usa las dos: This is becuse some systems ignore one or
       the other, and this combination gives the greatest chance of success"
       Entonces send<sub>arp</sub> es un demonio que hace de heartbeat de este host,
       entiendo.
</p>
<ul>
<li>La función ip_stop se ejecuta al llamar al RA con stop. Mata al demonio
  sendarp. Comprueba que estamos usando la IP virtual realmente. Elimina la
  regla de iptables en cadena CLUSTERIP, desregistra la IP (excepto si
  soporte LVS, entonces pone la IP como alias en "lo*" puesto que estamos
  pasando a ser un servidor real, no un balanceador).
</li>
</ul>



<ul>
<li>¿Hace falta algún tipo de soporte sysctl? Creo que no.
<ul>
<li>Además, entiendo que net.ipv4.ip_nonlocal_bind=1 no es lo que resolvió un
      problema a este tipo: <a href="http://en.it-usenet.org/thread/18723/3438/">http://en.it-usenet.org/thread/18723/3438/</a>

<p>
      NO: ip_nonlocal_bind permitía escuchar en IP no locales, es decir que puedas
      sniffar paquetes que van a cierta IP sin tú tener registrada ésta en
      alguna iface. Si ésto fuese importante, la única explicación que me he
      hecho es: pones la IP en un nodo, ése responde a las resoluciones ARP
      para que le manden paquetes TCP/IP, pero esos paquetes que se mandan
      llegan a los dos nodos, no sólo al que responde ARP, así que una regla de
      iptables y no el nivel 2 les permita descartar los que deben escuchar de
      los que no. Pero para que éso sea así, deben estar conectados
      directamente y por difusión (hub, no switch), no? En otro caso ambos
      tendrían que ponerse la misma MAC.
</p></li>
</ul>

</li>
</ul>





<pre class="src src-text">-node-max="2"
 # (as&#237; con clones lo hace tanto en el manual de pacemaker, como en el de suse
 #  para usar el cluster para samba:
 #  https://www.suse.com/documentation/sle_ha/book_sleha/?page=/documentation/sle_ha/book_sleha/data/sec_ha_samba_basicconf.html)
</pre>

</div>

</div>

<div id="outline-container-4-6-6" class="outline-4">
<h4 id="sec-4-6-6"><span class="section-number-4">4.6.6</span> Recursos DRBD, cLVM, OCFS, LVM</h4>
<div class="outline-text-4" id="text-4-6-6">




<pre class="src src-text"># DRBD
crm configure
primitive DRBDr0 ocf:linbit:drbd \
        params drbd_resource="r0" \
        operations $id="resDRBD-operations" \
        op monitor interval="20" role="Master" timeout="20" \
        op monitor interval="30" role="Slave" timeout="20"
 # (en manual drbd s&#243;lo usa las dos primeras l&#237;neas del primitive)
 # (en manual pacemaker s&#243;lo a&#241;ade op monitor interval="60s")
ms DRBDr0_Clone DRBDr0 meta master-max="2" clone="2" resource-stickines="100" notify="true" interleave="true"
 # (en manual pacemaker usa tras meta a:
 #  master-max="1" master-node-max="1" clone-max="2" clone-node-max="1" notify="true"
 # )
 # (en manual drbd a&#241;ade s&#243;lo el meta master-max="2" clone="2")

# Recurso de control del Distributed Lock Manager, que debe activarse en todos los
 # nodos (clones) y antes de clvm u ocfs2 (orden al def el grupo)
 primitive DLM ocf:pacemaker:controld op monitor interval="60" timeout="60"
 clone DLM_Clone DLM meta globally-unique="false" interleave="true"
 # (en manual drbd s&#243;lo usa &lt;nombre&gt; &lt;ra&gt;, sin m&#225;s opciones op/meta)
 # (en manual suse usa op monitor interval="60" timeout="60")

# Recurso Cluster Control de OCFS2:
 primitive O2CB ocf:pacemaker:o2cb op monitor interval="120s" timeout="60"
 clone O2CB_Clone O2CB meta globally-unique="false" interleave="true"
 # en la gu&#237;a de ubun usa ese RA, en la de suse/drbd usa ocf:ocfs2:o2cb pero
 # debe ser el mismo porque no hay otro haciendo un apt-file search en debian.
 # ... el recurso FS se registrar&#225; luego, tras 1) levantar los RA: DRBD, cLVM, DLM, OCFS2, Stonith; y 2) formatear.
 # (en manual drbd s&#243;lo usa &lt;nombre&gt; &lt;ra&gt;, sin m&#225;s opciones op/meta)
 # (en manual suse usa op monitor interval="60" timeout="60")

# Recurso Cluster LVM, seg&#250;n el manual de Suse:
 primitive CLVM ocf:lvm2:clvmd params daemon_timeout="30"
 # daemon_options="-d2". Si exclusive="yes" s&#243;lo podr&#237;a estar activo en uno.
 clone CLVM_Clone CLVM meta interleave="true" ordered="true"
                      # ... clone-max="2" clone-node-max="1"

 primitive LVMvg1 ocf:heartbeat:LVM params volgrpname="data1vg" op monitor interval="60" timeout="60"
 # as&#237; es en manual de suse, en otros he visto: op start interval="0" timeout="30" op stop interval="0" timeout="30" # exclusive="true"
 clone LVMvg1_Clone LVMvg1 meta interleave="true" ordered="true"

# Location, Colocation, Order:
colocation DLM-with-DRBDr0 inf: DLM_Clone DRBDr0_Clone:Master
colocation O2CB-with-DLM inf: O2CB_Clone DLM_Clone
order order-drbd-to-lvm inf: DRBDr0_Clone:promote DLM_Clone:start O2CB_Clone:start CLVM_Clone:start LVMvg1_Clone:start

# En suse viene:
#group base-group DRBDr0 DLM O2CB CLVM LVMvg1 FSr0
#clone base-clone base-group meta interleave="true"
#
#
#En manual drbd viene:
#order o_ocfs2 ms_drbd_ocfs2:promote cl_ocfs2mgmt:start cl_fs_ocfs2:start
#colocation c_ocfs2 cl_fs_ocfs2 cl_ocfs2mgmt ms_drbd_ocfs2:Master
#
# En manual pacemaker viene:
#order WebFS-after-WebData inf: DRBD_Clone:promote FS:start
#order WebSite-after-WebFS inf: FS Apache
#order apache-after-ip inf: ClusterIP Apache

commit
quit
</pre>


<p>
Note the master-max=2 meta variable; it enables dual-Master mode for a
Pacemaker master/slave set. This requires that allow-two-primaries is also set
to yes in the DRBD configuration. Otherwise, Pacemaker will flag a
configuration error during resource validation.
</p>
<p>
TODO: ClusterIP1 aún no tiene clon (si bien no puede tenerlo si no lo tiene
mysql, o creo ClusterIP2 para mysql)
</p>



<pre class="src src-sh">crm_resource -l
crm_resource -L
crm_mon -1      <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">si problemas, en ambos:</span>
invoke-rc.d corosync stop &amp;&amp; sleep 1 &amp;&amp; invoke-rc.d corosync start
</pre>


</div>
</div>

</div>

<div id="outline-container-4-7" class="outline-3">
<h3 id="sec-4-7"><span class="section-number-3">4.7</span> Creación de volúmenes LVM2</h3>
<div class="outline-text-3" id="text-4-7">

<p>Para debug en foreground: clvmd -T20 -Icorosync -d1
</p>



<pre class="src src-sh">ps aux | grep clvmd
invoke-rc.d clvm start
</pre>

<p>
&hellip; este paso daba error si lo hacía tras instalar clvm, también después, tras
instalar corosync, pero no ahora; creo que es porque es ahora cuando está
definido el DLM (también podría influir stonith?) y de alguna forma clvm no
podía registrarse en corosync. Sea como sea, clvmd debería estar cargado en
ambos hosts antes de proseguir.
</p>
<ul>
<li>En d1 por ejem:
</li>
</ul>




<pre class="src src-sh">drbdadm role r0       <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">(debe estar a Primary o el siguiente comando fallar&#225;</span>
                      <span style="color: #ff7f24;">#  </span><span style="color: #ff7f24;">incluso aunque /dev/drbd0 exista )</span>
pvcreate /dev/drbd0   <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">inicializo ese disco para usarlo con LVM2</span>
pvdisplay             <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">para que todo vaya bien, el UUID que muestre</span>
                      <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">este pvdisplay debe ser el mismo que el que</span>
                      <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">se muestre en otros nodos (podr&#237;amos ir ahora</span>
                      <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">quiz&#225;s a d2 y comprobarlo).</span>

vgcreate --clustered y data1vg /dev/drbd0      <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">N&#243;tese "--clustered y"!!!</span>
                                               <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Ser&#225; suficiente con &#233;so.</span>

lvcreate --size 10M -n root.r0        data1vg
lvcreate --size 15M -n r0.etc         data1vg
lvcreate --size 50M -n r0.varlibmysql data1vg
lvcreate --size 10M -n r0.varwww      data1vg
</pre>


<ul>
<li>En d2:
</li>
</ul>




<pre class="src src-sh">pvdisplay
lvdisplay
</pre>

<p>
&hellip; aparecen todos los volúmenes registrados allí también.
</p>
</div>

</div>

<div id="outline-container-4-8" class="outline-3">
<h3 id="sec-4-8"><span class="section-number-3">4.8</span> Formateo OCFS2</h3>
<div class="outline-text-3" id="text-4-8">

<ul>
<li>En d1 por ejemplo:
</li>
</ul>




<pre class="src src-sh">crm configure show
crm_mon -1
</pre>

<p>
&hellip; debieran estar definidos los recursos, y ejecutándose (O2CB y DLM). Sólo
entonces:
</p>



<pre class="src src-text">Opciones de mkfs.ocfs2:
 *  -L label -N n_concurr_nodes -C clustersize -b block_size --fs-features nobla,ble...
 *  http://oss.oracle.com/projects/ocfs2/ -N tiene un m&#225;ximo de 32.
</pre>



<pre class="src src-sh">mkfs.ocfs2 -N 2 -L OCFS2_DRBDr0_root        /dev/data1vg/root.r0
mkfs.ocfs2 -N 2 -L OCFS2_DRBDr0_etc         /dev/data1vg/r0.etc
mkfs.ocfs2 -N 2 -L OCFS2_DRBDr0_varlibmysql /dev/data1vg/r0.varlibmysql
mkfs.ocfs2 -N 2 -L OCFS2_DRBDr0_varwww      /dev/data1vg/r0.varwww

mkdir -p /mnt/drbdr0  <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">el punto de montaje del vol root.r0, que usaremos luego.</span>
</pre>


<p>
Notas:
</p><ul>
<li>OCFS2 permite quotas. Se activan en el fs (úsese features "usrquota" o
  "grpquota" con mkfs.ocfs2 al formatear, o con tunefs.ocfs2 luego). Se
  administran con el set típico en linux: apt-get install quota quotatool, etc
  y no necesitan otras utilidades como quotacheck (está toda la funcionalidad
  implementada driver/metadata ocfs2/fsck.ocfs2).
</li>
<li>Respecto a montar a mano, hay que asegurarse antes de que los recursos dlm,
  o2cb y stonith están online, que no tienes el montaje bajo control de
  pacemaker (o recurso FS offline), entonces puedes usar mount normalmente y,
  antes de pasar el control a pacemaker, umount.
  <a href="https://www.suse.com/documentation/sle_ha/book_sleha/data/sec_ha_ocfs2_mount.html">https://www.suse.com/documentation/sle_ha/book_sleha/data/sec_ha_ocfs2_mount.html</a>
</li>
<li>Speed, local mode. Véase:
  <a href="https://oss.oracle.com/pipermail/ocfs2-users/2009-April/003472.html">https://oss.oracle.com/pipermail/ocfs2-users/2009-April/003472.html</a>
</li>
<li>Nota: Hacer samba HA = samba + ctdb + shared FS
  <a href="https://www.suse.com/documentation/sle_ha/book_sleha/data/cha_ha_samba.html">https://www.suse.com/documentation/sle_ha/book_sleha/data/cha_ha_samba.html</a>
</li>
</ul>

</div>

</div>

<div id="outline-container-4-9" class="outline-3">
<h3 id="sec-4-9"><span class="section-number-3">4.9</span> Sorporte de Pacemaker para el resto de recursos (FS en adelante)</h3>
<div class="outline-text-3" id="text-4-9">




<pre class="src src-text">crm configure

# FSr0
primitive FSroot_r0 ocf:heartbeat:Filesystem params device="/dev/data1vg/root.r0" directory="/mnt/drbdr0" fstype="ocfs2" options="rw,noatime,acl" op monitor interval="120s" timeout="40"
primitive FSr0_etc ocf:heartbeat:Filesystem params device="/dev/data1vg/r0.etc" directory="/mnt/drbdr0/etc" fstype="ocfs2" options="rw,noatime,acl" op monitor interval="120s" timeout="40"
primitive FSr0_varlibmysql ocf:heartbeat:Filesystem params device="/dev/data1vg/r0.varlibmysql" directory="/mnt/drbdr0/var/lib/mysql" fstype="ocfs2" options="rw,noatime,acl" op monitor interval="120s" timeout="40"
primitive FSr0_varwww ocf:heartbeat:Filesystem params device="/dev/data1vg/r0.varwww" directory="/mnt/drbdr0/var/www" fstype="ocfs2" options="rw,noatime,acl" op monitor interval="120s" timeout="40"
 # (es como viene en suse, excepto que el manual de drbd a&#241;ade options=rw,noatime)
group FSr0 FSroot_r0 FSr0_etc FSr0_varlibmysql FSr0_varwww
 # ... a diferencia de m-s, los FS s&#237; llevan clones porque no se desactivan en
 #     un nodo cuando est&#225;n activados en el otro:
clone FSr0_Clone FSr0 meta interleave="true" ordered="true"
 # ... entiendo que no necesito definir un order porque por defecto se va a 
 #     respetar el orden en que he declarado los elementos del grupo.



# Links
primitive LINKS_varwww ocf:heartbeat:symlink \
     params target="/mnt/drbdr0/var/www" \
     link="/var/www" backup_suffix=".active"
primitive LINKS_etcapache2 ocf:heartbeat:symlink \
     params target="/mnt/drbdr0/etc/apache2" \
     link="/etc/apache2" backup_suffix=".active"
primitive LINKS_varlibmysql ocf:heartbeat:symlink \
     params target="/mnt/drbdr0/var/lib/mysql" \
     link="/var/lib/mysql" backup_suffix=".active"
primitive LINKS_etcmysql ocf:heartbeat:symlink \
     params target="/mnt/drbdr0/etc/mysql" \
     link="/etc/mysql" backup_suffix=".active"
primitive LINKS_etcphpmyadmin ocf:heartbeat:symlink \
     params target="/mnt/drbdr0/etc/phpmyadmin" \
     link="/etc/phpmyadmin" backup_suffix=".active"


group linksgroup LINKS_varwww LINKS_etcapache2 LINKS_varlibmysql LINKS_etcmysql LINKS_etcphpmyadmin
clone linksgroup_Clone linksgroup meta interleave="true"

 #colocation col_sym_with_util1 inf: p_sym_leseb ms_drbd_util1:Master

order LINKS_linksgroup-after-FSr0 inf: FSr0 linksgroup
 #order ord_sym_after_util1 inf: ms_drbd_util1:promote p_sym_leseb:start

# MySQL
# entiendo que MySQL podr&#237;a clonarse al estar usando OCFS2. Pero parece
# que InnoDB no est&#225; preparado para ello, de alguna forma. Pienso, de hecho,
# que un write no puede quedarse en memoria ni en otro fichero m&#225;s que el
# definitivo para que la cosa funcionase, y no s&#233; si es as&#237;.
# http://www.gossamer-threads.com/lists/linuxha/pacemaker/64357?do=post_view_threaded#64357
# para ello est&#225; Galera con MariaDB:
# http://www.hastexo.com/resources/docs/mysqlgalera-pacemaker-high-availability-clusters
# Rese&#241;o al menos que, si va a usarse s&#243;lo una instancia a la vez, podr&#237;a tener
# una IP virtual propia por si falla el mysql de un nodo pero s&#243;lo ese servicio.
# Nota: he le&#237;do que el RA de mysql soporta acciones relacionadas con sus
#       mecanismos de replicaci&#243;n.
##
#
# Para PGsql se crearon en 2011 al menos dos RA con soporte para replicaci&#243;n
# master-slave:
# - http://oss.clusterlabs.org/pipermail/pacemaker/2011-November/012196.html
#     https://github.com/t-matsuo/resource-agents/wiki/Resource-Agent-for-PostgreSQL-9.1-streaming-replication
# - http://oss.clusterlabs.org/pipermail/pacemaker/2011-November/012202.html
# ... parece que el primero ha entrado en pacemaker1.1:
# https://raw.github.com/ClusterLabs/resource-agents/master/heartbeat/pgsql
# ... sobre su uso:
# http://www.pgcon.org/2013/schedule/attachments/279_PostgreSQL_9_and_Linux_HA.pdf
primitive MySQL ocf:heartbeat:mysql
 #clone MySQL_Clone MySQL meta interleave="true" target-role="Started"

# Apache2: (necesita /server-status de mod_status operativo)
primitive Apache2 ocf:heartbeat:apache params configfile=/etc/apache2/apache2.conf op monitor interval=1min statusurl="http://127.0.0.1/server-status/"
 #primitive Apache2 ocf:heartbeat:apache params configfile="/etc/apache2/apache2.conf" options="-DSSL" operations $id="WebSite-operations" op start interval="0" timeout="40" op stop interval="0" timeout="60" op monitor interval="60" timeout="120" start-delay="0" statusurl="http://127.0.0.1/server-status/" meta target-role="Started"
clone Apache2_Clone Apache2 meta interleave="true" target-role="Started" #meta clone-max="2"
 # (en manual pacemaker viene el op monitor interval=1min)


# Location, Colocation, Order:
colocation FS-with-O2CB inf: FS_Clone O2CB_Clone

commit
quit
</pre>



</div>

<div id="outline-container-4-9-1" class="outline-4">
<h4 id="sec-4-9-1"><span class="section-number-4">4.9.1</span> Recurso PostgreSQL con streaming replication síncrona master-slave</h4>
<div class="outline-text-4" id="text-4-9-1">

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<caption></caption>
<colgroup><col class="left" />
</colgroup>
<tbody>
<tr><td class="left"><a href="https://github.com/t-matsuo/resource-agents/wiki/Resource-Agent-for-PostgreSQL-9.1-streaming-replication">https://github.com/t-matsuo/resource-agents/wiki/Resource-Agent-for-PostgreSQL-9.1-streaming-replication</a></td></tr>
<tr><td class="left"><a href="http://www.pgcon.org/2013/schedule/events/546.en.html">http://www.pgcon.org/2013/schedule/events/546.en.html</a></td></tr>
<tr><td class="left"><a href="http://www.pgcon.org/2013/schedule/attachments/279_PostgreSQL_9_and_Linux_HA.pdf">http://www.pgcon.org/2013/schedule/attachments/279_PostgreSQL_9_and_Linux_HA.pdf</a></td></tr>
</tbody>
</table>



<p>
No pretendo usar postgresql con su datadir en DRBD, no entiendo cómo podrían
funcionar varios nodos sobre ello, ni cómo configurar éso si fuese posible. Lo
que describo ahora es sin mover el datadir a drbd.
</p>
<p>
Tendremos una IP exclusiva para replicaciones, y otra para que los servicios se
conecten a pg (entiendo que podrían ser la misma y que la diferencia es sólo la
típica de tener una red distinta para replicación eficiente; a su vez, la ip
para servicios puede ser la ip virtual del cluster en que escuchan esos
servicios, pienso).
</p>
<ul>
<li>Create data directory on one node:
</li>
</ul>




<pre class="src src-sh">apt-get install postgresql
update-rc.d postgresql disable
</pre>

<ul>
<li>Setup the postgresql.conf, pg_hba.conf and conf files for replication:
  <a href="http://www.rassoc.com/gregr/weblog/2013/02/16/zero-to-postgresql-streaming-replication-in-10-mins/">http://www.rassoc.com/gregr/weblog/2013/02/16/zero-to-postgresql-streaming-replication-in-10-mins/</a>
</li>
</ul>




<ul>
<li>Master:
</li>
</ul>




<pre class="src src-sh">su postgres
psql -c <span style="color: #ffa07a;">"CREATE USER replicator REPLICATION LOGIN ENCRYPTED PASSWORD 'thepassword';"</span>
<span style="color: #00ffff;">exit</span>
vim /etc/postgresql/9.1/main/postgresql.conf
</pre>


<pre class="src src-sh">listen_address = <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">make sure we're listening as appropriate</span>
wal_level = hot_standby
max_wal_senders = 3
wal_keep_segments = 8 
checkpoint_segments = 8
hot_standby = on
<span style="color: #ff7f24;">#</span><span style="color: #ff7f24;">We&#8217;re configuring 8 WAL segments here; each is 16MB. If you expect your</span>
<span style="color: #ff7f24;">#</span><span style="color: #ff7f24;">database to have more than 128MB of changes in the time it will take to make a</span>
<span style="color: #ff7f24;">#</span><span style="color: #ff7f24;">copy of it across the network to your slave, or in the time you expect your</span>
<span style="color: #ff7f24;">#</span><span style="color: #ff7f24;">slave to be down for maintenance or something, then consider increasing those</span>
<span style="color: #ff7f24;">#</span><span style="color: #ff7f24;">values.</span>
</pre>


<pre class="src src-sh">vim /etc/postgresql/9.1/main/pg_hba.conf
</pre>


<pre class="src src-text">#host    all             all              127.0.0.1/32             trust
hostssl replication     replicator      &lt;replication ip&gt;            md5
</pre>


<ul>
<li>Slave: la clave, creo, es que es la misma configuración que en master, ya
  que será en la línea de comando al arrancar el servicio cuando se decida el
  rol. Por tanto, el RA de pacemaker se encargará de pasar el flag correcto.
</li>
</ul>




<pre class="src src-sh">su postgres
psql -c <span style="color: #ffa07a;">"CREATE USER replicator REPLICATION LOGIN ENCRYPTED PASSWORD 'thepassword';"</span>
<span style="color: #00ffff;">exit</span>
vim /etc/postgresql/9.1/main/postgresql.conf <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">igual que antes</span>
</pre>


<pre class="src src-sh">listen_address = <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">make sure we're listening as appropriate</span>
wal_level = hot_standby
max_wal_senders = 3
wal_keep_segments = 8
checkpoint_segments = 8
hot_standby = on
</pre>


<pre class="src src-sh">vim /etc/postgresql/9.1/main/pg_hba.conf     <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">igual que antes</span>
</pre>


<pre class="src src-text">hostssl replication     replicator      &lt;replication ip&gt;            md5
</pre>

<ul>
<li>Do a basebackup onto the slave node.
</li>
</ul>




<pre class="src src-sh">invoke-rc.d postgresql stop
</pre>


<p> 
Cleaning up old cluster directory:
</p>


<pre class="src src-sh">sudo -u postgres rm -rf /var/lib/postgresql/9.1/main
</pre>


<p> 
Starting base backup as replicator:
</p>


<pre class="src src-sh">sudo -u postgres pg_basebackup -h &lt;master&gt; -D /var/lib/postgresql/9.1/main -U replicator -v -P
</pre>


<ul>
<li>No need to create recovery.conf file for the Standby. The RA creates it itself.
    por tanto
</li>
</ul>




<pre class="src src-text"> (NO H&#193;GASE)
 echo Writing recovery.conf file
 sudo -u postgres bash -c "cat &gt; /var/lib/postgresql/9.2/main/recovery.conf &lt;&lt;- _EOF1_
 standby_mode = 'on'
 primary_conninfo = 'host=1.2.3.4 port=5432 user=replicator password=thepassword sslmode=require'
 trigger_file = '/tmp/postgresql.trigger'
_EOF1_
 "
</pre>

<p>
  &hellip; y sí relanzo a pg: 
</p>


<pre class="src src-sh">invoke-rc.d postgresql start
</pre>






<ul>
<li>Pacemaker conf:
</li>
</ul>




<pre class="src src-sh">crm configure edit
<span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">IP que ofrece pg con rol master a los slaves para replicaciones</span>
primitive vip-rep ocf:heartbeat:IPaddr2 <span style="color: #ffa07a;">\</span>
   params <span style="color: #eedd82;">ip</span>=<span style="color: #ffa07a;">"192.168.168.109"</span> <span style="color: #eedd82;">nic</span>=<span style="color: #ffa07a;">"eth0"</span> <span style="color: #ffa07a;">\</span>
   <span style="color: #eedd82;">cidr_netmask</span>=<span style="color: #ffa07a;">"24"</span> <span style="color: #ffa07a;">\</span>
   op start <span style="color: #eedd82;">interval</span>=<span style="color: #ffa07a;">"0s"</span> <span style="color: #eedd82;">timeout</span>=<span style="color: #ffa07a;">"60s"</span> on-fail=<span style="color: #ffa07a;">"stop"</span> <span style="color: #ffa07a;">\</span>
   op monitor <span style="color: #eedd82;">interval</span>=<span style="color: #ffa07a;">"10s"</span> <span style="color: #eedd82;">timeout</span>=<span style="color: #ffa07a;">"60s"</span> on-fail=<span style="color: #ffa07a;">"restart"</span> <span style="color: #ffa07a;">\</span>
   op stop <span style="color: #eedd82;">interval</span>=<span style="color: #ffa07a;">"0s"</span> <span style="color: #eedd82;">timeout</span>=<span style="color: #ffa07a;">"60s"</span> on-fail=<span style="color: #ffa07a;">"block"</span>

<span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">IP que ofrece pg con rol master accesible al resto de los servicios</span>
primitive vip-master ocf:heartbeat:IPaddr2 <span style="color: #ffa07a;">\</span>
   params <span style="color: #eedd82;">ip</span>=<span style="color: #ffa07a;">"192.168.168.108"</span> <span style="color: #eedd82;">nic</span>=<span style="color: #ffa07a;">"eth0"</span>
   <span style="color: #eedd82;">cidr_netmask</span>=<span style="color: #ffa07a;">"24"</span> <span style="color: #ffa07a;">\</span>
   op start <span style="color: #eedd82;">interval</span>=<span style="color: #ffa07a;">"0s"</span> <span style="color: #eedd82;">timeout</span>=<span style="color: #ffa07a;">"60s"</span> on-fail=<span style="color: #ffa07a;">"stop"</span> <span style="color: #ffa07a;">\</span>
   op monitor <span style="color: #eedd82;">interval</span>=<span style="color: #ffa07a;">"10s"</span> <span style="color: #eedd82;">timeout</span>=<span style="color: #ffa07a;">"60s"</span> on-fail=<span style="color: #ffa07a;">"restart"</span> 
   op stop <span style="color: #eedd82;">interval</span>=<span style="color: #ffa07a;">"0s"</span> <span style="color: #eedd82;">timeout</span>=<span style="color: #ffa07a;">"60s"</span> on-fail=<span style="color: #ffa07a;">"block"</span>

<span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">"You can create an additional IP resource to allow reads to</span>
<span style="color: #ff7f24;">#  </span><span style="color: #ff7f24;">be queried from Standby nodes as well".</span>
<span style="color: #ff7f24;">#</span>
<span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Supongo que es definir otra IP, clonearla y con colocation hacer</span>
<span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">que est&#233; siempre donde se haga de slave. El RA hace load balancing.</span>
<span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">http://oss.clusterlabs.org/pipermail/pacemaker/2011-November/012252.html</span>
<span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Pero para que sea &#250;til el resto de servicios tienen que saber que</span>
<span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">ah&#237; s&#243;lo se puede leer (o bien, pero &#233;sto no es as&#237; por defecto,</span>
<span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">saber c&#243;mo hacer que los slaves fuesen proxyes ante peticiones</span>
<span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">de escritura, como vi para slapd en su d&#237;a).</span>
<span style="color: #ff7f24;">#</span>
<span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Conclusi&#243;n: Paso de esta IP virtual para slaves por ahora, por lo</span>
<span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">tanto los pg no master son Hot Standby.</span>

group master-group vip-master vip-rep meta <span style="color: #eedd82;">ordered</span>=<span style="color: #ffa07a;">"false"</span>

<span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Recurso pg y clon multiestado:</span>
primitive pgsql ocf:heartbeat:pgsql <span style="color: #ffa07a;">\</span>
    params <span style="color: #eedd82;">repuser</span>=<span style="color: #ffa07a;">"stormdb"</span> <span style="color: #eedd82;">pgdba</span>=<span style="color: #ffa07a;">"stormdb"</span> <span style="color: #eedd82;">pgport</span>=<span style="color: #ffa07a;">"5472"</span> <span style="color: #ffa07a;">\</span>
    <span style="color: #eedd82;">pgctl</span>=<span style="color: #ffa07a;">"/usr/lib/postgresql/9.1/bin/pg_ctl"</span> <span style="color: #eedd82;">psql</span>=<span style="color: #ffa07a;">"/usr/bin/psql"</span> <span style="color: #ffa07a;">\</span>
    <span style="color: #eedd82;">pgdata</span>=<span style="color: #ffa07a;">"/var/lib/postgresql/9.1/main/"</span> <span style="color: #eedd82;">start_opt</span>=<span style="color: #ffa07a;">"-p 5472"</span> <span style="color: #ffa07a;">\</span>
    <span style="color: #eedd82;">rep_mode</span>=<span style="color: #ffa07a;">"sync"</span> <span style="color: #eedd82;">node_list</span>=<span style="color: #ffa07a;">"d1 d2"</span> <span style="color: #ffa07a;">\</span>
    <span style="color: #eedd82;">master_ip</span>=<span style="color: #ffa07a;">"192.168.168.109"</span> <span style="color: #eedd82;">stop_escalate</span>=<span style="color: #ffa07a;">"0"</span> <span style="color: #ffa07a;">\</span>
    <span style="color: #eedd82;">config</span>=<span style="color: #ffa07a;">"/etc/postgresql/9.1/main/postgresql.conf"</span> <span style="color: #ffa07a;">\</span>
    <span style="color: #eedd82;">pgctldata</span>=<span style="color: #ffa07a;">"/usr/lib/postgresql/9.1/bin/pg_controldata"</span> <span style="color: #ffa07a;">\</span>
    op start <span style="color: #eedd82;">interval</span>=<span style="color: #ffa07a;">"0s"</span> <span style="color: #eedd82;">timeout</span>=<span style="color: #ffa07a;">"60s"</span> on-fail=<span style="color: #ffa07a;">"restart"</span> <span style="color: #ffa07a;">\</span>
    op monitor <span style="color: #eedd82;">interval</span>=<span style="color: #ffa07a;">"7s"</span> <span style="color: #eedd82;">timeout</span>=<span style="color: #ffa07a;">"60s"</span> on-fail=<span style="color: #ffa07a;">"restart"</span> <span style="color: #ffa07a;">\</span>
    op monitor <span style="color: #eedd82;">interval</span>=<span style="color: #ffa07a;">"2s"</span> <span style="color: #eedd82;">role</span>=<span style="color: #ffa07a;">"Master"</span> <span style="color: #eedd82;">timeout</span>=<span style="color: #ffa07a;">"60s"</span> <span style="color: #eedd82;">onfail</span>=<span style="color: #ffa07a;">"restart"</span> <span style="color: #ffa07a;">\</span>
    op promote <span style="color: #eedd82;">interval</span>=<span style="color: #ffa07a;">"0s"</span> <span style="color: #eedd82;">timeout</span>=<span style="color: #ffa07a;">"60s"</span> on-fail=<span style="color: #ffa07a;">"restart"</span> <span style="color: #ffa07a;">\</span>
    op demote <span style="color: #eedd82;">interval</span>=<span style="color: #ffa07a;">"0s"</span> <span style="color: #eedd82;">timeout</span>=<span style="color: #ffa07a;">"60s"</span> on-fail=<span style="color: #ffa07a;">"stop"</span> <span style="color: #ffa07a;">\</span>
    op stop <span style="color: #eedd82;">interval</span>=<span style="color: #ffa07a;">"0s"</span> <span style="color: #eedd82;">timeout</span>=<span style="color: #ffa07a;">"60s"</span> on-fail=<span style="color: #ffa07a;">"block"</span> <span style="color: #ffa07a;">\</span>
    op notify <span style="color: #eedd82;">interval</span>=<span style="color: #ffa07a;">"0s"</span> <span style="color: #eedd82;">timeout</span>=<span style="color: #ffa07a;">"60s"</span>
 <span style="color: #ff7f24;">#</span><span style="color: #ff7f24;">restore_command="cp /var/lib/postgresql/9.1/main/pg_archive/%f %p"</span>

ms msPostgresql pgsql <span style="color: #ffa07a;">\</span>
    meta <span style="color: #ffa07a;">\</span>
    master-max=<span style="color: #ffa07a;">"1"</span> <span style="color: #ffa07a;">\</span>
    master-node-max=<span style="color: #ffa07a;">"1"</span> <span style="color: #ffa07a;">\</span>
    clone-max=<span style="color: #ffa07a;">"2"</span> <span style="color: #ffa07a;">\</span>
    clone-node-max=<span style="color: #ffa07a;">"1"</span> <span style="color: #ffa07a;">\</span>
    <span style="color: #eedd82;">notify</span>=<span style="color: #ffa07a;">"true"</span>

colocation rsc_colocation-1 inf: master-group msPostgresql:Master

order rsc_order-1 0: msPostgresql:promote master-group:start <span style="color: #eedd82;">symmetrical</span>=false
</pre>

<p>
Prueba:
</p>


<pre class="src src-sh">crm resource start msPostgresql
crm_mon -1r -A

sudo -u postgres psql -x -c <span style="color: #ffa07a;">"select * from pg_stat_replication;"</span>
</pre>

</div>
</div>

</div>

<div id="outline-container-4-10" class="outline-3">
<h3 id="sec-4-10"><span class="section-number-3">4.10</span> Alternativa a OCFS2: GFS2</h3>
<div class="outline-text-3" id="text-4-10">

<p><a href="https://wiki.ubuntu.com/ClusterStack/LucidTesting#Pacemaker.2C_drbd8_and_OCFS2_or_GFS2">https://wiki.ubuntu.com/ClusterStack/LucidTesting#Pacemaker.2C_drbd8_and_OCFS2_or_GFS2</a>
</p><ul>
<li>En todos:
</li>
</ul>




<pre class="src src-sh">apt-get install gfs-pcmk gfs2-tools
dpkg -L gfs-pcmk
</pre>


<ul>
<li>En d1 por ejemplo:
</li>
</ul>




<pre class="src src-sh"><span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">... configuraci&#243;n de propiedades globales y stonith</span>
<span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">... primitive DRBDr0 ocf:linbit:drbd params drbd_resource="r0" operations $id="resDRBD-operations" op monitor interval="20" role="Master" timeout="20" op monitor interval="30" role="Slave" timeout="20"</span>
<span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">... ms DRBDr0_Clone DRBDr0 meta master-max="2" clone="2" resource-stickines="100" notify="true" interleave="true"</span>
<span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">... primitive DLM ocf:pacemaker:controld op monitor interval="60" timeout="60"</span>
<span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">... clone DLM_Clone DLM meta globally-unique="false" interleave="true"</span>

<span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">Recurso Cluster Control de GFS2:</span>
crm configure primitive GFSD2 ocf:pacemaker:controld <span style="color: #ffa07a;">\</span>
        params <span style="color: #eedd82;">daemon</span>=<span style="color: #ffa07a;">"gfs_controld.pcmk"</span> <span style="color: #eedd82;">args</span>=<span style="color: #ffa07a;">""</span> <span style="color: #ffa07a;">\</span>
        op monitor <span style="color: #eedd82;">interval</span>=<span style="color: #ffa07a;">"120s"</span>
crm configure clone GFSD2_Clone GFSD2 <span style="color: #ffa07a;">\</span>
        meta globally-unique=<span style="color: #ffa07a;">"false"</span> <span style="color: #eedd82;">interleave</span>=<span style="color: #ffa07a;">"true"</span> target-role=<span style="color: #ffa07a;">"Started"</span>

<span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">... primitive CLVM ocf:lvm2:clvmd params daemon_timeout="30"</span>
<span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">... clone CLVM_Clone CLVM meta interleave="true" ordered="true" op monitor interval="60" timeout="60"</span>
<span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">... primitive LVMvg1 ocf:heartbeat:LVM params volgrpname="data1vg"</span>
<span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">... clone LVMvg1_Clone LVMvg1 meta interleave="true" ordered="true"</span>

<span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">... colocation DLM-with-DRBDr0 inf: DLM_Clone DRBDr0_Clone:Master</span>
crm configure colocation GFSD2-with-DLM inf: GFSD2_Clone DLM_Clone
crm configure order order-drbd-to-lvm inf: DRBDr0_Clone:promote DLM_Clone:start GFSD2_Clone:start CLVM_Clone:start

crm_mon -1
</pre>

<p>
&hellip; si todo bien, ya se puede formatear:
</p>
<ul>
<li>Formateo:
</li>
</ul>




<pre class="src src-sh">mkfs.gfs2 -p lock_dlm -j2 -t pcmk:OCFS2_DRBDr0_root        /dev/data1vg/root.r0
mkfs.gfs2 -p lock_dlm -j2 -t pcmk:OCFS2_DRBDr0_etc         /dev/data1vg/r0.etc
mkfs.gfs2 -p lock_dlm -j2 -t pcmk:OCFS2_DRBDr0_varlibmysql /dev/data1vg/r0.varlibmysql
mkfs.gfs2 -p lock_dlm -j2 -t pcmk:OCFS2_DRBDr0_varwww      /dev/data1vg/r0.varwww

mkdir -p /mnt/drbdr0  <span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">el punto de montaje del vol root.r0, que usaremos luego.</span>
</pre>



<pre class="src src-text">-j num_journals -O ser&#237;a para overwrite sin preguntar
-p proto_de_lock  -t tipo_tabla_de_lock_segun_el_proto es un doblete
clustername:fsname y clustername debe aparecer en el cluster.conf de cman?
espero que pcmk sea una palabra reservada para pacemaker, pues no uso cman.
</pre>


<ul>
<li>Resto de recursos:
</li>
</ul>




<pre class="src src-sh">crm configure primitive FSroot_r0 ocf:heartbeat:Filesystem params <span style="color: #eedd82;">device</span>=<span style="color: #ffa07a;">"/dev/data1vg/root.r0"</span> <span style="color: #eedd82;">directory</span>=<span style="color: #ffa07a;">"/mnt/drbdr0"</span> <span style="color: #eedd82;">fstype</span>=<span style="color: #ffa07a;">"gfs2"</span> op monitor <span style="color: #eedd82;">interval</span>=<span style="color: #ffa07a;">"120s"</span> meta target-role=<span style="color: #ffa07a;">"Started"</span>
crm configure primitive FSr0_etc ocf:heartbeat:Filesystem params <span style="color: #eedd82;">device</span>=<span style="color: #ffa07a;">"/dev/data1vg/r0.etc"</span> <span style="color: #eedd82;">directory</span>=<span style="color: #ffa07a;">"/mnt/drbdr0/etc"</span> <span style="color: #eedd82;">fstype</span>=<span style="color: #ffa07a;">"gfs2"</span> op monitor <span style="color: #eedd82;">interval</span>=<span style="color: #ffa07a;">"120s"</span> meta target-role=<span style="color: #ffa07a;">"Started"</span>
crm configure primitive FSr0_varlibmysql ocf:heartbeat:Filesystem params <span style="color: #eedd82;">device</span>=<span style="color: #ffa07a;">"/dev/data1vg/r0.varlibmysql"</span> <span style="color: #eedd82;">directory</span>=<span style="color: #ffa07a;">"/mnt/drbdr0/var/lib/mysql"</span> <span style="color: #eedd82;">fstype</span>=<span style="color: #ffa07a;">"gfs2"</span> op monitor <span style="color: #eedd82;">interval</span>=<span style="color: #ffa07a;">"120s"</span> meta target-role=<span style="color: #ffa07a;">"Started"</span>
crm configure primitive FSroot_r0 ocf:heartbeat:Filesystem params <span style="color: #eedd82;">device</span>=<span style="color: #ffa07a;">"/dev/data1vg/r0.varwww"</span> <span style="color: #eedd82;">directory</span>=<span style="color: #ffa07a;">"/mnt/drbdr0/var/www"</span> <span style="color: #eedd82;">fstype</span>=<span style="color: #ffa07a;">"gfs2"</span> op monitor <span style="color: #eedd82;">interval</span>=<span style="color: #ffa07a;">"120s"</span> meta target-role=<span style="color: #ffa07a;">"Started"</span>
crm configure group FSr0 FSroot_r0 FSr0_etc FSr0_varlibmysql FSr0_varwww
crm configure clone FSr0_Clone meta <span style="color: #eedd82;">interleave</span>=<span style="color: #ffa07a;">"true"</span> <span style="color: #eedd82;">ordered</span>=<span style="color: #ffa07a;">"true"</span> target-role=<span style="color: #ffa07a;">"Started"</span>

crm configure colocation FSr0-with-GFS2D inf: FSr0_Clone GFS2D_Clone
crm configure edit order-drbd-to-lvm
    order order-drbd-to-lvm inf: DRBDr0_Clone:promote DLM_Clone:start GFSD2_Clone:start CLVM_Clone:start LVMvg1_Clone:start


<span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">... primitive LINKS_varwww ocf:heartbeat:symlink params target="/mnt/drbdr0/var/www" link="/var/www" backup_suffix=".active"</span>
<span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">... primitive LINKS_etcapache2 ocf:heartbeat:symlink params target="/mnt/drbdr0/etc/apache2" link="/etc/apache2" backup_suffix=".active"</span>
<span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">... primitive LINKS_varlibmysql ocf:heartbeat:symlink params target="/mnt/drbdr0/var/lib/mysql" link="/var/lib/mysql" backup_suffix=".active"</span>
<span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">... primitive LINKS_etcmysql ocf:heartbeat:symlink params target="/mnt/drbdr0/etc/mysql" link="/etc/mysql" backup_suffix=".active"</span>
<span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">... primitive LINKS_etcphpmyadmin ocf:heartbeat:symlink params target="/mnt/drbdr0/etc/phpmyadmin" link="/etc/phpmyadmin" backup_suffix=".active"</span>
<span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">... group linksgroup LINKS_varwww LINKS_etcapache2 LINKS_varlibmysql LINKS_etcmysql LINKS_etcphpmyadmin</span>
<span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">... clone linksgroup_Clone clone linksgroup_Clone linksgroup meta interleave="true"</span>
<span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">... order LINKS_linksgroup-after-FSr0 inf: FSr0 linksgroup</span>
<span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">... primitive MySQL ocf:heartbeat:mysql</span>
<span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">... primitive Apache2 ocf:heartbeat:apache params configfile=/etc/apache2/apache2.conf op monitor interval=1min statusurl="http://127.0.0.1/server-status/"</span>
<span style="color: #ff7f24;"># </span><span style="color: #ff7f24;">... clone Apache2_Clone Apache2 meta interleave="true" target-role="Started" #meta clone-max="2"</span>
</pre>


</div>

</div>

<div id="outline-container-4-11" class="outline-3">
<h3 id="sec-4-11"><span class="section-number-3">4.11</span> Load balancing with LVS y Bonding (High Throughput)</h3>
<div class="outline-text-3" id="text-4-11">

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<caption></caption>
<colgroup><col class="left" />
</colgroup>
<tbody>
<tr><td class="left"><a href="http://doc.opensuse.org/products_new/draft/SLE-HA/SLE-ha/index.html">http://doc.opensuse.org/products_new/draft/SLE-HA/SLE-ha/index.html</a></td></tr>
<tr><td class="left"><a href="https://wiki.ubuntu.com/ClusterStack/LucidTesting#Load_Balancing_with_Pacemaker.2BAC8-ldirectord">https://wiki.ubuntu.com/ClusterStack/LucidTesting#Load_Balancing_with_Pacemaker.2BAC8-ldirectord</a></td></tr>
</tbody>
</table>

</div>
</div>

</div>

<div id="outline-container-5" class="outline-2">
<h2 id="sec-5"><span class="section-number-2">5</span> Anotaciones Multi-site clusters</h2>
<div class="outline-text-2" id="text-5">

<p>  <a href="https://www.suse.com/documentation/sle_ha/book_sleha/data/cha_ha_geo.html">https://www.suse.com/documentation/sle_ha/book_sleha/data/cha_ha_geo.html</a>
</p>
<ul>
<li>Al estar muy alejados (varias WAN), no se pueden usar fiablemente
  comunicaciones o replicaciones síncronas.
<ul>
<li>La solución es hacer cada sitio más autónomo (reduciendo así la necesidad
    de comunicación entre sitios): cada subcluster se entiende como un nodo
    (overlay) en un "cluster de sites".
</li>
<li>y para funcionar asíncronamente, cada overlay recibe un ticket que memoriza
    la decisión de qué overlay sirve qué servicios mientras dure el ticket. Un
    sw Booth (cabina) gestiona los tickets y por tanto los failover entre
    sitios. Cada sitio ejecuta un boothd, si alguien poseedor de un ticket
    queda incomunicado, se hace quorum.
<ul>
<li>Arbitrator es un sitio donde se ejecuta boothd (en un modo especial), pero
      no servicios. Se necesita si número par de sitios (ej: 2).
</li>
</ul>

</li>
<li>Administrativamente, surge por tanto una nueva restricción que es el ticket
    multi-site, al que se le asocian servicios. Y un nuevo primitive (para
    boothd), el cual tiene que tener una IP distinta en cada sitio, por lo que
    en cada sitio se suele crear una IP virtual (distinta) asociada a éste; el
    arbitrator no es manejado por el clúster (quizás no hay clúster alguno
    donde él reside).
</li>
</ul>

</li>
</ul>






<ul>
<li>Creamos un ticket y le asociamos los servicios (rsc1)
</li>
</ul>




<pre class="src src-text">crm configure rsc_ticket rsc1-req-ticketA ticketA: rsc1 loss-policy="fence"
  ... si el servicios es multistate le sufijamos :Master
  crm configure rsc_ticket rsc1-req-ticketA ticketA: rsc1:Master loss-policy="fence"
  ... loss-policy configura qu&#233; se hace con los recursos que dependen de otro
  cuyo ticket acaba de expirar/ser retirado de sus site.
</pre>

<p>
Antes de lanzar a boothd hay que, manualmente, asignar el ticket a algún site
</p>


<pre class="src src-sh">booth client list
booth client grant -t ticketA -s 147.2.207.14 <span style="color: #ff7f24;">#</span><span style="color: #ff7f24;">booth client revoke -t ticketA -s 147.2.207.14</span>
</pre>


<p>
También hay que configurarlo:
</p>


<pre class="src src-sh">vim /etc/booth/booth.conf
</pre>


<pre class="src src-sh"><span style="color: #eedd82;">transport</span>=<span style="color: #ffa07a;">"UDP"</span>
<span style="color: #eedd82;">port</span>=<span style="color: #ffa07a;">"6666"</span>
<span style="color: #eedd82;">arbitrator</span>=<span style="color: #ffa07a;">"147.2.207.14"</span>
<span style="color: #eedd82;">site</span>=<span style="color: #ffa07a;">"147.4.215.19"</span>
<span style="color: #eedd82;">site</span>=<span style="color: #ffa07a;">"147.18.2.1"</span>
<span style="color: #eedd82;">ticket</span>=<span style="color: #ffa07a;">"ticketA;1000"</span>
<span style="color: #eedd82;">ticket</span>=<span style="color: #ffa07a;">"ticketB;1000"</span>
</pre>


<pre class="src src-sh">scp /etc/booth/booth.conf &lt;other sites&gt; &lt;arbitrator&gt;
</pre>


<ul>
<li>Creamos el recurso boothd y su ip virtual asociada:
</li>
</ul>




<pre class="src src-text">primitive booth-ip ocf:heartbeat:IPaddr2 params ip="IP_ADDRESS"
primitive booth ocf:pacemaker:booth-site  \
      meta resource-stickiness="INFINITY" \
      op monitor interval="10s" timeout="20s"
group g-booth booth-ip booth
</pre>

<ul>
<li>El administrador de tickets ha de ejecutarse antes que los servicios:
</li>
</ul>




<pre class="src src-text">crm configure order order-booth-rsc1 inf: g-booth rsc1
</pre>

<p>
  &hellip; si multistate, sufijamos promote:
  crm configure order order-booth-rsc1 inf: g-booth rsc1:promote
</p>
<p>
En los arbitrator:
</p>


<pre class="src src-sh">/etc/init.d/booth-arbitrator start
</pre>


<ul>
<li>Creo que no se habla de fencing quizás porque se entiende que no vas a
  crear un despliegue multisite que lo necesite a ese nivel.
</li>
<li>Al no ser síncrono no necesita NTP entre sitios
</li>
</ul>



<p>
Lo anterior debiera responder a estas preguntas:
</p><ul>
<li>How to make sure that a cluster site is up and running?

</li>
<li>How to make sure that resources are only started once?

</li>
<li>How to make sure that quorum can be reached between the different sites and
    a split brain scenario can be avoided?

</li>
<li>How to manage failover between the sites?

</li>
<li>How to deal with high latency in case of resources that need to be stopped? 
</li>
</ul>

</div>

</div>

<div id="outline-container-6" class="outline-2">
<h2 id="sec-6"><span class="section-number-2">6</span> HA y firewalls</h2>
<div class="outline-text-2" id="text-6">

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<caption></caption>
<colgroup><col class="left" />
</colgroup>
<tbody>
<tr><td class="left"><a href="http://backreference.org/2013/04/03/firewall-ha-with-conntrackd-and-keepalived/">http://backreference.org/2013/04/03/firewall-ha-with-conntrackd-and-keepalived/</a></td></tr>
<tr><td class="left"><a href="http://blogs.it.ox.ac.uk/jamest/2011/06/03/replicating-block-devices-with-drbd/">http://blogs.it.ox.ac.uk/jamest/2011/06/03/replicating-block-devices-with-drbd/</a></td></tr>
</tbody>
</table>


<p>
iptables -I FORWARD -m physdev &ndash;physdev-is-bridged -j ACCEPT
</p>
<ul>
<li>TODO
</li>
</ul>

</div>

</div>

<div id="outline-container-7" class="outline-2">
<h2 id="sec-7"><span class="section-number-2">7</span> Tests y desempeño</h2>
<div class="outline-text-2" id="text-7">

<ul>
<li>Testing and performance para almacenamiento:
    <a href="http://crunchtools.com/kvm-cluster-with-drbd-gfs2/#Testing__Performance">http://crunchtools.com/kvm-cluster-with-drbd-gfs2/#Testing__Performance</a>
<ul>
<li>Ping Pong: Small C program used to check plocks/sec performance in GFS2
</li>
<li>Bonnie++: All disk I/O was generated with bonnie++. Bonnie++ also
      provides results on benchmarking
</li>
<li>iostat: Disk I/O was also measured with iostat during the bonnie++
      testing to confirm results
</li>
<li>sar: Network traffic was monitored with sar to verify the network I/O was
      coherent with the disk I/O data

</li>
</ul>

</li>
<li>Testing and performance para el servicio www:
<ul>
<li><a href="http://crunchtools.com/kvm-cluster-with-drbd-gfs2/#Analysis">http://crunchtools.com/kvm-cluster-with-drbd-gfs2/#Analysis</a>
</li>
<li>ab
</li>
</ul>

</li>
<li>TODO
</li>
</ul>

</div>

</div>

<div id="outline-container-8" class="outline-2">
<h2 id="sec-8"><span class="section-number-2">8</span> Anotación Samba clustering</h2>
<div class="outline-text-2" id="text-8">

<p><a href="https://www.suse.com/documentation/sle_ha/book_sleha/data/cha_ha_samba.html">https://www.suse.com/documentation/sle_ha/book_sleha/data/cha_ha_samba.html</a>
</p></div>

</div>

<div id="outline-container-9" class="outline-2">
<h2 id="sec-9"><span class="section-number-2">9</span> Anotación Ganeti2</h2>
<div class="outline-text-2" id="text-9">

<p><a href="https://code.google.com/p/ganeti/">https://code.google.com/p/ganeti/</a>
</p><ul>
<li>Cluster virtual server management software tool. Usa SSH y socat. Escrito en py.
</li>
<li>Used by: Google, Debian Infraestructure, &hellip;
</li>
<li>KVM/XEN. DRBD, LVM.
</li>
<li>1/40 nodes.
</li>
<li>No soporte especial para HA (corosync/pacemaker), pero obbiamente los
  servicios de las vm pueden beneficiarse de un despligue HA.
</li>
</ul>

</div>
</div>
</div>

<div id="postamble">
<p class="author">Autor: Félix Alises-Casas</p>
<p class="email"><a href="mailto:felix.alises.io@gmail.com">felix.alises.io@gmail.com</a></p>
<a href="http://validator.w3.org/check?uri=referer">Validate XHTML 1.0</a>

</div>
</body>
</html>
